{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8d91c0f",
   "metadata": {},
   "source": [
    "# Practical 2A — Embeddings-first workflows with a Geospatial Foundation Model (Clay)\n",
    "\n",
    "**Road to SKA: Foundation Models, Embeddings, and Latent Spaces**\n",
    "\n",
    "In this practical you will build an **embeddings-first** workflow:\n",
    "\n",
    "1. Load **pre-computed Clay embeddings** for a small AOI.\n",
    "2. Create labels by joining training points to embedding “chips”.\n",
    "3. Train a lightweight classifier (Random Forest / Logistic Regression) on embeddings.\n",
    "4. Run inference over *all* embeddings to “discover” target locations.\n",
    "5. Build a **similarity search** index (nearest neighbours) to do retrieval and qualitative QA.\n",
    "\n",
    "This approach mirrors a common pattern for foundation models:\n",
    "> **Freeze the foundation model → treat embeddings as features → train a small head**\n",
    "\n",
    "---\n",
    "\n",
    "## Key links / references\n",
    "\n",
    "**Clay Foundation Model**\n",
    "- Docs: https://clay-foundation.github.io/model/  \n",
    "- “Basic use” (generating embeddings): https://clay-foundation.github.io/model/getting-started/basic_use.html  \n",
    "- “Train classification on embeddings” tutorial (this practical is a simplified, dependency-light version): https://clay-foundation.github.io/model/finetune/finetune-on-embeddings.html  \n",
    "\n",
    "**Example embedding dataset used here**\n",
    "- Hugging Face dataset: `made-with-clay/classify-embeddings-sf-baseball-marinas`  \n",
    "  (Embeddings in GeoParquet + training points in GeoJSON, used in Clay’s tutorial.)  \n",
    "  https://huggingface.co/datasets/made-with-clay/classify-embeddings-sf-baseball-marinas\n",
    "\n",
    "**Background**\n",
    "- Embeddings as representations: https://clay-foundation.github.io/model/  \n",
    "- Masked Autoencoders (context): https://arxiv.org/abs/2111.06377\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lcdl5mkhjeg",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cambridge-iccs/R2SKA_Advanced_Tutorial/blob/main/Session2A_Embeddings_Workflow.ipynb)\n\n---\n\n## Environment Setup (Colab / Local)\n\nRun the cell below to detect your environment and set up paths. On **Google Colab**, it will install required geospatial packages automatically.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "qtbzeo0j85",
   "source": "# Detect environment and set up paths\nimport sys\n\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"Running in Google Colab\")\n    DATA_ROOT = '/content/data'\n    # Install required geospatial packages not available in Colab by default\n    !pip install -q shapely contextily faiss-cpu\nelse:\n    print(\"Running locally\")\n    DATA_ROOT = './data'\n\nprint(f\"Data directory: {DATA_ROOT}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "74c93438",
   "metadata": {},
   "source": [
    "## 1. About Clay and Geospatial Embeddings\n",
    "\n",
    "### What is Clay?\n",
    "\n",
    "**Clay** is an open-source **geospatial foundation model** trained on petabytes of satellite imagery from multiple sensors (Sentinel-2, Landsat, NAIP, etc.). Like foundation models in NLP (BERT, GPT) or vision (CLIP, DINOv2), Clay learns general-purpose representations that transfer to many downstream tasks.\n",
    "\n",
    "Clay uses a **Masked Autoencoder (MAE)** architecture:\n",
    "1. Input satellite image chips are divided into patches\n",
    "2. A large fraction (~75%) of patches are randomly masked\n",
    "3. The model learns to reconstruct the missing patches\n",
    "4. The encoder learns rich representations in the process\n",
    "\n",
    "The key insight: by learning to \"fill in the blanks\" across diverse imagery, the model captures semantic understanding of land cover, urban structure, water bodies, vegetation patterns, and more — without any labels.\n",
    "\n",
    "### About the Clay embeddings\n",
    "\n",
    "Clay's encoder maps each 256×256 pixel chip to a dense vector:\n",
    "\n",
    "$$\\text{image chip} \\xrightarrow{\\text{Clay encoder}} \\mathbf{z} \\in \\mathbb{R}^{d}$$\n",
    "\n",
    "- **Clay v1** (ViT-Base backbone): **768 dimensions** — used for the precomputed embeddings in this practical\n",
    "- **Clay v1.5** (larger backbone): **1024 dimensions**\n",
    "\n",
    "These embeddings have useful properties:\n",
    "- **Semantic similarity**: chips with similar content (e.g., two marinas) have similar embeddings\n",
    "- **Transferability**: embeddings trained on one task generalise to others\n",
    "- **Efficiency**: classifiers on 768-d vectors are much cheaper than training on raw pixels\n",
    "\n",
    "### The embeddings-first workflow\n",
    "\n",
    "Instead of fine-tuning the entire foundation model (expensive, requires GPU), we:\n",
    "\n",
    "1. **Freeze** the pretrained Clay encoder\n",
    "2. **Extract embeddings** for our imagery (can be precomputed)\n",
    "3. **Train a lightweight head** (Random Forest, logistic regression, small MLP) on embeddings\n",
    "4. **Run inference** using the small head\n",
    "\n",
    "This is the same pattern used with CLIP embeddings for image search, or sentence transformers for text retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s36yme9rfl",
   "metadata": {},
   "source": [
    "## 2. Setup\n",
    "\n",
    "This notebook aims to keep dependencies lightweight.\n",
    "\n",
    "Required:\n",
    "- `numpy`, `pandas`, `scikit-learn`\n",
    "- `pyarrow` (for reading parquet)\n",
    "\n",
    "Recommended:\n",
    "- `shapely` (spatial point-in-polygon checks)\n",
    "\n",
    "Optional:\n",
    "- `faiss-cpu` (fast similarity search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1351d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional installs (uncomment if needed)\n",
    "# %pip -q install pandas numpy scikit-learn pyarrow shapely\n",
    "# %pip -q install faiss-cpu  # optional\n",
    "# %pip -q install contextily  # for basemap tiles\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Shapely for geometry ops (optional but recommended)\n",
    "try:\n",
    "    from shapely.geometry import shape, Point\n",
    "    from shapely import from_wkb  # shapely>=2\n",
    "    SHAPELY_OK = True\n",
    "except Exception as e:\n",
    "    SHAPELY_OK = False\n",
    "    print(\"Shapely not available. Spatial join will not work without it. Error:\", repr(e))\n",
    "\n",
    "# Contextily for basemap tiles (optional)\n",
    "try:\n",
    "    import contextily as cx\n",
    "    CONTEXTILY_OK = True\n",
    "except ImportError:\n",
    "    CONTEXTILY_OK = False\n",
    "    print(\"contextily not available. Maps will render without basemap tiles.\")\n",
    "    print(\"Install with: pip install contextily\")\n",
    "\n",
    "# Helper: convert lon/lat (EPSG:4326) to Web Mercator (EPSG:3857)\n",
    "def lonlat_to_webmerc(lon, lat):\n",
    "    \"\"\"Convert longitude/latitude arrays to Web Mercator coordinates.\"\"\"\n",
    "    lon = np.asarray(lon)\n",
    "    lat = np.asarray(lat)\n",
    "    x = lon * 20037508.34 / 180\n",
    "    y = np.log(np.tan((90 + lat) * np.pi / 360)) / (np.pi / 180)\n",
    "    y = y * 20037508.34 / 180\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f81f57",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "### About the SF Bay Area embedding dataset\n",
    "\n",
    "We use **precomputed Clay embeddings** from the Hugging Face dataset:  \n",
    "[`made-with-clay/classify-embeddings-sf-baseball-marinas`](https://huggingface.co/datasets/made-with-clay/classify-embeddings-sf-baseball-marinas)\n",
    "\n",
    "#### How the embeddings were created\n",
    "\n",
    "1. **Source imagery**: NAIP (National Agriculture Imagery Program) aerial imagery from 2022, covering the San Francisco Bay Area at 60 cm/pixel resolution.\n",
    "\n",
    "2. **Chipping**: The imagery was divided into non-overlapping **256×256 pixel chips** (~150m × 150m on the ground). Each chip has a geographic footprint stored as a polygon geometry.\n",
    "\n",
    "3. **Embedding extraction**: Each chip was passed through the **Clay v1 encoder** (ViT-Base), producing a **768-dimensional embedding vector**.\n",
    "\n",
    "4. **Storage format**: Embeddings are saved as **GeoParquet** files (`.gpq`), with columns:\n",
    "   - `embeddings`: the 768-d vector (as a list/array)\n",
    "   - `geometry`: the chip's geographic footprint (WKB-encoded polygon)\n",
    "   - `item_id`: identifier linking back to the source NAIP tile\n",
    "\n",
    "#### File naming convention\n",
    "\n",
    "The filenames follow NAIP tile naming:\n",
    "```\n",
    "embeddings_ca_m_3712213_ne_10_060_20220518.gpq\n",
    "          │   │ │       │  │  │   └── date (YYYYMMDD)\n",
    "          │   │ │       │  │  └── resolution (60cm)\n",
    "          │   │ │       │  └── UTM zone\n",
    "          │   │ │       └── quadrant (ne/nw/se/sw)\n",
    "          │   │ └── USGS tile ID\n",
    "          │   └── \"m\" for \"mosaic\"\n",
    "          └── state (California)\n",
    "```\n",
    "\n",
    "#### Spatial coverage\n",
    "\n",
    "Each GeoParquet file covers one NAIP tile quadrant (~3.75 × 3.75 km). Together, the 20 files span portions of San Francisco, Oakland, and the East Bay — areas containing the target features (baseball fields and marinas).\n",
    "\n",
    "#### Training labels\n",
    "\n",
    "The dataset includes hand-labelled **GeoJSON point files**:\n",
    "- `marinas.geojson`: points marking marina locations (class=1) and negative examples (class=0)\n",
    "- `baseball.geojson`: points marking baseball diamonds (class=1) and negatives (class=0)\n",
    "\n",
    "We spatially join these points to embedding chips to create training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab4a47",
   "metadata": {},
   "outputs": [],
   "source": "DATA_DIR = Path(f\"{DATA_ROOT}/practical2_clay\")\nDATA_DIR.mkdir(parents=True, exist_ok=True)\n\nHF_BASE = \"https://huggingface.co/datasets/made-with-clay/classify-embeddings-sf-baseball-marinas/resolve/main\"\n\n# All available embedding files from the Clay tutorial (GeoParquet)\n# Organized by USGS tile ID for geographic splitting\nEMBED_FILES_ALL = [\n    # Tile row 3712212-3712214 (northern area)\n    \"embeddings_ca_m_3712212_ne_10_060_20220519.gpq\",\n    \"embeddings_ca_m_3712212_nw_10_060_20220519.gpq\",\n    \"embeddings_ca_m_3712212_se_10_060_20220519.gpq\",\n    \"embeddings_ca_m_3712213_ne_10_060_20220518.gpq\",\n    \"embeddings_ca_m_3712213_nw_10_060_20220518.gpq\",\n    \"embeddings_ca_m_3712213_se_10_060_20220518.gpq\",\n    \"embeddings_ca_m_3712213_sw_10_060_20220518.gpq\",\n    \"embeddings_ca_m_3712214_sw_10_060_20220518.gpq\",\n    \"embeddings_ca_m_3712214_nw_10_060_20220518.gpq\",\n    # Tile row 3712220-3712222 (central area)\n    \"embeddings_ca_m_3712220_ne_10_060_20220519.gpq\",\n    \"embeddings_ca_m_3712221_ne_10_060_20220518.gpq\",\n    \"embeddings_ca_m_3712221_nw_10_060_20220518.gpq\",\n    \"embeddings_ca_m_3712221_sw_10_060_20220518.gpq\",\n    \"embeddings_ca_m_3712221_se_10_060_20220518.gpq\",\n    \"embeddings_ca_m_3712222_sw_10_060_20220518.gpq\",\n    \"embeddings_ca_m_3712222_nw_10_060_20220518.gpq\",\n    # Tile row 3712228-3712230 (southern area - HELD OUT FOR SPATIAL TEST)\n    \"embeddings_ca_m_3712228_ne_10_060_20220519.gpq\",\n    \"embeddings_ca_m_3712229_ne_10_060_20220518.gpq\",\n    \"embeddings_ca_m_3712229_nw_10_060_20220518.gpq\",\n    \"embeddings_ca_m_3712230_nw_10_060_20220518.gpq\",\n]\n\n# Split files geographically: train on north/central, test on southern tiles\nTRAIN_FILES = [f for f in EMBED_FILES_ALL if not any(t in f for t in ['3712228', '3712229', '3712230'])]\nSPATIAL_TEST_FILES = [f for f in EMBED_FILES_ALL if any(t in f for t in ['3712228', '3712229', '3712230'])]\n\nprint(f\"Training files: {len(TRAIN_FILES)} tiles (north/central Bay Area)\")\nprint(f\"Spatial test files: {len(SPATIAL_TEST_FILES)} tiles (southern Bay Area - held out)\")\n\n# Training point files (GeoJSON)\nPOINT_FILES = [\"marinas.geojson\", \"baseball.geojson\"]\n\nTASK = \"marinas\"  # \"marinas\" or \"baseball\"\n\nRANDOM_SEED = 42\n\nprint(f\"\\nTask: {TASK}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cc47246b",
   "metadata": {},
   "source": [
    "## 4. Download embedding files + training points\n",
    "\n",
    "This uses direct URLs from Hugging Face (same as Clay's tutorial). If files already exist locally, we skip re-downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a6e1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file(url: str, dst: Path, chunk_size: int = 1 << 20):\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if dst.exists() and dst.stat().st_size > 0:\n",
    "        return\n",
    "    with requests.get(url, stream=True, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(dst, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "# Download all files (training + spatial test + labels)\n",
    "all_embed_files = TRAIN_FILES + SPATIAL_TEST_FILES\n",
    "for fn in all_embed_files + POINT_FILES:\n",
    "    url = f\"{HF_BASE}/{fn}\"\n",
    "    dst = DATA_DIR / fn\n",
    "    print(\"Downloading:\", fn)\n",
    "    download_file(url, dst)\n",
    "\n",
    "print(\"\\nDone. Local files:\")\n",
    "for p in sorted(DATA_DIR.glob(\"*\")):\n",
    "    print(\" -\", p.name, f\"({p.stat().st_size/1e6:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4cc862",
   "metadata": {},
   "source": [
    "## 5. Load and visualise embeddings\n",
    "\n",
    "Each embedding row corresponds to a small image chip with:\n",
    "- `embeddings`: 1024-dimensional vector representation from Clay encoder\n",
    "- `geometry`: chip footprint polygon (~150m × 150m)\n",
    "\n",
    "We load the GeoParquet files into a single dataframe.\n",
    "\n",
    "> If you are memory-limited, keep `MAX_FILES` small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2737d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(parquet_paths):\n",
    "    dfs = []\n",
    "    for p in parquet_paths:\n",
    "        df = pd.read_parquet(p)\n",
    "        df[\"source_file\"] = Path(p).name\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Load TRAINING embeddings only (spatial test set loaded separately later)\n",
    "emb_df = load_embeddings([DATA_DIR / f for f in TRAIN_FILES])\n",
    "print(\"Training embeddings dataframe:\", emb_df.shape)\n",
    "print(\"Columns:\", emb_df.columns.tolist())\n",
    "\n",
    "# Inspect one row\n",
    "row = emb_df.iloc[0]\n",
    "print(\"Example item_id:\", row.get(\"item_id\", None))\n",
    "print(\"Embedding length:\", len(row[\"embeddings\"]))\n",
    "print(\"Geometry type:\", type(row[\"geometry\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e9b16",
   "metadata": {},
   "source": [
    "### Visualizing a single embedding chip\n",
    "\n",
    "Each embedding corresponds to a **256×256 pixel chip** from NAIP imagery at 60cm resolution, covering approximately **150m × 150m** on the ground. The Clay v1 encoder compresses this (~260K pixel values) into a **768-dimensional vector**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a single chip's footprint and its embedding\n",
    "from matplotlib.patches import Polygon as MplPolygon\n",
    "\n",
    "# Helper to convert geometry to shapely polygon\n",
    "def to_polygon(g):\n",
    "    if isinstance(g, (bytes, bytearray, memoryview)):\n",
    "        return from_wkb(bytes(g))\n",
    "    try:\n",
    "        return shape(g)\n",
    "    except:\n",
    "        return g\n",
    "\n",
    "# Helper to extract centroid from geometry\n",
    "def get_centroid(g):\n",
    "    if isinstance(g, (bytes, bytearray, memoryview)):\n",
    "        poly = from_wkb(bytes(g))\n",
    "    else:\n",
    "        try:\n",
    "            poly = shape(g)\n",
    "        except:\n",
    "            poly = g\n",
    "    c = poly.centroid\n",
    "    return c.x, c.y\n",
    "\n",
    "if SHAPELY_OK:\n",
    "\n",
    "    # Get centroids of all chips\n",
    "    centroids = [get_centroid(g) for g in emb_df[\"geometry\"].values]\n",
    "    xs, ys = zip(*centroids)\n",
    "\n",
    "    # Convert to Web Mercator for basemap\n",
    "    xs_merc, ys_merc = lonlat_to_webmerc(xs, ys)\n",
    "\n",
    "    # Pick a chip near the center of the dataset\n",
    "    chip_idx = len(emb_df) // 3\n",
    "    chip_row = emb_df.iloc[chip_idx]\n",
    "    chip_geom = to_polygon(chip_row[\"geometry\"])\n",
    "    chip_emb = chip_row[\"embeddings\"]\n",
    "    \n",
    "    # Get polygon coordinates and convert to Web Mercator\n",
    "    poly_coords = list(chip_geom.exterior.coords)\n",
    "    poly_lons, poly_lats = zip(*poly_coords)\n",
    "    poly_xs, poly_ys = lonlat_to_webmerc(poly_lons, poly_lats)\n",
    "    \n",
    "    # Calculate chip size\n",
    "    chip_width_m = chip_geom.bounds[2] - chip_geom.bounds[0]  # in degrees\n",
    "    chip_width_m_approx = chip_width_m * 111000 * np.cos(np.radians(np.mean(poly_lats)))  # rough meters\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Left: Chip footprint on map with surrounding chips\n",
    "    ax = axes[0]\n",
    "    \n",
    "    # Show nearby chips as context (within ~500m)\n",
    "    cx_chip, cy_chip = chip_geom.centroid.x, chip_geom.centroid.y\n",
    "    nearby_mask = [(abs(xs[i] - cx_chip) < 0.01) and (abs(ys[i] - cy_chip) < 0.01) \n",
    "                   for i in range(len(xs))]\n",
    "    nearby_xs = [xs_merc[i] for i, m in enumerate(nearby_mask) if m]\n",
    "    nearby_ys = [ys_merc[i] for i, m in enumerate(nearby_mask) if m]\n",
    "    ax.scatter(nearby_xs, nearby_ys, s=10, alpha=0.9, c='black', label='Nearby chip centroids')\n",
    "    \n",
    "    # Draw the selected chip polygon\n",
    "    poly_verts = list(zip(poly_xs, poly_ys))\n",
    "    patch = MplPolygon(poly_verts, fill=True, facecolor='blue', edgecolor='red', \n",
    "                       alpha=0.4, linewidth=2, zorder=5)\n",
    "    ax.add_patch(patch)\n",
    "    \n",
    "    # Mark centroid\n",
    "    cx_merc, cy_merc = lonlat_to_webmerc([cx_chip], [cy_chip])\n",
    "    ax.scatter(cx_merc, cy_merc, s=100, c='red', marker='x', zorder=10, label='Selected chip')\n",
    "    \n",
    "    ax.set_xlabel(\"Easting (Web Mercator)\")\n",
    "    ax.set_ylabel(\"Northing (Web Mercator)\")\n",
    "    ax.set_title(f\"Single chip footprint (idx={chip_idx})\\n~{chip_width_m_approx:.0f}m × {chip_width_m_approx:.0f}m on ground\")\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    # Zoom to chip area\n",
    "    pad = 500  # meters padding\n",
    "    ax.set_xlim(cx_merc[0] - pad, cx_merc[0] + pad)\n",
    "    ax.set_ylim(cy_merc[0] - pad, cy_merc[0] + pad)\n",
    "    \n",
    "    if CONTEXTILY_OK:\n",
    "        cx.add_basemap(ax, crs=\"EPSG:3857\", source=cx.providers.OpenStreetMap.Mapnik, alpha=0.8)\n",
    "    \n",
    "    # Right: The embedding vector itself\n",
    "    ax2 = axes[1]\n",
    "    ax2.bar(range(len(chip_emb)), chip_emb, width=1.0, color='steelblue', edgecolor='none')\n",
    "    ax2.set_xlabel(\"Embedding dimension\")\n",
    "    ax2.set_ylabel(\"Value\")\n",
    "    ax2.set_title(f\"{len(chip_emb)}-dimensional embedding vector\\nfor chip idx={chip_idx}\")\n",
    "    ax2.axhline(0, color='black', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Chip {chip_idx}:\")\n",
    "    print(f\"  Location: {cx_chip:.4f}°, {cy_chip:.4f}°\")\n",
    "    print(f\"  Approx size: {chip_width_m_approx:.0f}m × {chip_width_m_approx:.0f}m\")\n",
    "    print(f\"  Embedding: {len(chip_emb)} dimensions, range [{min(chip_emb):.2f}, {max(chip_emb):.2f}]\")\n",
    "else:\n",
    "    print(\"Shapely required for this visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979d18c6",
   "metadata": {},
   "source": [
    "### Visualizing the distribution and spatial coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6v93g2lrixv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings: distribution and spatial coverage\n",
    "\n",
    "# Plot 1: Embedding value distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "sample_embs = np.vstack(emb_df[\"embeddings\"].head(500).values)\n",
    "ax.hist(sample_embs.flatten(), bins=50, edgecolor='none', alpha=0.7)\n",
    "ax.set_xlabel(\"Embedding value\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Distribution of embedding values (sample of 500 chips × 1024 dims)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Spatial coverage of chips with basemap\n",
    "if SHAPELY_OK:\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    ax.scatter(xs_merc, ys_merc, s=3, alpha=0.6, zorder=2)\n",
    "    ax.set_xlabel(\"Easting (Web Mercator)\")\n",
    "    ax.set_ylabel(\"Northing (Web Mercator)\")\n",
    "    ax.set_title(f\"Spatial coverage of {len(emb_df)} embedding chips\")\n",
    "    \n",
    "    # Add basemap if available\n",
    "    if CONTEXTILY_OK:\n",
    "        cx.add_basemap(ax, crs=\"EPSG:3857\", source=cx.providers.OpenStreetMap.Mapnik, alpha=0.7)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Shapely required for spatial plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb61f79",
   "metadata": {},
   "source": [
    "## 6. Load training points (GeoJSON)\n",
    "\n",
    "The dataset includes two small hand-labelled sets of points:\n",
    "- `marinas.geojson`\n",
    "- `baseball.geojson`\n",
    "\n",
    "Each feature is expected to include a `class` property:\n",
    "- `1` = positive example (marina or baseball field)\n",
    "- `0` = negative example (other land cover)\n",
    "\n",
    "We will use these to label the embedding chips by spatial containment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7f0abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_geojson_points(path: Path):\n",
    "    with open(path, \"r\") as f:\n",
    "        gj = json.load(f)\n",
    "    feats = gj[\"features\"]\n",
    "    records = []\n",
    "    for feat in feats:\n",
    "        geom = feat[\"geometry\"]\n",
    "        props = feat.get(\"properties\", {})\n",
    "        records.append({\n",
    "            \"geometry\": geom,\n",
    "            \"class\": props.get(\"class\", None),\n",
    "            \"properties\": props\n",
    "        })\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "points_path = DATA_DIR / (f\"{TASK}.geojson\")\n",
    "pts_df = load_geojson_points(points_path)\n",
    "print(\"Points:\", pts_df.shape)\n",
    "print(\"Class counts:\\n\", pts_df[\"class\"].value_counts(dropna=False))\n",
    "\n",
    "if not SHAPELY_OK:\n",
    "    print(\"WARNING: Shapely not available, cannot perform spatial join.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ejw4xl3bxjo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training points on top of chip coverage with basemap\n",
    "if SHAPELY_OK:\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Background: all chip centroids (light gray) in Web Mercator\n",
    "    ax.scatter(xs_merc, ys_merc, s=2, alpha=0.3, c='gray', label='Chips', zorder=2)\n",
    "    \n",
    "    # Training points colored by class\n",
    "    pt_coords = []\n",
    "    pt_classes = []\n",
    "    for _, r in pts_df.iterrows():\n",
    "        pt = shape(r[\"geometry\"])\n",
    "        if pt.geom_type == \"Point\":\n",
    "            pt_coords.append((pt.x, pt.y))\n",
    "            pt_classes.append(r[\"class\"])\n",
    "    \n",
    "    if pt_coords:\n",
    "        pt_xs, pt_ys = zip(*pt_coords)\n",
    "        pt_xs_merc, pt_ys_merc = lonlat_to_webmerc(pt_xs, pt_ys)\n",
    "        colors = ['red' if c == 0 else 'green' for c in pt_classes]\n",
    "        ax.scatter(pt_xs_merc, pt_ys_merc, s=80, c=colors, edgecolor='white', linewidth=0.5, zorder=5)\n",
    "        \n",
    "        # Legend\n",
    "        from matplotlib.lines import Line2D\n",
    "        legend_elements = [\n",
    "            Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='Positive (class=1)'),\n",
    "            Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Negative (class=0)'),\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    ax.set_xlabel(\"Easting (Web Mercator)\")\n",
    "    ax.set_ylabel(\"Northing (Web Mercator)\")\n",
    "    ax.set_title(f\"Training points for '{TASK}' task ({len(pt_coords)} labelled points)\")\n",
    "    \n",
    "    # Add basemap if available\n",
    "    if CONTEXTILY_OK:\n",
    "        cx.add_basemap(ax, crs=\"EPSG:3857\", source=cx.providers.OpenStreetMap.Mapnik, alpha=0.7)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Shapely required for spatial visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b4e00b",
   "metadata": {},
   "source": [
    "## 7. Spatial join: assign point labels to embedding chips\n",
    "\n",
    "We label each embedding chip by checking which training points fall within its geographic footprint (point-in-polygon test).\n",
    "\n",
    "This creates a training table of `(embedding_vector, class_label)`.\n",
    "\n",
    "This is intentionally implemented **without GeoPandas** to reduce heavy dependencies.\n",
    "\n",
    "If this is slow, reduce `MAX_FILES`, or sample fewer points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1110287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embedding geometries (stored as WKB) to shapely polygons (shapely>=2)\n",
    "# Many parquet writers store geometry as bytes (WKB). If yours stores as WKT/GeoJSON, adapt below.\n",
    "def to_polygon(g):\n",
    "    if isinstance(g, (bytes, bytearray, memoryview)):\n",
    "        return from_wkb(bytes(g))\n",
    "    # sometimes already shapely geometry or dict\n",
    "    try:\n",
    "        return shape(g)\n",
    "    except Exception:\n",
    "        return g  # fallback\n",
    "\n",
    "# Convert points to shapely Points\n",
    "def to_point(geom_dict):\n",
    "    shp = shape(geom_dict)\n",
    "    if shp.geom_type == \"Point\":\n",
    "        return shp\n",
    "    # If it's something else, take centroid\n",
    "    return shp.centroid\n",
    "\n",
    "# Precompute polygons + bounds for speed\n",
    "polys = [to_polygon(g) for g in emb_df[\"geometry\"].values]\n",
    "bounds = np.array([p.bounds for p in polys])  # (minx, miny, maxx, maxy)\n",
    "print(\"Prepared\", len(polys), \"polygons\")\n",
    "\n",
    "# For each point, find candidate polygons by bbox test then exact contains()\n",
    "train_rows = []\n",
    "for i, r in pts_df.iterrows():\n",
    "    cls = r[\"class\"]\n",
    "    if cls is None:\n",
    "        continue\n",
    "    pt = to_point(r[\"geometry\"])\n",
    "    x, y = pt.x, pt.y\n",
    "\n",
    "    # bbox filter\n",
    "    cand = np.where((bounds[:,0] <= x) & (bounds[:,2] >= x) & (bounds[:,1] <= y) & (bounds[:,3] >= y))[0]\n",
    "    found = None\n",
    "    for j in cand:\n",
    "        if polys[j].contains(pt):\n",
    "            found = j\n",
    "            break\n",
    "    if found is not None:\n",
    "        train_rows.append({\"emb_idx\": int(found), \"class\": int(cls)})\n",
    "\n",
    "train_df = pd.DataFrame(train_rows).drop_duplicates(\"emb_idx\")\n",
    "print(\"Training rows found:\", len(train_df))\n",
    "print(train_df[\"class\"].value_counts())\n",
    "\n",
    "# Merge labels onto embeddings (only for training rows)\n",
    "train_merged = emb_df.iloc[train_df[\"emb_idx\"].values].copy()\n",
    "train_merged[\"class\"] = train_df[\"class\"].values\n",
    "\n",
    "X = np.vstack(train_merged[\"embeddings\"].values)\n",
    "y = train_merged[\"class\"].values\n",
    "\n",
    "print(\"X:\", X.shape, \"y:\", y.shape, \"pos rate:\", y.mean() if len(y) else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2178800",
   "metadata": {},
   "source": [
    "## 8. Train a classifier on embeddings\n",
    "\n",
    "We'll try:\n",
    "- **Random Forest**: robust, non-linear, strong baseline\n",
    "- **Logistic Regression**: fast linear probe baseline\n",
    "\n",
    "The goal is to show how far you can get **without fine-tuning the foundation model** — just training a small classifier on frozen embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0235c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(np.unique(y)) < 2:\n",
    "    raise RuntimeError(\"Need at least two classes in training data. Try switching TASK or increasing MAX_FILES.\")\n",
    "\n",
    "# Check if we have enough samples for stratified splitting\n",
    "min_class_count = min(np.bincount(y))\n",
    "use_stratify = min_class_count >= 2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_SEED, \n",
    "    stratify=y if use_stratify else None\n",
    ")\n",
    "\n",
    "if not use_stratify:\n",
    "    print(\"Note: Using non-stratified split due to small sample size. Increase MAX_FILES for better results.\")\n",
    "\n",
    "# Show class balance\n",
    "print(\"Training set class distribution:\", dict(zip(*np.unique(y_train, return_counts=True))))\n",
    "print(\"Test set class distribution:\", dict(zip(*np.unique(y_test, return_counts=True))))\n",
    "\n",
    "# Baseline: logistic regression (linear probe) with balanced class weights\n",
    "lin = LogisticRegression(max_iter=2000, class_weight='balanced')\n",
    "lin.fit(X_train, y_train)\n",
    "pred_lin = lin.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Logistic Regression (linear probe, balanced weights) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred_lin))\n",
    "print(\"Precision:\", precision_score(y_test, pred_lin, zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test, pred_lin, zero_division=0))\n",
    "print(classification_report(y_test, pred_lin, zero_division=0))\n",
    "\n",
    "# Strong baseline: Random Forest with balanced class weights\n",
    "rf = RandomForestClassifier(n_estimators=300, class_weight='balanced', random_state=RANDOM_SEED)\n",
    "rf.fit(X_train, y_train)\n",
    "pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Random Forest (balanced weights) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred_rf))\n",
    "print(\"Precision:\", precision_score(y_test, pred_rf, zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test, pred_rf, zero_division=0))\n",
    "print(classification_report(y_test, pred_rf, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6mbtdwv76wx",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classifier performance\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Confusion matrices (force 2x2 by specifying labels)\n",
    "for ax, preds, name in [(axes[0], pred_lin, \"Logistic Regression\"), (axes[1], pred_rf, \"Random Forest\")]:\n",
    "    cm = confusion_matrix(y_test, preds, labels=[0, 1])  # force 2x2\n",
    "    im = ax.imshow(cm, cmap='Blues')\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels(['Neg (0)', 'Pos (1)'])\n",
    "    ax.set_yticklabels(['Neg (0)', 'Pos (1)'])\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    ax.set_title(f\"{name}\\nConfusion Matrix\")\n",
    "    # Annotate cells\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, str(cm[i, j]), ha='center', va='center', \n",
    "                   color='white' if cm[i, j] > cm.max()/2 else 'black', fontsize=14)\n",
    "\n",
    "# Feature importance (Random Forest)\n",
    "importances = rf.feature_importances_\n",
    "top_k = 20\n",
    "top_idx = np.argsort(importances)[-top_k:]\n",
    "axes[2].barh(range(top_k), importances[top_idx], color='steelblue')\n",
    "axes[2].set_yticks(range(top_k))\n",
    "axes[2].set_yticklabels([f\"dim {i}\" for i in top_idx])\n",
    "axes[2].set_xlabel(\"Importance\")\n",
    "axes[2].set_title(f\"Top {top_k} embedding dimensions\\n(Random Forest feature importance)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ejbrk1zs4p",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA visualization of training embeddings colored by class\n",
    "pca = PCA(n_components=2, random_state=RANDOM_SEED)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "colors = ['red' if c == 0 else 'green' for c in y]\n",
    "ax.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, s=40, alpha=0.7, edgecolor='white', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)\")\n",
    "ax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)\")\n",
    "ax.set_title(\"Training embeddings (PCA projection)\\nColored by class: green=positive, red=negative\")\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='Positive (1)'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Negative (0)'),\n",
    "]\n",
    "ax.legend(handles=legend_elements)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"PCA explained variance: {pca.explained_variance_ratio_.sum()*100:.1f}% in 2 components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c2502f",
   "metadata": {},
   "source": [
    "## 9. Inference over the entire embedding table\n",
    "\n",
    "Now we apply the trained classifier over **all** embeddings (not just labelled ones) to \"discover\" likely target locations across the AOI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0c62db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which classifier to use for inference\n",
    "model = rf  # or lin\n",
    "\n",
    "X_all = np.vstack(emb_df[\"embeddings\"].values)\n",
    "pred_all = model.predict(X_all)\n",
    "print(\"Predicted positives:\", int(pred_all.sum()), \"out of\", len(pred_all))\n",
    "\n",
    "# Keep top candidates (if model supports predict_proba)\n",
    "if hasattr(model, \"predict_proba\"):\n",
    "    proba = model.predict_proba(X_all)[:, 1]\n",
    "    topk = np.argsort(-proba)[:20]\n",
    "    print(\"\\nTop-20 candidates by probability:\")\n",
    "    for rank, idx in enumerate(topk, 1):\n",
    "        print(rank, \"idx\", int(idx), \"p\", float(proba[idx]))\n",
    "else:\n",
    "    topk = np.where(pred_all == 1)[0][:20]\n",
    "    print(\"\\nTop candidates (no probabilities):\", topk[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xtuzolrln79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize inference results\n",
    "\n",
    "# Plot 1: Probability distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "if hasattr(model, \"predict_proba\"):\n",
    "    ax.hist(proba, bins=50, edgecolor='none', alpha=0.7)\n",
    "    ax.axvline(0.5, color='red', linestyle='--', label='Decision threshold')\n",
    "    ax.set_xlabel(\"Predicted probability (class=1)\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(\"Distribution of predicted probabilities\")\n",
    "    ax.legend()\n",
    "else:\n",
    "    ax.hist(pred_all, bins=3, edgecolor='white')\n",
    "    ax.set_title(\"Prediction distribution (no probabilities)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Spatial prediction map with basemap\n",
    "if SHAPELY_OK:\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Color by probability (or prediction if no proba)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        colors = proba\n",
    "        label = 'Probability'\n",
    "    else:\n",
    "        colors = pred_all\n",
    "        label = 'Prediction'\n",
    "    \n",
    "    sc = ax.scatter(xs_merc, ys_merc, c=colors, cmap='RdYlGn', s=5, alpha=0.8, \n",
    "                    vmin=0, vmax=1, zorder=2)\n",
    "    plt.colorbar(sc, ax=ax, label=label, shrink=0.8)\n",
    "    ax.set_xlabel(\"Easting (Web Mercator)\")\n",
    "    ax.set_ylabel(\"Northing (Web Mercator)\")\n",
    "    ax.set_title(f\"Spatial predictions for '{TASK}' (green=likely positive, red=likely negative)\")\n",
    "    \n",
    "    # Add basemap if available\n",
    "    if CONTEXTILY_OK:\n",
    "        cx.add_basemap(ax, crs=\"EPSG:3857\", source=cx.providers.OpenStreetMap.Mapnik, alpha=0.5)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Shapely required for spatial visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xqkm93m1wi",
   "metadata": {},
   "source": [
    "## 9. Spatial generalization test (held-out coastal area)\n",
    "\n",
    "A critical test for geospatial models is whether they **generalize spatially** — can a model trained on one area detect targets in a nearby but unseen region?\n",
    "\n",
    "We held out the southern Bay Area tiles (3712228, 3712229, 3712230) during training. Now we evaluate predictions on this unseen coastline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ubhvw8452i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spatial test embeddings (held-out southern tiles)\n",
    "spatial_test_df = load_embeddings([DATA_DIR / f for f in SPATIAL_TEST_FILES])\n",
    "print(f\"Spatial test set: {len(spatial_test_df)} chips from {len(SPATIAL_TEST_FILES)} tiles\")\n",
    "\n",
    "# Extract embeddings and predict\n",
    "X_spatial = np.vstack(spatial_test_df[\"embeddings\"].values)\n",
    "pred_spatial = model.predict(X_spatial)\n",
    "print(f\"Predicted positives in spatial test area: {pred_spatial.sum()} / {len(pred_spatial)}\")\n",
    "\n",
    "# Get probabilities if available\n",
    "if hasattr(model, \"predict_proba\"):\n",
    "    proba_spatial = model.predict_proba(X_spatial)[:, 1]\n",
    "    print(f\"Probability range: {proba_spatial.min():.3f} - {proba_spatial.max():.3f}\")\n",
    "    print(f\"Mean probability: {proba_spatial.mean():.3f}\")\n",
    "\n",
    "# Get centroids for spatial test chips\n",
    "if SHAPELY_OK:\n",
    "    spatial_centroids = [get_centroid(g) for g in spatial_test_df[\"geometry\"].values]\n",
    "    xs_spatial, ys_spatial = zip(*spatial_centroids)\n",
    "    xs_spatial_merc, ys_spatial_merc = lonlat_to_webmerc(xs_spatial, ys_spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bp1n1jo088i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spatial test predictions\n",
    "if SHAPELY_OK:\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Color by probability\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        colors = proba_spatial\n",
    "        label = 'Probability'\n",
    "    else:\n",
    "        colors = pred_spatial\n",
    "        label = 'Prediction'\n",
    "    \n",
    "    sc = ax.scatter(xs_spatial_merc, ys_spatial_merc, c=colors, cmap='RdYlGn', \n",
    "                    s=5, alpha=0.8, vmin=0, vmax=1, zorder=2)\n",
    "    plt.colorbar(sc, ax=ax, label=label, shrink=0.8)\n",
    "    \n",
    "    # Highlight predicted positives\n",
    "    pos_mask = pred_spatial == 1\n",
    "    if pos_mask.sum() > 0:\n",
    "        ax.scatter(np.array(xs_spatial_merc)[pos_mask], np.array(ys_spatial_merc)[pos_mask],\n",
    "                   s=100, facecolors='none', edgecolors='blue', linewidth=2, \n",
    "                   zorder=5, label=f'Predicted positive ({pos_mask.sum()})')\n",
    "        ax.legend(loc='upper right')\n",
    "    \n",
    "    ax.set_xlabel(\"Easting (Web Mercator)\")\n",
    "    ax.set_ylabel(\"Northing (Web Mercator)\")\n",
    "    ax.set_title(f\"Spatial generalization test: predictions on HELD-OUT southern Bay Area\\n\"\n",
    "                 f\"({pred_spatial.sum()} predicted positives out of {len(pred_spatial)} chips)\")\n",
    "    \n",
    "    if CONTEXTILY_OK:\n",
    "        cx.add_basemap(ax, crs=\"EPSG:3857\", source=cx.providers.OpenStreetMap.Mapnik, alpha=0.5)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Shapely required for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acb96kdssi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined view: training area vs spatial test area\n",
    "if SHAPELY_OK:\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    \n",
    "    # Training area (gray)\n",
    "    ax.scatter(xs_merc, ys_merc, s=2, alpha=0.3, c='gray', label='Training area', zorder=1)\n",
    "    \n",
    "    # Spatial test area (blue outline)\n",
    "    ax.scatter(xs_spatial_merc, ys_spatial_merc, s=2, alpha=0.3, c='steelblue', \n",
    "               label='Spatial test area (held out)', zorder=1)\n",
    "    \n",
    "    # Predicted positives in training area\n",
    "    train_pos = pred_all == 1\n",
    "    if train_pos.sum() > 0:\n",
    "        ax.scatter(np.array(xs_merc)[train_pos], np.array(ys_merc)[train_pos],\n",
    "                   s=60, c='green', edgecolor='white', linewidth=0.5,\n",
    "                   zorder=5, label=f'Train predictions ({train_pos.sum()})')\n",
    "    \n",
    "    # Predicted positives in test area\n",
    "    test_pos = pred_spatial == 1\n",
    "    if test_pos.sum() > 0:\n",
    "        ax.scatter(np.array(xs_spatial_merc)[test_pos], np.array(ys_spatial_merc)[test_pos],\n",
    "                   s=80, c='lime', edgecolor='blue', linewidth=1.5,\n",
    "                   zorder=6, label=f'Test predictions ({test_pos.sum()})')\n",
    "    \n",
    "    ax.set_xlabel(\"Easting (Web Mercator)\")\n",
    "    ax.set_ylabel(\"Northing (Web Mercator)\")\n",
    "    ax.set_title(f\"Geographic split: Training vs Held-out Test Area\\n\"\n",
    "                 f\"Task: {TASK}\")\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "    if CONTEXTILY_OK:\n",
    "        cx.add_basemap(ax, crs=\"EPSG:3857\", source=cx.providers.OpenStreetMap.Mapnik, alpha=0.6)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  Training area: {len(emb_df)} chips, {train_pos.sum()} predicted positives\")\n",
    "    print(f\"  Test area:     {len(spatial_test_df)} chips, {test_pos.sum()} predicted positives\")\n",
    "else:\n",
    "    print(\"Shapely required for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3336dc42",
   "metadata": {},
   "source": [
    "## 10. Similarity search on embeddings (retrieval)\n",
    "\n",
    "Because embeddings capture semantic content, we can find **similar chips** using vector distance. Two common uses:\n",
    "\n",
    "1. **QA / debugging**: \"show me chips similar to this false positive\"\n",
    "2. **Weak supervision / curation**: \"find more examples like my few positives\"\n",
    "\n",
    "We build a cosine-distance nearest-neighbour index and query from one positive example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61a0bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a cosine nearest-neighbour index (sklearn; CPU-friendly)\n",
    "X_norm = X_all / (np.linalg.norm(X_all, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=10, metric=\"cosine\")\n",
    "nn.fit(X_norm)\n",
    "\n",
    "# Pick a query embedding: a known positive from training set if possible\n",
    "pos_indices = train_merged.index[train_merged[\"class\"] == 1].tolist()\n",
    "if len(pos_indices) == 0:\n",
    "    query_idx = int(topk[0])\n",
    "else:\n",
    "    query_idx = int(pos_indices[0])\n",
    "\n",
    "dist, ind = nn.kneighbors(X_norm[query_idx:query_idx+1], n_neighbors=10, return_distance=True)\n",
    "print(\"Query idx:\", query_idx)\n",
    "print(\"Nearest neighbours (cosine distance):\")\n",
    "for d, i in zip(dist[0], ind[0]):\n",
    "    print(f\"  idx={int(i):5d}  dist={float(d):.4f}  pred={int(pred_all[i])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zfedmt5cnko",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize retrieval results\n",
    "\n",
    "# Plot 1: Bar chart of distances to nearest neighbors\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.barh(range(len(ind[0])), dist[0][::-1], color='steelblue')\n",
    "ax.set_yticks(range(len(ind[0])))\n",
    "ax.set_yticklabels([f\"idx {i}\" for i in ind[0][::-1]])\n",
    "ax.set_xlabel(\"Cosine distance\")\n",
    "ax.set_title(f\"Nearest neighbors to query idx={query_idx} (smaller = more similar)\")\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Spatial view - highlight query and neighbors with basemap\n",
    "if SHAPELY_OK:\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Background chips\n",
    "    ax.scatter(xs_merc, ys_merc, s=2, alpha=0.15, c='gray', zorder=1)\n",
    "    \n",
    "    # Highlight neighbors\n",
    "    neighbor_xs_merc = [xs_merc[i] for i in ind[0]]\n",
    "    neighbor_ys_merc = [ys_merc[i] for i in ind[0]]\n",
    "    neighbor_colors = dist[0]  # color by distance\n",
    "    sc = ax.scatter(neighbor_xs_merc, neighbor_ys_merc, c=neighbor_colors, cmap='viridis_r', \n",
    "                    s=150, edgecolor='black', linewidth=1.5, zorder=5)\n",
    "    plt.colorbar(sc, ax=ax, label='Cosine distance', shrink=0.8)\n",
    "    \n",
    "    # Mark query with star\n",
    "    ax.scatter([xs_merc[query_idx]], [ys_merc[query_idx]], marker='*', s=400, c='red', \n",
    "               edgecolor='black', linewidth=1.5, zorder=10, label='Query')\n",
    "    \n",
    "    ax.set_xlabel(\"Easting (Web Mercator)\")\n",
    "    ax.set_ylabel(\"Northing (Web Mercator)\")\n",
    "    ax.legend(loc='upper right', fontsize=12)\n",
    "    ax.set_title(\"Spatial location of query and neighbors (query=star, neighbors=circles)\")\n",
    "    \n",
    "    # Add basemap if available\n",
    "    if CONTEXTILY_OK:\n",
    "        cx.add_basemap(ax, crs=\"EPSG:3857\", source=cx.providers.OpenStreetMap.Mapnik, alpha=0.7)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Shapely required for spatial visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613c9434",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This practical demonstrated the **embeddings-first** workflow:\n",
    "- Freeze a foundation model encoder\n",
    "- Train lightweight classifiers on embeddings\n",
    "- Use vector similarity for retrieval\n",
    "\n",
    "**Next steps:**\n",
    "- **Fine-tuning / LoRA:** instead of freezing the embedding model, update a small subset of parameters (Practical 3)\n",
    "- **Operationalising:** chip creation (STAC), batching, vector stores, and monitoring\n",
    "\n",
    "**Extensions for this notebook:**\n",
    "1. Replace precomputed embeddings with **fresh embeddings** generated by Clay for a new AOI/time window\n",
    "2. Swap the classifier for a small MLP head (PyTorch) and compare\n",
    "3. Build a proper vector index (FAISS/HNSW) and add metadata filtering (time/location/sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5f2c17",
   "metadata": {},
   "source": [
    "## Exercise: Apply to MiraBest Radio Galaxies\n",
    "\n",
    "Apply the embeddings-first workflow to radio astronomy data!\n",
    "\n",
    "### About MiraBest\n",
    "\n",
    "**MiraBest** contains labelled Fanaroff-Riley radio galaxies:\n",
    "- **FRI** (class 0): edge-darkened morphology\n",
    "- **FRII** (class 1): edge-brightened morphology\n",
    "- ~800 images (150×150 grayscale) in CIFAR-style pickle format\n",
    "- Zenodo: https://doi.org/10.5281/zenodo.4288837\n",
    "\n",
    "### Setup\n",
    "\n",
    "Use your autoencoder from Session 1A (or train a new one) to generate embeddings for MiraBest images. Alternatively, use PCA on flattened pixels as a baseline.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. **Load MiraBest** — Download `batches.tar.gz` and load pickle batches (same code as Session 1A)\n",
    "\n",
    "2. **Generate embeddings** — Two options:\n",
    "   - Use your trained encoder from Session 1A\n",
    "   - PCA on flattened pixels (simpler baseline)\n",
    "\n",
    "3. **Train classifiers** — Random Forest and Logistic Regression on embeddings\n",
    "\n",
    "4. **Evaluate** — Create confusion matrices for FRI vs FRII classification\n",
    "   - Random baseline is 50% (binary classification)\n",
    "\n",
    "5. **Similarity search** — Find similar galaxies using embedding distance\n",
    "\n",
    "### Questions to explore\n",
    "\n",
    "- How does autoencoder embedding accuracy compare to raw-pixel PCA?\n",
    "- Which FR class is harder to classify?\n",
    "- Do similar embeddings correspond to similar morphologies?\n",
    "\n",
    "### Solution\n",
    "\n",
    "See **Session2B_Extension_MiraBest.ipynb** for a complete worked solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2c3094",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r2ska-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}