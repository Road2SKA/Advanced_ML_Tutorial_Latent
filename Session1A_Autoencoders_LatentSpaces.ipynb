{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb2c077",
   "metadata": {},
   "source": [
    "# Practical 1A - Autoencoders & Latent Spaces (Embeddings)\n",
    "\n",
    "**Road to SKA: Foundation Models, Embeddings, and Latent Spaces**\n",
    "\n",
    "In this practical you will:\n",
    "\n",
    "1. Train a small **autoencoder** on an image dataset (default: MNIST).\n",
    "2. Extract **latent vectors** (embeddings) and visualise the latent space with **PCA** and **UMAP**.\n",
    "3. Explore latent-space operations:\n",
    "   - **Interpolation** between examples\n",
    "   - **Latent traversals** - vary one latent dimension and decode\n",
    "\n",
    "---\n",
    "\n",
    "## Datasets & references\n",
    "\n",
    "**Datasets**\n",
    "- MNIST: http://yann.lecun.com/exdb/mnist/  \n",
    "- Fashion-MNIST: https://github.com/zalandoresearch/fashion-mnist  \n",
    "  Paper: https://academic.oup.com/rasti/article/2/1/293/7202349  \n",
    "\n",
    "**Core references**\n",
    "- Hinton & Salakhutdinov (2006), *Reducing the Dimensionality of Data with Neural Networks* (autoencoders): https://www.cs.toronto.edu/~hinton/absps/science.pdf  \n",
    "- Kingma & Welling (2013), *Auto-Encoding Variational Bayes* (VAE concept): https://arxiv.org/abs/1312.6114  \n",
    "- McInnes et al. (2018), *UMAP* (visualisation): https://arxiv.org/abs/1802.03426  \n",
    "- PyTorch/Torchvision dataset docs: https://docs.pytorch.org/vision/main/datasets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fkej6vsonl",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cambridge-iccs/R2SKA_Advanced_Tutorial/blob/main/Session1A_Autoencoders_LatentSpaces.ipynb)\n\n---\n\n## Environment Setup (Colab / Local)\n\nRun the cell below to detect your environment and set up paths. On **Google Colab**, it will install required packages automatically.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2p88eqtahnr",
   "source": "# Detect environment and set up paths\nimport sys\n\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"Running in Google Colab\")\n    DATA_ROOT = '/content/data'\n    # Install required packages not available in Colab by default\n    !pip install -q umap-learn\nelse:\n    print(\"Running locally\")\n    DATA_ROOT = './data'\n\nprint(f\"Data directory: {DATA_ROOT}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ef9df2e7",
   "metadata": {},
   "source": "## 1. Imports\n\nThe cell below imports all required libraries. If running locally, ensure you have the `r2ska-tutorial` conda environment activated (see [README.md](README.md))."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06668c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass, fields\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c579f162",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Use the config below to control:\n",
    "- dataset choice (MNIST / Fashion-MNIST)\n",
    "- training size (subset for speed)\n",
    "- latent dimension\n",
    "- epochs and batch size\n",
    "\n",
    "**Tip:** start small (e.g. `max_train_samples=10000`, `epochs=3`) on CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5021d0",
   "metadata": {},
   "outputs": [],
   "source": "# Control the experiment via a dataclass\n@dataclass\nclass Config:\n    dataset: str = \"mnist\"       # \"mnist\" or \"fashion_mnist\"\n    data_dir: str = DATA_ROOT    # Uses environment-detected path\n    batch_size: int = 128\n    epochs: int = 5\n    lr: float = 1e-3\n    latent_dim: int = 16\n    max_train_samples: Optional[int] = 20000   # None for full set\n    max_test_samples: Optional[int] = 5000     # None for full set\n    seed: int = 42\n\n    # Formatted string representation for readable output\n    def __repr__(self):\n        lines = [f\"{self.__class__.__name__}:\"]\n        for f in fields(self):\n            lines.append(f\"  {f.name:20s} = {getattr(self, f.name)!r}\")\n        return \"\\n\".join(lines)\n\ncfg = Config()\n\n# Set a consistent random seed for reproducibility\nrandom.seed(cfg.seed)\nnp.random.seed(cfg.seed)\ntorch.manual_seed(cfg.seed)\n\n# Select accelerated device, if available: CUDA > MPS (Apple Silicon) > CPU\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(\"Device:\", device)\nprint(cfg)"
  },
  {
   "cell_type": "markdown",
   "id": "8a618bfa",
   "metadata": {},
   "source": [
    "## 3. Load dataset\n",
    "\n",
    "We use torchvision datasets for convenience. They download automatically into `data_dir`.\n",
    "Images are scaled to `[0, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da371c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(cfg: Config):\n",
    "    \"\"\"\n",
    "    Load datasets based on name specified in the configuration.\n",
    "    Train and test splits have already been separated and are returned as \n",
    "    separate datasets. The dataset format is a torchvision.datasets object.\n",
    "\n",
    "    Args:\n",
    "        cfg: Configuration object with dataset parameters.\n",
    "\n",
    "    Returns:\n",
    "        train_ds: Training dataset.\n",
    "        test_ds: Testing dataset.\n",
    "        n_classes: Number of classes in the dataset.\n",
    "    \"\"\"\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.ToTensor(),  # -> [0,1]\n",
    "    ])\n",
    "    if cfg.dataset.lower() == \"mnist\":\n",
    "        train_ds = datasets.MNIST(root=cfg.data_dir, train=True, download=True, transform=tfm)\n",
    "        test_ds  = datasets.MNIST(root=cfg.data_dir, train=False, download=True, transform=tfm)\n",
    "        n_classes = 10\n",
    "    elif cfg.dataset.lower() in [\"fashion_mnist\", \"fashion-mnist\", \"fmnist\"]:\n",
    "        train_ds = datasets.FashionMNIST(root=cfg.data_dir, train=True, download=True, transform=tfm)\n",
    "        test_ds  = datasets.FashionMNIST(root=cfg.data_dir, train=False, download=True, transform=tfm)\n",
    "        n_classes = 10\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset. Use 'mnist' or 'fashion_mnist'.\")\n",
    "    return train_ds, test_ds, n_classes\n",
    "\n",
    "train_ds, test_ds, n_classes = get_datasets(cfg)\n",
    "\n",
    "\n",
    "def select_subset(ds, max_n: Optional[int], seed: int = 0):\n",
    "    \"\"\"\n",
    "    Return a random subset of the dataset if max_n is specified and less than\n",
    "    the dataset size. Otherwise, return the original dataset.\n",
    "\n",
    "    Args:\n",
    "        ds: The original dataset.\n",
    "        max_n: Maximum number of samples to include in the subset.\n",
    "        seed: Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        A subset of the dataset or the original dataset.\n",
    "    \"\"\"\n",
    "    if max_n is None or max_n >= len(ds):\n",
    "        return ds\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = rng.choice(len(ds), size=max_n, replace=False)\n",
    "    return Subset(ds, idx)\n",
    "\n",
    "# Subsample the datasets if specified in the config\n",
    "train_ds_small = select_subset(train_ds, cfg.max_train_samples, seed=cfg.seed)\n",
    "test_ds_small  = select_subset(test_ds, cfg.max_test_samples, seed=cfg.seed + 1)\n",
    "\n",
    "# Create PyTorch DataLoaders that will provide batches during training/testing\n",
    "# num_workers=0 for compatibility (set higher if multiprocessing works on your system)\n",
    "train_loader = DataLoader(train_ds_small, batch_size=cfg.batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds_small,  batch_size=cfg.batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(\"Train samples:\", len(train_ds_small))\n",
    "print(\"Test samples: \", len(test_ds_small))\n",
    "\n",
    "# Peek at a few examples using matplotlib\n",
    "x0, y0 = next(iter(train_loader))\n",
    "print(\"Batch:\", x0.shape, y0.shape)\n",
    "fig, axes = plt.subplots(2, 8, figsize=(10, 3))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(x0[i,0].numpy(), cmap=\"gray\")\n",
    "    ax.set_title(int(y0[i]))\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f12db",
   "metadata": {},
   "source": [
    "## 4. Define a small convolutional autoencoder\n",
    "\n",
    "An autoencoder consists of two parts:\n",
    "- **Encoder**: compresses the input image into a compact latent vector `z`\n",
    "- **Decoder**: reconstructs the image from `z`\n",
    "\n",
    "### Architecture overview\n",
    "\n",
    "```\n",
    "INPUT IMAGE (1, 28, 28)\n",
    "         │\n",
    "         ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  ENCODER                                                │\n",
    "│                                                         │\n",
    "│  Conv2d(1→16, k=3, s=2, p=1)  →  (16, 14, 14)           │\n",
    "│  ReLU                                                   │\n",
    "│  Conv2d(16→32, k=3, s=2, p=1) →  (32, 7, 7)             │\n",
    "│  ReLU                                                   │\n",
    "│  Flatten                      →  (1568,)                │\n",
    "│  Linear(1568 → latent_dim)    →  (latent_dim,)          │\n",
    "│                                                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "         │\n",
    "         ▼\n",
    "    LATENT VECTOR z (latent_dim,)  ← \"bottleneck\"\n",
    "         │\n",
    "         ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  DECODER                                                │\n",
    "│                                                         │\n",
    "│  Linear(latent_dim → 1568)    →  (1568,)                │\n",
    "│  Reshape                      →  (32, 7, 7)             │\n",
    "│  ConvTranspose2d(32→16, k=4, s=2, p=1) → (16, 14, 14)   │\n",
    "│  ReLU                                                   │\n",
    "│  ConvTranspose2d(16→1, k=4, s=2, p=1)  → (1, 28, 28)    │\n",
    "│  Sigmoid                      →  output in [0, 1]       │\n",
    "│                                                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "         │\n",
    "         ▼\n",
    "RECONSTRUCTED IMAGE (1, 28, 28)\n",
    "```\n",
    "\n",
    "### Understanding the shapes\n",
    "\n",
    "**Encoder path:**\n",
    "1. Input: `(B, 1, 28, 28)` — batch of grayscale 28×28 images\n",
    "2. First conv with stride 2: spatial size halves → `(B, 16, 14, 14)`\n",
    "3. Second conv with stride 2: spatial size halves again → `(B, 32, 7, 7)`\n",
    "4. Flatten: 32 × 7 × 7 = **1568** features → `(B, 1568)`\n",
    "5. Linear layer projects to latent dimension → `(B, latent_dim)`\n",
    "\n",
    "**Latent vector `z`:**\n",
    "- Shape: `(B, latent_dim)` where `latent_dim=16` by default\n",
    "- This is the **bottleneck** — all information about the image is compressed here\n",
    "- Compression ratio: 784 pixels → 16 values ≈ **49× compression**\n",
    "\n",
    "**Decoder path (mirrors encoder):**\n",
    "1. Linear layer expands latent back to 1568 → `(B, 1568)`\n",
    "2. Reshape to spatial format → `(B, 32, 7, 7)`\n",
    "3. First transposed conv (upsampling): → `(B, 16, 14, 14)`\n",
    "4. Second transposed conv (upsampling): → `(B, 1, 28, 28)`\n",
    "5. Sigmoid ensures pixel values are in [0, 1]\n",
    "\n",
    "### Key design choices\n",
    "\n",
    "- **Stride 2** in convolutions does downsampling (encoder) and upsampling (decoder)\n",
    "- **ReLU activations** add non-linearity, allowing complex mappings\n",
    "- **Sigmoid output** matches the [0, 1] range of normalised input images\n",
    "- The architecture is intentionally small/fast — deeper networks with more channels would capture finer details but take longer to train\n",
    "\n",
    "This architecture works well for MNIST but would need to be deeper for more complex images like astronomical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e992c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Autoencoder for 28x28 grayscale images.\n",
    "\n",
    "    Args:\n",
    "        latent_dim: Dimension of the latent space.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the autoencoder architecture\n",
    "    def __init__(self, latent_dim: int = 16):\n",
    "        super().__init__()\n",
    "        # Input: (B, 1, 28, 28)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),  # -> (B,16,14,14)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1), # -> (B,32,7,7)\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.enc_fc = nn.Linear(32 * 7 * 7, latent_dim)\n",
    "\n",
    "        self.dec_fc = nn.Linear(latent_dim, 32 * 7 * 7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1),  # -> (B,16,14,14)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 4, stride=2, padding=1),   # -> (B,1,28,28)\n",
    "            nn.Sigmoid(),  # ensures output in [0,1]\n",
    "        )\n",
    "\n",
    "    # Encode input images to latent space\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        z = self.enc_fc(h)\n",
    "        return z\n",
    "\n",
    "    # Decode latent vectors back to images\n",
    "    def decode(self, z):\n",
    "        h = self.dec_fc(z)\n",
    "        h = h.view(h.size(0), 32, 7, 7)\n",
    "        x_hat = self.decoder(h)\n",
    "        return x_hat\n",
    "\n",
    "    # Full forward pass: encode then decode\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, z\n",
    "\n",
    "# Instantiate the model on the device\n",
    "model = ConvAutoencoder(latent_dim=cfg.latent_dim).to(device)\n",
    "print(model)\n",
    "\n",
    "# Set the loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=cfg.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de074d9b",
   "metadata": {},
   "source": [
    "## 5. Train the encoder\n",
    "\n",
    "We minimise reconstruction MSE between `x` and `x_hat`.\n",
    "\n",
    "On CPU, try:\n",
    "- `epochs=3–5`\n",
    "- `max_train_samples=10k–20k`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87182808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def run_epoch(model, loader, train: bool, desc: str = \"\"):\n",
    "    \"\"\"\n",
    "    Run one epoch of training or evaluation.\n",
    "\n",
    "    Args:\n",
    "        model: The autoencoder model.\n",
    "        loader: DataLoader providing the data.\n",
    "        train: If True, run in training mode; otherwise, evaluation mode.\n",
    "        desc: Description for the progress bar.\n",
    "\n",
    "    Returns:\n",
    "        Average loss over the epoch.     \n",
    "    \"\"\"\n",
    "    model.train(train) # NB: set train mode\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    # Progress bar for the epoch\n",
    "    pbar = tqdm(loader, desc=desc, leave=False)\n",
    "    for x, _y in pbar:\n",
    "        x = x.to(device)\n",
    "        if train:\n",
    "            opt.zero_grad(set_to_none=True)  # NB: reset gradients at start of step\n",
    "\n",
    "        # Forward pass\n",
    "        x_hat, _z = model(x)\n",
    "        loss = loss_fn(x_hat, x)\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        bs = x.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        n += bs\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix(loss=total_loss / n)\n",
    "    return total_loss / max(n, 1)\n",
    "\n",
    "# Record training history here\n",
    "history = {\"train_loss\": [], \"test_loss\": []}\n",
    "\n",
    "# Main training loop\n",
    "for epoch in tqdm(range(1, cfg.epochs + 1), desc=\"Training\"):\n",
    "    tr = run_epoch(model, train_loader, train=True, desc=f\"Epoch {epoch} [train]\")\n",
    "    te = run_epoch(model, test_loader, train=False, desc=f\"Epoch {epoch} [test]\")\n",
    "    history[\"train_loss\"].append(tr)\n",
    "    history[\"test_loss\"].append(te)\n",
    "    print(f\"Epoch {epoch:02d} | train {tr:.5f} | test {te:.5f}\")\n",
    "\n",
    "# Plot training and test loss over epochs\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history[\"train_loss\"], label=\"train\")\n",
    "plt.plot(history[\"test_loss\"], label=\"test\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507eba33",
   "metadata": {},
   "source": [
    "## 6. Visualise reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae21af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some reconstructions from the test set\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Get a batch of test data\n",
    "x, y = next(iter(test_loader))\n",
    "x = x.to(device)\n",
    "\n",
    "# Disable gradient calculations and get reconstructions\n",
    "with torch.no_grad():\n",
    "    # Forward pass to get reconstructions and latent representations\n",
    "    x_hat, z = model(x) \n",
    "\n",
    "# Move tensors to CPU for visualization\n",
    "x = x.cpu()\n",
    "x_hat = x_hat.cpu()\n",
    "\n",
    "# Plot 1st 12 originals and reconstructions\n",
    "n_show = 12\n",
    "fig, axes = plt.subplots(2, n_show, figsize=(12, 3))\n",
    "for i in range(n_show):\n",
    "    axes[0, i].imshow(x[i,0], cmap=\"gray\")\n",
    "    axes[0, i].set_title(f\"y={int(y[i])}\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "    axes[1, i].imshow(x_hat[i,0], cmap=\"gray\")\n",
    "    axes[1, i].set_title(\"recon\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Latent batch shape:\", z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f5179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot embedding vectors for multiple examples from different classes\n",
    "# This shows how different digits are encoded in the latent space\n",
    "\n",
    "z_cpu = z.cpu().numpy()\n",
    "y_cpu = y.numpy()\n",
    "\n",
    "# Find indices for different classes (up to 6 different digits)\n",
    "classes_to_show = []\n",
    "for c in range(10):\n",
    "    idx = np.where(y_cpu == c)[0]\n",
    "    if len(idx) > 0:\n",
    "        classes_to_show.append((c, idx[0]))\n",
    "    if len(classes_to_show) >= 6:\n",
    "        break\n",
    "\n",
    "n_examples = len(classes_to_show)\n",
    "fig, axes = plt.subplots(n_examples, 2, figsize=(12, 2 * n_examples), \n",
    "                         gridspec_kw={'width_ratios': [1, 4]})\n",
    "\n",
    "# Color map for consistent coloring across dimensions\n",
    "cmap = plt.cm.RdBu_r  # Red for negative, blue for positive\n",
    "\n",
    "for row, (class_label, idx) in enumerate(classes_to_show):\n",
    "    # Show the original image\n",
    "    axes[row, 0].imshow(x[idx, 0], cmap=\"gray\")\n",
    "    axes[row, 0].set_title(f\"Class {class_label}\", fontsize=11)\n",
    "    axes[row, 0].axis(\"off\")\n",
    "    \n",
    "    # Show the embedding vector as a bar chart\n",
    "    z_vec = z_cpu[idx]\n",
    "    colors = [cmap(0.5 + v / (2 * max(abs(z_vec.min()), abs(z_vec.max())) + 1e-6)) \n",
    "              for v in z_vec]\n",
    "    bars = axes[row, 1].bar(range(len(z_vec)), z_vec, width=0.8, color=colors, edgecolor='none')\n",
    "    axes[row, 1].axhline(0, color='black', linewidth=0.5)\n",
    "    axes[row, 1].set_xlim(-0.5, len(z_vec) - 0.5)\n",
    "    axes[row, 1].set_ylabel(\"Value\", fontsize=9)\n",
    "    \n",
    "    # Only show x-axis labels on bottom plot\n",
    "    if row == n_examples - 1:\n",
    "        axes[row, 1].set_xlabel(\"Latent dimension\", fontsize=10)\n",
    "        axes[row, 1].set_xticks(range(len(z_vec)))\n",
    "    else:\n",
    "        axes[row, 1].set_xticks([])\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle(f\"Embedding vectors ({cfg.latent_dim} dimensions) for different digit classes\", \n",
    "             fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEmbedding shape: {z.shape} — each image compressed from {28*28}={28*28} pixels to {cfg.latent_dim} values\")\n",
    "print(f\"Compression ratio: {28*28 / cfg.latent_dim:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0690bc1a",
   "metadata": {},
   "source": [
    "## 7. Extract embeddings and visualise\n",
    "\n",
    "We compute `z = encoder(x)` for the test set and visualise it in 2D with PCA and UMAP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b89e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_latents(model, loader):\n",
    "    \"\"\"\n",
    "    Collect latent representations and labels for the entire dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The autoencoder model.\n",
    "        loader: DataLoader providing the data.\n",
    "\n",
    "    Returns:\n",
    "        Z: Numpy array of latent representations.\n",
    "        Y: Numpy array of corresponding labels.\n",
    "    \"\"\"\n",
    "    model.eval() # set eval mode\n",
    "    Z = []\n",
    "    Y = []\n",
    "    with torch.no_grad(): # disable gradients\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            # Encode input to get latent representation\n",
    "            z = model.encode(x).cpu().numpy()\n",
    "            Z.append(z)\n",
    "            Y.append(y.numpy())\n",
    "    # Concatenate all batches\n",
    "    Z = np.concatenate(Z, axis=0)\n",
    "    Y = np.concatenate(Y, axis=0)\n",
    "    return Z, Y\n",
    "\n",
    "Z, Y = collect_latents(model, test_loader)\n",
    "print(\"Z:\", Z.shape, \"Y:\", Y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb74c8",
   "metadata": {},
   "source": [
    "\n",
    "- **PCA** gives a fast linear view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb528069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def scatter_2d(A2, labels, title: str, n_classes: int = 10):\n",
    "    \"\"\"Scatter plot with discrete colormap for class labels.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    \n",
    "    # Use tab10 qualitative colormap (designed for 10 distinct categories)\n",
    "    cmap = plt.cm.tab10\n",
    "    bounds = np.arange(n_classes + 1) - 0.5\n",
    "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "    \n",
    "    sc = ax.scatter(A2[:, 0], A2[:, 1], c=labels, cmap=cmap, norm=norm, s=8, alpha=0.7)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"dim 1\")\n",
    "    ax.set_ylabel(\"dim 2\")\n",
    "    \n",
    "    # Discrete colorbar with class labels\n",
    "    cbar = fig.colorbar(sc, ax=ax, ticks=np.arange(n_classes))\n",
    "    cbar.set_label(\"class\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Apply PCA to reduce latent space to 2D and plot\n",
    "pca = PCA(n_components=2, random_state=cfg.seed)\n",
    "Z_pca = pca.fit_transform(Z)\n",
    "scatter_2d(Z_pca, Y, f\"Latent space (PCA) — latent_dim={cfg.latent_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa082d2",
   "metadata": {},
   "source": [
    "- **UMAP** often reveals non-linear structure. Takes a minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ca2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "# n_jobs=1 required when random_state is set (reproducibility disables parallelism)\n",
    "reducer = umap.UMAP(n_components=2, random_state=cfg.seed, n_jobs=1)\n",
    "Z_umap = reducer.fit_transform(Z)\n",
    "scatter_2d(Z_umap, Y, f\"Latent space (UMAP) — latent_dim={cfg.latent_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6a04ba",
   "metadata": {},
   "source": [
    "## 8. Latent interpolation\n",
    "\n",
    "**Latent interpolation** is a powerful technique for probing what an autoencoder has learned. The idea is simple:\n",
    "\n",
    "1. Take two input images $x_A$ and $x_B$\n",
    "2. Encode them to latent vectors $z_A = \\text{encode}(x_A)$ and $z_B = \\text{encode}(x_B)$\n",
    "3. Create intermediate points along the line between them: $z_\\alpha = (1 - \\alpha) z_A + \\alpha z_B$ for $\\alpha \\in [0, 1]$\n",
    "4. Decode each intermediate point: $\\hat{x}_\\alpha = \\text{decode}(z_\\alpha)$\n",
    "\n",
    "### Interpreting the output\n",
    "\n",
    "- **Smooth transitions** between decoded images suggest the latent space is *continuous* and well-organised — nearby points in latent space correspond to semantically similar images.\n",
    "- **Abrupt changes** or artifacts indicate \"holes\" in the latent space where the decoder hasn't learned meaningful structure.\n",
    "- For digits, you might see a \"3\" gradually morph into an \"8\" through plausible intermediate forms.\n",
    "\n",
    "### Connection to generative models\n",
    "\n",
    "This property — that you can move smoothly through latent space and get coherent outputs — is foundational for:\n",
    "- **VAEs**: regularise the latent space to be smooth and sampleable\n",
    "- **GANs**: latent interpolation (\"latent walks\") reveal what the generator has learned\n",
    "- **Diffusion models**: operate in latent space for efficiency (Stable Diffusion)\n",
    "\n",
    "A well-trained autoencoder should produce smooth, semantically meaningful interpolations rather than noisy transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e41f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_latent_interpolation(model, ds, idx_a: int, idx_b: int, steps: int = 10):\n",
    "    \"\"\"\n",
    "    Show interpolation in latent space between two dataset samples.\n",
    "    \n",
    "    Args:\n",
    "        model: The autoencoder model.\n",
    "        ds: Dataset to sample from.\n",
    "        idx_a: Index of the first sample.\n",
    "        idx_b: Index of the second sample.\n",
    "        steps: Number of interpolation steps between the two samples.\n",
    "\n",
    "    Returns:\n",
    "        None (displays a plot).\n",
    "    \"\"\"\n",
    "    model.eval() # set eval mode\n",
    "\n",
    "    # Get the two samples\n",
    "    xa, ya = ds[idx_a]\n",
    "    xb, yb = ds[idx_b]\n",
    "    xa = xa.unsqueeze(0).to(device) # add batch dimension\n",
    "    xb = xb.unsqueeze(0).to(device)\n",
    "\n",
    "    # Compute latent representations\n",
    "    with torch.no_grad(): # disable gradients\n",
    "        za = model.encode(xa)\n",
    "        zb = model.encode(xb)\n",
    "\n",
    "        # Interpolate in latent space\n",
    "        alphas = torch.linspace(0, 1, steps, device=device).view(-1,1)\n",
    "        z_interp = (1 - alphas) * za + alphas * zb\n",
    "        x_interp = model.decode(z_interp).cpu()\n",
    "\n",
    "    # Plot the results\n",
    "    fig, axes = plt.subplots(1, steps + 2, figsize=(1.2*(steps+2), 2))\n",
    "    axes[0].imshow(xa.cpu()[0,0], cmap=\"gray\"); axes[0].set_title(f\"A ({int(ya)})\"); axes[0].axis(\"off\")\n",
    "    for i in range(steps):\n",
    "        axes[i+1].imshow(x_interp[i,0], cmap=\"gray\"); axes[i+1].set_title(f\"{i+1}\"); axes[i+1].axis(\"off\")\n",
    "    axes[-1].imshow(xb.cpu()[0,0], cmap=\"gray\"); axes[-1].set_title(f\"B ({int(yb)})\"); axes[-1].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Choose two random test indices and show interpolation\n",
    "idx_a = np.random.randint(0, len(test_ds_small))\n",
    "idx_b = np.random.randint(0, len(test_ds_small))\n",
    "print(\"Interpolating between indices:\", idx_a, idx_b)\n",
    "show_latent_interpolation(model, test_ds_small, idx_a, idx_b, steps=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c73898",
   "metadata": {},
   "source": [
    "## 9. Latent traversal (vary one dimension)\n",
    "\n",
    "**Latent traversals** (also called \"latent walks\" or \"dimension sweeps\") help us understand what each dimension of the latent space encodes.\n",
    "\n",
    "### How it works\n",
    "\n",
    "1. Encode an image to get its latent vector $z = [z_1, z_2, \\ldots, z_d]$\n",
    "2. Pick one dimension $i$ to vary\n",
    "3. Sweep $z_i$ across a range (e.g., $z_i \\pm 3$) while keeping all other dimensions fixed\n",
    "4. Decode each modified vector and observe how the output changes\n",
    "\n",
    "### What to look for\n",
    "\n",
    "- **Interpretable factors**: In a well-structured latent space, individual dimensions may capture meaningful attributes:\n",
    "  - For digits: stroke thickness, slant, width, loop size\n",
    "  - For faces: pose, lighting, expression, age\n",
    "  - For galaxies: morphology, orientation, brightness\n",
    "  \n",
    "- **Entangled vs disentangled**: \n",
    "  - *Disentangled*: each dimension controls one independent factor (ideal)\n",
    "  - *Entangled*: changing one dimension affects multiple attributes (common in standard autoencoders)\n",
    "\n",
    "- **Smooth changes**: Gradual visual changes indicate the decoder has learned a continuous mapping\n",
    "\n",
    "### Why this matters\n",
    "\n",
    "Latent traversals are a key diagnostic for:\n",
    "- **Interpretability**: understanding what the model has learned\n",
    "- **Controllable generation**: if dimensions are disentangled, you can edit specific attributes\n",
    "- **Debugging**: erratic outputs suggest the dimension encodes noise or the model is undertrained\n",
    "\n",
    "*Variational* auto Encoders (VAEs, later) tend to learn more disentangled representations than standard autoencoders, making traversals more interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8323d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_latent_traversal(model, ds, idx: int, dim: int = 0, n_steps: int = 11, span: float = 3.0):\n",
    "    \"\"\"\n",
    "    Show latent space traversal along a specified dimension for a given sample.\n",
    "    \n",
    "    Args:\n",
    "        model: The autoencoder model.\n",
    "        ds: Dataset to sample from.\n",
    "        idx: Index of the sample to traverse.\n",
    "        dim: Latent dimension to vary.\n",
    "        n_steps: Number of steps in the traversal.\n",
    "        span: Range to traverse around the original latent value.\n",
    "\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    model.eval() # set eval mode\n",
    "\n",
    "    # Get the sample\n",
    "    x, y = ds[idx]\n",
    "    x = x.unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Original latent representation\n",
    "        z0 = model.encode(x).squeeze(0)\n",
    "        # Sample values around the current coordinate\n",
    "        vals = torch.linspace(-span, span, n_steps, device=device) + z0[dim]\n",
    "        # Create latent vectors for traversal       \n",
    "        Zs = z0.repeat(n_steps, 1)\n",
    "        Zs[:, dim] = vals\n",
    "        # Decode the traversed latent vectors\n",
    "        xs = model.decode(Zs).cpu()\n",
    "\n",
    "    # Plot the results\n",
    "    fig, axes = plt.subplots(1, n_steps + 1, figsize=(1.2*(n_steps+1), 2))\n",
    "    axes[0].imshow(x.cpu()[0,0], cmap=\"gray\")\n",
    "    axes[0].set_title(f\"orig y={int(y)}\")\n",
    "    axes[0].axis(\"off\")\n",
    "    for i in range(n_steps):\n",
    "        axes[i+1].imshow(xs[i,0], cmap=\"gray\")\n",
    "        axes[i+1].set_title(f\"{vals[i].item():.1f}\")\n",
    "        axes[i+1].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Choose a random test index and latent dimension to traverse\n",
    "idx = np.random.randint(0, len(test_ds_small))\n",
    "dim = np.random.randint(0, cfg.latent_dim)\n",
    "print(\"Traversal idx:\", idx, \"dim:\", dim)\n",
    "show_latent_traversal(model, test_ds_small, idx=idx, dim=dim, n_steps=11, span=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4253a4e3",
   "metadata": {},
   "source": [
    "## 10. Embeddings and Foundation Models?\n",
    "\n",
    "A very common “foundation model” pattern is:\n",
    "\n",
    "1. Freeze an encoder that produces embeddings.\n",
    "2. Train a tiny model on top (linear probe / kNN).\n",
    "\n",
    "Here we do **kNN classification** in latent space as a quick check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b349994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Build a train embedding set for kNN\n",
    "Ztr, Ytr = collect_latents(model, DataLoader(train_ds_small, batch_size=cfg.batch_size, shuffle=False, num_workers=0))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(Ztr, Ytr)\n",
    "pred = knn.predict(Z)\n",
    "acc = accuracy_score(Y, pred)\n",
    "print(f\"kNN accuracy using latent embeddings: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7f6b5d",
   "metadata": {},
   "source": [
    "Now let's visualise the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0oqoyd7c3dug",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# --- Confusion matrix ---\n",
    "cm = confusion_matrix(Y, pred, labels=np.arange(n_classes))\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(n_classes))\n",
    "disp.plot(ax=ax, cmap=\"Blues\", colorbar=True)\n",
    "ax.set_title(f\"kNN Confusion Matrix (k=5, acc={acc:.3f})\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Per-class accuracy ---\n",
    "per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "bars = ax.bar(np.arange(n_classes), per_class_acc, color=\"steelblue\", edgecolor=\"black\")\n",
    "ax.axhline(acc, color=\"red\", linestyle=\"--\", label=f\"Overall acc: {acc:.3f}\")\n",
    "ax.set_xlabel(\"Class\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Per-Class Accuracy from kNN on Latent Embeddings\")\n",
    "ax.set_xticks(np.arange(n_classes))\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend()\n",
    "for i, v in enumerate(per_class_acc):\n",
    "    ax.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\", fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Examples: correct and incorrect classifications ---\n",
    "correct_mask = (pred == Y)\n",
    "incorrect_mask = ~correct_mask\n",
    "\n",
    "# Get indices for correct and incorrect predictions\n",
    "correct_idx = np.where(correct_mask)[0]\n",
    "incorrect_idx = np.where(incorrect_mask)[0]\n",
    "\n",
    "# Sample up to 8 of each\n",
    "n_examples = min(8, len(correct_idx), len(incorrect_idx)) if len(incorrect_idx) > 0 else min(8, len(correct_idx))\n",
    "\n",
    "fig, axes = plt.subplots(2, n_examples, figsize=(1.5 * n_examples, 3.5))\n",
    "if n_examples == 1:\n",
    "    axes = axes.reshape(2, 1)\n",
    "\n",
    "# Show correct predictions\n",
    "for i in range(n_examples):\n",
    "    idx = correct_idx[i]\n",
    "    # Find original image from test dataset\n",
    "    orig_idx = test_ds_small.indices[idx] if hasattr(test_ds_small, 'indices') else idx\n",
    "    img, _ = test_ds[orig_idx] if hasattr(test_ds_small, 'indices') else test_ds_small[idx]\n",
    "    axes[0, i].imshow(img[0], cmap=\"gray\")\n",
    "    axes[0, i].set_title(f\"True:{Y[idx]} Pred:{pred[idx]}\", fontsize=9, color=\"green\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "\n",
    "# Show incorrect predictions\n",
    "if len(incorrect_idx) > 0:\n",
    "    for i in range(min(n_examples, len(incorrect_idx))):\n",
    "        idx = incorrect_idx[i]\n",
    "        orig_idx = test_ds_small.indices[idx] if hasattr(test_ds_small, 'indices') else idx\n",
    "        img, _ = test_ds[orig_idx] if hasattr(test_ds_small, 'indices') else test_ds_small[idx]\n",
    "        axes[1, i].imshow(img[0], cmap=\"gray\")\n",
    "        axes[1, i].set_title(f\"True:{Y[idx]} Pred:{pred[idx]}\", fontsize=9, color=\"red\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "    # Hide unused subplots\n",
    "    for i in range(len(incorrect_idx), n_examples):\n",
    "        axes[1, i].axis(\"off\")\n",
    "else:\n",
    "    for i in range(n_examples):\n",
    "        axes[1, i].axis(\"off\")\n",
    "    axes[1, 0].text(0.5, 0.5, \"No misclassifications!\", ha=\"center\", va=\"center\", fontsize=12)\n",
    "\n",
    "axes[0, 0].set_ylabel(\"Correct\", fontsize=11)\n",
    "axes[1, 0].set_ylabel(\"Incorrect\", fontsize=11)\n",
    "plt.suptitle(\"kNN Classification Examples\", fontsize=12)\n",
    "#plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCorrect: {correct_mask.sum()} / {len(Y)} ({100*correct_mask.mean():.1f}%)\")\n",
    "print(f\"Misclassified: {incorrect_mask.sum()} / {len(Y)} ({100*incorrect_mask.mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96698c91",
   "metadata": {},
   "source": [
    "## 11. Variational Autoencoder (VAE)\n",
    "\n",
    "A **Variational Autoencoder (VAE)** learns a *probabilistic* latent space.  \n",
    "Instead of mapping an input to a single latent vector `z`, the encoder predicts a **distribution**:\n",
    "\n",
    "- mean: $\\mu(x)$\n",
    "- log-variance: $\\log \\sigma^2(x)$\n",
    "\n",
    "Then we sample using the **reparameterisation trick**:\n",
    "\n",
    "$$z = \\mu + \\sigma \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n",
    "\n",
    "Training minimises a combination of:\n",
    "1. **Reconstruction loss**: make the decoded output match the input\n",
    "2. **KL divergence**: keep the latent distribution close to a unit Gaussian\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}[\\text{recon}(x, \\hat{x})] + \\beta \\, D_{KL}(q(z|x) \\,||\\, \\mathcal{N}(0,I))$$\n",
    "\n",
    "### Advantages of VAEs\n",
    "- The latent space becomes **smooth** and **sampleable**\n",
    "- Interpolations tend to look more coherent\n",
    "- You can generate new samples by drawing $z \\sim \\mathcal{N}(0, I)$ and decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4ddbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model (small convolutional)\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, latent_dim: int = 16):\n",
    "        super().__init__()\n",
    "        # Encoder conv stack\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),  # (B,16,14,14)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1), # (B,32,7,7)\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.enc_fc_mu = nn.Linear(32 * 7 * 7, latent_dim)\n",
    "        self.enc_fc_logvar = nn.Linear(32 * 7 * 7, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec_fc = nn.Linear(latent_dim, 32 * 7 * 7)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1),  # (B,16,14,14)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 4, stride=2, padding=1),   # (B,1,28,28)\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.enc(x).view(x.size(0), -1)\n",
    "        mu = self.enc_fc_mu(h)\n",
    "        logvar = self.enc_fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.dec_fc(z).view(z.size(0), 32, 7, 7)\n",
    "        return self.dec(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mu, logvar, z\n",
    "\n",
    "vae = ConvVAE(latent_dim=cfg.latent_dim).to(device)\n",
    "vae_opt = torch.optim.Adam(vae.parameters(), lr=cfg.lr)\n",
    "\n",
    "def vae_loss(x_hat, x, mu, logvar, beta: float = 1.0):\n",
    "    # Reconstruction: sum over pixels (not mean) to balance with KL\n",
    "    recon = torch.sum((x_hat - x) ** 2, dim=(1, 2, 3))  # per-sample, summed over pixels\n",
    "    # KL divergence: sum over latent dimensions\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    # Average over batch\n",
    "    loss = torch.mean(recon + beta * kl)\n",
    "    return loss, recon.mean().item(), kl.mean().item()\n",
    "\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47170cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VAE with KL annealing\n",
    "vae_history = {\"loss\": [], \"recon\": [], \"kl\": [], \"beta\": []}\n",
    "\n",
    "beta_max = 1.0      # final beta value\n",
    "warmup_epochs = 2   # epochs to linearly ramp beta from 0 to beta_max\n",
    "\n",
    "for epoch in tqdm(range(1, cfg.epochs + 1), desc=\"VAE Training\"):\n",
    "    # KL annealing: linearly increase beta during warmup\n",
    "    if epoch <= warmup_epochs:\n",
    "        beta = beta_max * (epoch / warmup_epochs)\n",
    "    else:\n",
    "        beta = beta_max\n",
    "    \n",
    "    vae.train(True)\n",
    "    total_loss = total_recon = total_kl = 0.0\n",
    "    n = 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch} (β={beta:.2f})\", leave=False)\n",
    "    for x, _y in pbar:\n",
    "        x = x.to(device)\n",
    "        vae_opt.zero_grad(set_to_none=True)\n",
    "        x_hat, mu, logvar, _z = vae(x)\n",
    "        loss, recon_m, kl_m = vae_loss(x_hat, x, mu, logvar, beta=beta)\n",
    "        loss.backward()\n",
    "        vae_opt.step()\n",
    "\n",
    "        bs = x.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_recon += recon_m * bs\n",
    "        total_kl += kl_m * bs\n",
    "        n += bs\n",
    "        pbar.set_postfix(loss=total_loss / n, recon=total_recon / n, kl=total_kl / n)\n",
    "\n",
    "    vae_history[\"loss\"].append(total_loss / n)\n",
    "    vae_history[\"recon\"].append(total_recon / n)\n",
    "    vae_history[\"kl\"].append(total_kl / n)\n",
    "    vae_history[\"beta\"].append(beta)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | β={beta:.2f} | loss {vae_history['loss'][-1]:.1f} | recon {vae_history['recon'][-1]:.1f} | kl {vae_history['kl'][-1]:.1f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axes[0].plot(vae_history[\"loss\"], label=\"total\")\n",
    "axes[0].plot(vae_history[\"recon\"], label=\"recon\")\n",
    "axes[0].set_xlabel(\"epoch\")\n",
    "axes[0].set_ylabel(\"loss\")\n",
    "axes[0].legend()\n",
    "axes[0].set_title(\"VAE losses\")\n",
    "\n",
    "axes[1].plot(vae_history[\"kl\"], label=\"KL\", color=\"green\")\n",
    "ax2 = axes[1].twinx()\n",
    "ax2.plot(vae_history[\"beta\"], label=\"β\", color=\"orange\", linestyle=\"--\")\n",
    "axes[1].set_xlabel(\"epoch\")\n",
    "axes[1].set_ylabel(\"KL divergence\", color=\"green\")\n",
    "ax2.set_ylabel(\"β\", color=\"orange\")\n",
    "axes[1].set_title(\"KL divergence & β schedule\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstructions (VAE)\n",
    "vae.eval()\n",
    "x, y = next(iter(test_loader))\n",
    "x = x.to(device)\n",
    "with torch.no_grad():\n",
    "    x_hat, mu, logvar, z = vae(x)\n",
    "\n",
    "x = x.cpu()\n",
    "x_hat = x_hat.cpu()\n",
    "n_show = 12\n",
    "\n",
    "fig, axes = plt.subplots(2, n_show, figsize=(12, 3))\n",
    "for i in range(n_show):\n",
    "    axes[0, i].imshow(x[i,0], cmap=\"gray\")\n",
    "    axes[0, i].set_title(f\"y={int(y[i])}\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "    axes[1, i].imshow(x_hat[i,0], cmap=\"gray\")\n",
    "    axes[1, i].set_title(\"recon\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Latent batch shape:\", z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc3249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling: generate new images by sampling z ~ N(0, I)\n",
    "vae.eval()\n",
    "n_gen = 16\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(n_gen, cfg.latent_dim, device=device)\n",
    "    x_gen = vae.decode(z).cpu()\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(10, 3))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(x_gen[i,0], cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"VAE samples (z ~ N(0, I))\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7a699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent interpolation (VAE) — uses the *mean* mu for stability\n",
    "def show_vae_interpolation(vae, ds, idx_a: int, idx_b: int, steps: int = 10):\n",
    "    vae.eval()\n",
    "    xa, ya = ds[idx_a]\n",
    "    xb, yb = ds[idx_b]\n",
    "    xa = xa.unsqueeze(0).to(device)\n",
    "    xb = xb.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mua, _lva = vae.encode(xa)\n",
    "        mub, _lvb = vae.encode(xb)\n",
    "\n",
    "        alphas = torch.linspace(0, 1, steps, device=device).view(-1,1)\n",
    "        z_interp = (1 - alphas) * mua + alphas * mub\n",
    "        x_interp = vae.decode(z_interp).cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(1, steps + 2, figsize=(1.2*(steps+2), 2))\n",
    "    axes[0].imshow(xa.cpu()[0,0], cmap=\"gray\"); axes[0].set_title(f\"A ({int(ya)})\"); axes[0].axis(\"off\")\n",
    "    for i in range(steps):\n",
    "        axes[i+1].imshow(x_interp[i,0], cmap=\"gray\"); axes[i+1].set_title(f\"{i+1}\"); axes[i+1].axis(\"off\")\n",
    "    axes[-1].imshow(xb.cpu()[0,0], cmap=\"gray\"); axes[-1].set_title(f\"B ({int(yb)})\"); axes[-1].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "idx_a = np.random.randint(0, len(test_ds_small))\n",
    "idx_b = np.random.randint(0, len(test_ds_small))\n",
    "print(\"VAE interpolation between indices:\", idx_a, idx_b)\n",
    "show_vae_interpolation(vae, test_ds_small, idx_a, idx_b, steps=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7b479",
   "metadata": {},
   "source": [
    "## Exercise: Apply to MiraBest Radio Galaxies\n",
    "\n",
    "Now apply what you've learned to a real astronomy dataset!\n",
    "\n",
    "**MiraBest** is a labelled dataset of Fanaroff-Riley (FR) radio galaxies from the FIRST survey:\n",
    "- **FRI** (class 0): edge-darkened — jets fade with distance from the core\n",
    "- **FRII** (class 1): edge-brightened — jets terminate in bright hotspots\n",
    "- ~800 images total (150×150 grayscale)\n",
    "- Zenodo: https://doi.org/10.5281/zenodo.4288837\n",
    "\n",
    "### Data format\n",
    "\n",
    "MiraBest uses a **CIFAR-style pickle format** (not PNG files):\n",
    "- Download `batches.tar.gz` (~136 MB) from Zenodo\n",
    "- Extract to get `data_batch_1`, `data_batch_2`, ... files\n",
    "- Each batch is a Python pickle containing:\n",
    "  - `data`: list of 150×150 numpy arrays\n",
    "  - `labels`: list of class labels (0=FRI, 1=FRII)\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. **Download and load MiraBest** — extract the tarball and load pickle batches\n",
    "\n",
    "2. **Create a Dataset class** that:\n",
    "   - Loads images from pickle batches\n",
    "   - Resizes 150×150 images to 64×64 for the autoencoder\n",
    "\n",
    "3. **Adapt the autoencoder** for 64×64 images\n",
    "   - Hint: after two stride-2 convolutions, you get 64→32→16\n",
    "   - Flattened size becomes 32 × 16 × 16 = 8192\n",
    "\n",
    "4. **Train and visualise**:\n",
    "   - Reconstructions — do jets and lobes appear?\n",
    "   - PCA/UMAP of latent space coloured by FR class\n",
    "   - Latent interpolation between an FRI and FRII galaxy\n",
    "\n",
    "5. **Evaluate with kNN** — can embeddings distinguish FR classes?\n",
    "   - Random baseline is 50% (binary classification)\n",
    "\n",
    "### Solution\n",
    "\n",
    "See **Session1A_Extension_MiraBest.ipynb** for a complete worked solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r2ska-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}