{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb2c077",
   "metadata": {},
   "source": [
    "# Practical 1B - Extension: Autoencoders on MiraBest Radio Galaxies\n",
    "\n",
    "**Road to SKA: Foundation Models, Embeddings, and Latent Spaces**\n",
    "\n",
    "This notebook applies the autoencoder techniques from Session 1A to **MiraBest**,\n",
    "a labelled dataset of Fanaroff–Riley (FR) radio galaxies.\n",
    "\n",
    "1. Download and prepare the MiraBest dataset from Zenodo\n",
    "2. Adapt the convolutional autoencoder for 64×64 radio galaxy images\n",
    "3. Visualise the latent space and observe clustering by FR morphology\n",
    "4. Perform latent interpolation between FRI and FRII galaxies\n",
    "5. Evaluate embeddings using kNN classification\n",
    "\n",
    "---\n",
    "\n",
    "## About MiraBest\n",
    "\n",
    "**MiraBest** (Miraghaei + Best) is a labelled dataset of ~800 radio galaxy images from the FIRST survey, classified by Fanaroff-Riley morphology:\n",
    "\n",
    "- **FRI (class 0)**: Edge-darkened — jets fade with distance from the core\n",
    "- **FRII (class 1)**: Edge-brightened — jets terminate in bright hotspots\n",
    "\n",
    "This classification is fundamental in radio astronomy as it relates to the power and environment of active galactic nuclei.\n",
    "\n",
    "The dataset uses a CIFAR-style pickle format with 150×150 grayscale images.\n",
    "\n",
    "**References:**\n",
    "- Zenodo dataset: https://doi.org/10.5281/zenodo.4288837\n",
    "- Paper: https://academic.oup.com/rasti/article/2/1/293/7202349"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w5qexrs90b",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Road2SKA/Advanced_ML_Tutorial_Latent/blob/colab/Session1B_Extension_MiraBest.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Setup (Colab / Local)\n",
    "\n",
    "Run the cell below to detect your environment and set up paths. On **Google Colab**, it will install required packages automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l6p66onae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect environment and set up paths\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    DATA_ROOT = '/content/data'\n",
    "    # Install required packages not available in Colab by default\n",
    "    !pip install -q umap-learn\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    DATA_ROOT = './data'\n",
    "\n",
    "print(f\"Data directory: {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9df2e7",
   "metadata": {},
   "source": [
    "## 1. Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06668c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, fields, field\n",
    "from typing import Tuple, Optional, List\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c579f162",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "MiraBest images are 150×150 pixels. We resize to 64×64 for faster training while preserving structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5021d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    data_dir: str = field(default_factory=lambda: f\"{DATA_ROOT}/mirabest\")  # Deferred evaluation for Colab\n",
    "    image_size: int = 64          # resize images to 64x64\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 30              # more epochs for better convergence\n",
    "    lr: float = 5e-4              # slightly lower learning rate\n",
    "    latent_dim: int = 64          # larger latent space for richer representations\n",
    "    test_fraction: float = 0.2   # train/test split\n",
    "    seed: int = 42\n",
    "    use_augmentation: bool = True  # enable data augmentation\n",
    "\n",
    "    def __repr__(self):\n",
    "        lines = [f\"{self.__class__.__name__}:\"]\n",
    "        for f in fields(self):\n",
    "            lines.append(f\"  {f.name:20s} = {getattr(self, f.name)!r}\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "\n",
    "# Select device: CUDA > MPS (Apple Silicon) > CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a618bfa",
   "metadata": {},
   "source": [
    "## 3. Download MiraBest\n",
    "\n",
    "The dataset is hosted on Zenodo as `batches.tar.gz` (~18 MB). It uses a CIFAR-style pickle format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da371c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIRABEST_TAR_URL = \"https://zenodo.org/records/4288837/files/batches.tar.gz?download=1\"\n",
    "\n",
    "def download_with_retries(url: str, dst: Path, retries: int = 5, chunk_size: int = 1 << 20):\n",
    "    \"\"\"\n",
    "    Download a file from URL with retry logic and progress bar.\n",
    "    \"\"\"\n",
    "    dst = Path(dst)\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if dst.exists() and dst.stat().st_size > 0:\n",
    "        print(f\"File already exists: {dst}\")\n",
    "        return\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            print(f\"Downloading {dst.name} (attempt {attempt + 1}/{retries})...\")\n",
    "            with requests.get(url, stream=True, timeout=120) as r:\n",
    "                r.raise_for_status()\n",
    "                total_size = int(r.headers.get('content-length', 0))\n",
    "                \n",
    "                with open(dst, \"wb\") as f:\n",
    "                    with tqdm(total=total_size, unit='B', unit_scale=True, desc=dst.name) as pbar:\n",
    "                        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                            if chunk:\n",
    "                                f.write(chunk)\n",
    "                                pbar.update(len(chunk))\n",
    "            print(f\"Downloaded: {dst}\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Download failed (attempt {attempt + 1}/{retries}): {repr(e)}\")\n",
    "            if dst.exists():\n",
    "                dst.unlink()\n",
    "    \n",
    "    raise RuntimeError(\n",
    "        f\"Could not download MiraBest after {retries} attempts.\\n\"\n",
    "        f\"You can manually download from Zenodo and place batches.tar.gz in {cfg.data_dir}/\"\n",
    "    )\n",
    "\n",
    "# Download the tarball\n",
    "data_dir = Path(cfg.data_dir)\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "tar_path = data_dir / \"batches.tar.gz\"\n",
    "\n",
    "download_with_retries(MIRABEST_TAR_URL, tar_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract_tar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the tarball\n",
    "extract_dir = data_dir / \"batches\"\n",
    "\n",
    "# Check for the nested batches/batches structure or data_batch files\n",
    "batches_inner = extract_dir / \"batches\"\n",
    "if batches_inner.exists() and (batches_inner / \"data_batch_1\").exists():\n",
    "    batches_path = batches_inner\n",
    "    print(f\"Using existing extraction: {batches_path}\")\n",
    "elif (extract_dir / \"data_batch_1\").exists():\n",
    "    batches_path = extract_dir\n",
    "    print(f\"Using existing extraction: {batches_path}\")\n",
    "else:\n",
    "    print(f\"Extracting {tar_path.name}...\")\n",
    "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=extract_dir)\n",
    "    \n",
    "    # Handle nested extraction\n",
    "    if (extract_dir / \"batches\" / \"data_batch_1\").exists():\n",
    "        batches_path = extract_dir / \"batches\"\n",
    "    else:\n",
    "        batches_path = extract_dir\n",
    "    print(f\"Extracted to: {batches_path}\")\n",
    "\n",
    "# Verify extraction\n",
    "batch_files = sorted(batches_path.glob(\"data_batch_*\"))\n",
    "print(f\"Found {len(batch_files)} data batch files\")\n",
    "print(f\"Test batch exists: {(batches_path / 'test_batch').exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f12db",
   "metadata": {},
   "source": [
    "## 4. Load MiraBest Data\n",
    "\n",
    "MiraBest uses a CIFAR-style pickle format:\n",
    "- `data`: list of 150×150 numpy arrays\n",
    "- `labels`: list of class labels (0=FRI, 1=FRII)\n",
    "- `filenames`: original source filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e992c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class names from the dataset\n",
    "CLASS_NAMES = [\"FRI\", \"FRII\"]\n",
    "\n",
    "def load_mirabest_batch(batch_path: Path):\n",
    "    \"\"\"\n",
    "    Load a single MiraBest batch file.\n",
    "    \n",
    "    Returns:\n",
    "        images: list of numpy arrays (150x150)\n",
    "        labels: list of integer labels\n",
    "    \"\"\"\n",
    "    with open(batch_path, 'rb') as f:\n",
    "        batch = pickle.load(f, encoding='bytes')\n",
    "    \n",
    "    # Handle both string and bytes keys\n",
    "    if b'data' in batch:\n",
    "        images = batch[b'data']\n",
    "        labels = batch[b'labels']\n",
    "    else:\n",
    "        images = batch['data']\n",
    "        labels = batch['labels']\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def load_all_mirabest(batches_path: Path, include_test: bool = True):\n",
    "    \"\"\"\n",
    "    Load all MiraBest data from batch files.\n",
    "    \n",
    "    Returns:\n",
    "        images: numpy array of shape (N, 150, 150)\n",
    "        labels: numpy array of shape (N,)\n",
    "    \"\"\"\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Load training batches\n",
    "    for i in range(1, 9):  # data_batch_1 through data_batch_8\n",
    "        batch_path = batches_path / f\"data_batch_{i}\"\n",
    "        if batch_path.exists():\n",
    "            images, labels = load_mirabest_batch(batch_path)\n",
    "            all_images.extend(images)\n",
    "            all_labels.extend(labels)\n",
    "            print(f\"Loaded {batch_path.name}: {len(images)} images\")\n",
    "    \n",
    "    # Optionally load test batch\n",
    "    if include_test:\n",
    "        test_path = batches_path / \"test_batch\"\n",
    "        if test_path.exists():\n",
    "            images, labels = load_mirabest_batch(test_path)\n",
    "            all_images.extend(images)\n",
    "            all_labels.extend(labels)\n",
    "            print(f\"Loaded test_batch: {len(images)} images\")\n",
    "    \n",
    "    return np.array(all_images), np.array(all_labels)\n",
    "\n",
    "\n",
    "# Load all data\n",
    "images_raw, labels = load_all_mirabest(batches_path)\n",
    "\n",
    "print(f\"\\nTotal: {len(images_raw)} images\")\n",
    "print(f\"Image shape: {images_raw[0].shape}\")\n",
    "print(f\"Label range: {labels.min()} to {labels.max()}\")\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "for i, name in enumerate(CLASS_NAMES):\n",
    "    count = (labels == i).sum()\n",
    "    print(f\"  {name} (class {i}): {count} images ({100*count/len(labels):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resize_images",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize images and apply preprocessing\n",
    "def preprocess_images(images, target_size, use_log_scale=True):\n",
    "    \"\"\"\n",
    "    Preprocess images: resize and normalize.\n",
    "    \n",
    "    Radio astronomy images often have high dynamic range (bright cores, faint lobes).\n",
    "    Log-scaling helps compress this range and makes faint features more visible.\n",
    "    \n",
    "    Args:\n",
    "        images: numpy array of shape (N, H, W)\n",
    "        target_size: target size (height, width)\n",
    "        use_log_scale: if True, apply asinh (soft log) scaling\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed images as numpy array of shape (N, target_size, target_size)\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for img in tqdm(images, desc=\"Preprocessing images\"):\n",
    "        # Resize using PIL\n",
    "        pil_img = Image.fromarray(img)\n",
    "        pil_img = pil_img.resize((target_size, target_size), Image.BILINEAR)\n",
    "        arr = np.array(pil_img, dtype=np.float32)\n",
    "        \n",
    "        if use_log_scale:\n",
    "            # asinh scaling: like log but handles zeros and negatives\n",
    "            # This compresses bright pixels while preserving faint structure\n",
    "            arr = np.arcsinh(arr / 10.0)  # 10.0 is a softening parameter\n",
    "        \n",
    "        processed.append(arr)\n",
    "    \n",
    "    processed = np.array(processed)\n",
    "    \n",
    "    # Normalize to [0, 1] range per-dataset (not per-image, for consistency)\n",
    "    vmin, vmax = processed.min(), processed.max()\n",
    "    processed = (processed - vmin) / (vmax - vmin + 1e-8)\n",
    "    \n",
    "    return processed\n",
    "\n",
    "\n",
    "# Preprocess images with log-scaling for better dynamic range handling\n",
    "images = preprocess_images(images_raw, cfg.image_size, use_log_scale=True)\n",
    "print(f\"Preprocessed images shape: {images.shape}\")\n",
    "print(f\"Value range: [{images.min():.3f}, {images.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from each class\n",
    "fig, axes = plt.subplots(2, 8, figsize=(14, 4))\n",
    "\n",
    "for class_idx in range(2):\n",
    "    class_mask = labels == class_idx\n",
    "    class_images = images[class_mask]\n",
    "    \n",
    "    for j in range(8):\n",
    "        if j < len(class_images):\n",
    "            axes[class_idx, j].imshow(class_images[j], cmap=\"hot\")\n",
    "        axes[class_idx, j].axis(\"off\")\n",
    "        if j == 0:\n",
    "            axes[class_idx, j].set_ylabel(CLASS_NAMES[class_idx], fontsize=12)\n",
    "\n",
    "plt.suptitle(\"MiraBest Radio Galaxy Samples by FR Class\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de074d9b",
   "metadata": {},
   "source": [
    "## 5. Create PyTorch Dataset and Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiraBestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for MiraBest radio galaxy images.\n",
    "    \n",
    "    Supports data augmentation: rotations and flips.\n",
    "    Radio galaxies have no preferred orientation, so these are physically valid.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, images: np.ndarray, labels: np.ndarray, augment: bool = False):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx].copy()  # copy to avoid modifying original\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Data augmentation (training only)\n",
    "        if self.augment:\n",
    "            # Random rotation by 0, 90, 180, or 270 degrees\n",
    "            k = np.random.randint(0, 4)\n",
    "            img = np.rot90(img, k)\n",
    "            \n",
    "            # Random horizontal flip\n",
    "            if np.random.random() > 0.5:\n",
    "                img = np.fliplr(img)\n",
    "            \n",
    "            # Random vertical flip\n",
    "            if np.random.random() > 0.5:\n",
    "                img = np.flipud(img)\n",
    "        \n",
    "        # Ensure contiguous array for torch\n",
    "        img = np.ascontiguousarray(img)\n",
    "        \n",
    "        # Convert to tensor with channel dimension: (1, H, W)\n",
    "        tensor = torch.from_numpy(img).unsqueeze(0).float()\n",
    "        \n",
    "        return tensor, label\n",
    "\n",
    "\n",
    "# Create stratified train/test split\n",
    "indices = np.arange(len(images))\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    indices,\n",
    "    test_size=cfg.test_fraction,\n",
    "    stratify=labels,\n",
    "    random_state=cfg.seed\n",
    ")\n",
    "\n",
    "# Create datasets (augmentation only for training)\n",
    "train_dataset = MiraBestDataset(images[train_idx], labels[train_idx], augment=cfg.use_augmentation)\n",
    "test_dataset = MiraBestDataset(images[test_idx], labels[test_idx], augment=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)} (augmentation: {cfg.use_augmentation})\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507eba33",
   "metadata": {},
   "source": [
    "## 6. Convolutional Autoencoder for 64×64 Images\n",
    "\n",
    "We adapt the architecture from Session 1 for 64×64 images:\n",
    "- After two stride-2 convolutions: 64 → 32 → 16\n",
    "- Flattened size: 32 channels × 16 × 16 = 8192\n",
    "- Batch normalisation to help train better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae21af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder64(nn.Module):\n",
    "    \"\"\"\n",
    "    Deeper Convolutional Autoencoder for 64×64 grayscale images.\n",
    "    \n",
    "    Architecture improvements over basic version:\n",
    "    - 3 conv layers instead of 2 for richer feature extraction\n",
    "    - BatchNorm for training stability\n",
    "    - More channels (32, 64, 128) to capture complex patterns\n",
    "    \n",
    "    Args:\n",
    "        latent_dim: Dimension of the latent space\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim: int = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder: (B, 1, 64, 64) -> (B, latent_dim)\n",
    "        # Conv layers with BatchNorm for stable training\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=2, padding=1),    # -> (B, 32, 32, 32)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),   # -> (B, 64, 16, 16)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),  # -> (B, 128, 8, 8)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Flattened: 128 * 8 * 8 = 8192\n",
    "        self.enc_fc = nn.Linear(128 * 8 * 8, latent_dim)\n",
    "        \n",
    "        # Decoder: (B, latent_dim) -> (B, 1, 64, 64)\n",
    "        self.dec_fc = nn.Linear(latent_dim, 128 * 8 * 8)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # -> (B, 64, 16, 16)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),   # -> (B, 32, 32, 32)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 4, stride=2, padding=1),    # -> (B, 1, 64, 64)\n",
    "            nn.Sigmoid(),  # Output in [0, 1]\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input images to latent vectors.\"\"\"\n",
    "        h = self.encoder(x)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        z = self.enc_fc(h)\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent vectors back to images.\"\"\"\n",
    "        h = self.dec_fc(z)\n",
    "        h = h.view(h.size(0), 128, 8, 8)\n",
    "        x_hat = self.decoder(h)\n",
    "        return x_hat\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Full forward pass: encode then decode.\"\"\"\n",
    "        z = self.encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, z\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "model = ConvAutoencoder64(latent_dim=cfg.latent_dim).to(device)\n",
    "print(model)\n",
    "\n",
    "# Loss function and optimizer with weight decay for regularization\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler - reduce LR when loss plateaus\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0690bc1a",
   "metadata": {},
   "source": [
    "## 7. Train the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b89e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, train: bool, desc: str = \"\"):\n",
    "    \"\"\"\n",
    "    Run one epoch of training or evaluation.\n",
    "    \"\"\"\n",
    "    model.train(train)\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=desc, leave=False)\n",
    "    for x, _y in pbar:\n",
    "        x = x.to(device)\n",
    "        \n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Forward pass\n",
    "        x_hat, _z = model(x)\n",
    "        loss = loss_fn(x_hat, x)\n",
    "        \n",
    "        # Backward pass\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Accumulate\n",
    "        bs = x.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        n += bs\n",
    "        pbar.set_postfix(loss=total_loss / n)\n",
    "    \n",
    "    return total_loss / max(n, 1)\n",
    "\n",
    "\n",
    "# Training loop with LR scheduler\n",
    "history = {\"train_loss\": [], \"test_loss\": [], \"lr\": []}\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "patience_counter = 0\n",
    "early_stop_patience = 15\n",
    "\n",
    "for epoch in tqdm(range(1, cfg.epochs + 1), desc=\"Training\"):\n",
    "    tr_loss = run_epoch(model, train_loader, train=True, desc=f\"Epoch {epoch} [train]\")\n",
    "    te_loss = run_epoch(model, test_loader, train=False, desc=f\"Epoch {epoch} [test]\")\n",
    "    \n",
    "    # Update scheduler based on test loss\n",
    "    scheduler.step(te_loss)\n",
    "    \n",
    "    # Record history\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    history[\"train_loss\"].append(tr_loss)\n",
    "    history[\"test_loss\"].append(te_loss)\n",
    "    history[\"lr\"].append(current_lr)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if te_loss < best_test_loss:\n",
    "        best_test_loss = te_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:02d} | train {tr_loss:.5f} | test {te_loss:.5f} | lr {current_lr:.2e}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= early_stop_patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch} (no improvement for {early_stop_patience} epochs)\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest test loss: {best_test_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history[\"train_loss\"], label=\"Train\", linewidth=2)\n",
    "plt.plot(history[\"test_loss\"], label=\"Test\", linewidth=2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Autoencoder Training on MiraBest\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb74c8",
   "metadata": {},
   "source": [
    "## 8. Visualize Reconstructions\n",
    "\n",
    "Compare original radio galaxy images with their reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb528069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch for visualization\n",
    "model.eval()\n",
    "x, y = next(iter(test_loader))\n",
    "x = x.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_hat, z = model(x)\n",
    "\n",
    "x = x.cpu()\n",
    "x_hat = x_hat.cpu()\n",
    "\n",
    "# Plot originals and reconstructions\n",
    "n_show = min(10, len(x))\n",
    "fig, axes = plt.subplots(2, n_show, figsize=(15, 3.5))\n",
    "\n",
    "for i in range(n_show):\n",
    "    # Original\n",
    "    axes[0, i].imshow(x[i, 0].numpy(), cmap=\"hot\")\n",
    "    axes[0, i].set_title(CLASS_NAMES[y[i]], fontsize=10)\n",
    "    axes[0, i].axis(\"off\")\n",
    "    \n",
    "    # Reconstruction\n",
    "    axes[1, i].imshow(x_hat[i, 0].numpy(), cmap=\"hot\")\n",
    "    axes[1, i].set_title(\"recon\", fontsize=10)\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "axes[0, 0].set_ylabel(\"Original\", fontsize=11)\n",
    "axes[1, 0].set_ylabel(\"Reconstructed\", fontsize=11)\n",
    "plt.suptitle(\"Original vs Reconstructed Radio Galaxies\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Latent vector shape: {z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa082d2",
   "metadata": {},
   "source": [
    "## 9. Extract Embeddings and Visualize Latent Space\n",
    "\n",
    "We extract latent representations for the entire test set and visualize with PCA and UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ca2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_latents(model, loader):\n",
    "    \"\"\"\n",
    "    Collect latent representations for all samples in a DataLoader.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    Z, Y = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            z = model.encode(x).cpu().numpy()\n",
    "            Z.append(z)\n",
    "            Y.append(y.numpy())\n",
    "    \n",
    "    return np.concatenate(Z), np.concatenate(Y)\n",
    "\n",
    "\n",
    "# Collect embeddings\n",
    "Z_test, Y_test = collect_latents(model, test_loader)\n",
    "Z_train, Y_train = collect_latents(model, train_loader)\n",
    "\n",
    "print(f\"Test embeddings: {Z_test.shape}\")\n",
    "print(f\"Train embeddings: {Z_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA visualization\n",
    "def scatter_fr_classes(Z2, Y, title: str):\n",
    "    \"\"\"Scatter plot with FR class colors.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e']  # Blue, Orange\n",
    "    \n",
    "    for i, name in enumerate(CLASS_NAMES):\n",
    "        mask = Y == i\n",
    "        if mask.sum() > 0:\n",
    "            ax.scatter(Z2[mask, 0], Z2[mask, 1], c=colors[i], label=name, \n",
    "                      s=40, alpha=0.7, edgecolors='white', linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Apply PCA to test embeddings\n",
    "pca = PCA(n_components=2, random_state=cfg.seed)\n",
    "Z_pca = pca.fit_transform(Z_test)\n",
    "\n",
    "print(f\"PCA explained variance: {pca.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "scatter_fr_classes(Z_pca, Y_test, f\"MiraBest Latent Space (PCA) — latent_dim={cfg.latent_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "umap_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reducer = umap.UMAP(n_components=2, random_state=cfg.seed, n_neighbors=15, min_dist=0.1, n_jobs=1)\n",
    "Z_umap = reducer.fit_transform(Z_test)\n",
    "\n",
    "scatter_fr_classes(Z_umap, Y_test, f\"MiraBest Latent Space (UMAP) — latent_dim={cfg.latent_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6a04ba",
   "metadata": {},
   "source": [
    "## 10. Latent Interpolation: FRI → FRII\n",
    "\n",
    "Interpolate between an FRI and FRII galaxy in latent space to observe how morphology transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e41f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_latent_interpolation(model, images, labels, idx_a: int, idx_b: int, steps: int = 10):\n",
    "    \"\"\"\n",
    "    Show interpolation in latent space between two samples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get the two samples\n",
    "    xa = torch.from_numpy(images[idx_a]).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "    xb = torch.from_numpy(images[idx_b]).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "    ya, yb = labels[idx_a], labels[idx_b]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        za = model.encode(xa)\n",
    "        zb = model.encode(xb)\n",
    "        \n",
    "        # Interpolate\n",
    "        alphas = torch.linspace(0, 1, steps, device=device).view(-1, 1)\n",
    "        z_interp = (1 - alphas) * za + alphas * zb\n",
    "        x_interp = model.decode(z_interp).cpu()\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, steps + 2, figsize=(1.5 * (steps + 2), 2))\n",
    "    \n",
    "    # Start image\n",
    "    axes[0].imshow(xa.cpu()[0, 0], cmap=\"hot\")\n",
    "    axes[0].set_title(f\"{CLASS_NAMES[ya]}\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    # Interpolated images\n",
    "    for i in range(steps):\n",
    "        axes[i + 1].imshow(x_interp[i, 0], cmap=\"hot\")\n",
    "        axes[i + 1].set_title(f\"α={alphas[i].item():.1f}\")\n",
    "        axes[i + 1].axis(\"off\")\n",
    "    \n",
    "    # End image\n",
    "    axes[-1].imshow(xb.cpu()[0, 0], cmap=\"hot\")\n",
    "    axes[-1].set_title(f\"{CLASS_NAMES[yb]}\")\n",
    "    axes[-1].axis(\"off\")\n",
    "    \n",
    "    plt.suptitle(\"Latent Space Interpolation\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Find FRI and FRII samples\n",
    "fri_indices = np.where(labels == 0)[0]\n",
    "frii_indices = np.where(labels == 1)[0]\n",
    "\n",
    "np.random.seed(cfg.seed)\n",
    "idx_fri = fri_indices[np.random.randint(len(fri_indices))]\n",
    "idx_frii = frii_indices[np.random.randint(len(frii_indices))]\n",
    "\n",
    "print(f\"Interpolating: FRI (idx={idx_fri}) → FRII (idx={idx_frii})\")\n",
    "show_latent_interpolation(model, images, labels, idx_fri, idx_frii, steps=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interp_more",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a few more interpolations\n",
    "print(\"More FRI → FRII interpolations:\")\n",
    "for _ in range(2):\n",
    "    idx_a = fri_indices[np.random.randint(len(fri_indices))]\n",
    "    idx_b = frii_indices[np.random.randint(len(frii_indices))]\n",
    "    show_latent_interpolation(model, images, labels, idx_a, idx_b, steps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c73898",
   "metadata": {},
   "source": [
    "## 11. Latent Traversal\n",
    "\n",
    "Vary individual latent dimensions to see what visual features they encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8323d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_latent_traversal(model, images, labels, idx: int, dim: int = 0, n_steps: int = 11, span: float = 3.0):\n",
    "    \"\"\"\n",
    "    Show latent space traversal along a specified dimension.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    x = torch.from_numpy(images[idx]).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "    y = labels[idx]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z0 = model.encode(x).squeeze(0)\n",
    "        \n",
    "        # Create range of values for this dimension\n",
    "        vals = torch.linspace(-span, span, n_steps, device=device) + z0[dim]\n",
    "        \n",
    "        # Create modified latent vectors\n",
    "        Zs = z0.repeat(n_steps, 1)\n",
    "        Zs[:, dim] = vals\n",
    "        \n",
    "        # Decode\n",
    "        xs = model.decode(Zs).cpu()\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, n_steps + 1, figsize=(1.3 * (n_steps + 1), 2))\n",
    "    \n",
    "    # Original\n",
    "    axes[0].imshow(x.cpu()[0, 0], cmap=\"hot\")\n",
    "    axes[0].set_title(f\"orig\\n{CLASS_NAMES[y]}\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    # Traversed images\n",
    "    for i in range(n_steps):\n",
    "        axes[i + 1].imshow(xs[i, 0], cmap=\"hot\")\n",
    "        axes[i + 1].set_title(f\"{vals[i].item():.1f}\")\n",
    "        axes[i + 1].axis(\"off\")\n",
    "    \n",
    "    plt.suptitle(f\"Latent Traversal — Dimension {dim}\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Traverse a few dimensions for an FRI galaxy\n",
    "idx = fri_indices[0]\n",
    "for dim in [0, 5, 10, 15]:\n",
    "    show_latent_traversal(model, images, labels, idx=idx, dim=dim, n_steps=9, span=4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4253a4e3",
   "metadata": {},
   "source": [
    "## 12. kNN Classification on Embeddings\n",
    "\n",
    "Evaluate whether the latent representations capture FR morphology by training a kNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b349994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different kNN configurations to find the best\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize embeddings (important for distance-based methods)\n",
    "scaler = StandardScaler()\n",
    "Z_train_scaled = scaler.fit_transform(Z_train)\n",
    "Z_test_scaled = scaler.transform(Z_test)\n",
    "\n",
    "print(\"Evaluating different kNN configurations...\\n\")\n",
    "\n",
    "results = []\n",
    "for k in [3, 5, 7, 11]:\n",
    "    for metric in ['cosine', 'euclidean']:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
    "        knn.fit(Z_train_scaled, Y_train)\n",
    "        Y_pred = knn.predict(Z_test_scaled)\n",
    "        acc = accuracy_score(Y_test, Y_pred)\n",
    "        results.append({'k': k, 'metric': metric, 'accuracy': acc})\n",
    "        print(f\"k={k:2d}, {metric:10s}: accuracy = {acc:.3f}\")\n",
    "\n",
    "# Find best configuration\n",
    "best = max(results, key=lambda x: x['accuracy'])\n",
    "print(f\"\\nBest: k={best['k']}, metric={best['metric']}, accuracy={best['accuracy']:.3f}\")\n",
    "\n",
    "# Use best configuration for final evaluation\n",
    "knn = KNeighborsClassifier(n_neighbors=best['k'], metric=best['metric'])\n",
    "knn.fit(Z_train_scaled, Y_train)\n",
    "Y_pred = knn.predict(Z_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(f\"\\nFinal kNN accuracy: {accuracy:.3f}\")\n",
    "print(f\"Random baseline (2 classes): 0.500\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(Y_test, Y_pred, target_names=CLASS_NAMES, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion_matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASS_NAMES)\n",
    "disp.plot(ax=axes[0], cmap='Blues', colorbar=True)\n",
    "axes[0].set_title(f\"Confusion Matrix (kNN, k=5, acc={accuracy:.3f})\")\n",
    "\n",
    "# Per-class accuracy\n",
    "per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "bars = axes[1].bar(CLASS_NAMES, per_class_acc, color=['#1f77b4', '#ff7f0e'], edgecolor='black')\n",
    "axes[1].axhline(accuracy, color='red', linestyle='--', label=f'Overall: {accuracy:.3f}')\n",
    "axes[1].set_xlabel(\"FR Class\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].set_title(\"Per-Class Accuracy\")\n",
    "axes[1].set_ylim(0, 1.05)\n",
    "axes[1].legend()\n",
    "\n",
    "for bar, acc in zip(bars, per_class_acc):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, acc + 0.02, f\"{acc:.2f}\", ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4mzem1nhepu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with other classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(\"Comparing multiple classifiers on autoencoder embeddings:\\n\")\n",
    "\n",
    "classifiers = {\n",
    "    'kNN (best)': KNeighborsClassifier(n_neighbors=best['k'], metric=best['metric']),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=cfg.seed),\n",
    "    'Logistic Reg': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=cfg.seed),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', class_weight='balanced', random_state=cfg.seed),\n",
    "}\n",
    "\n",
    "clf_results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(Z_train_scaled, Y_train)\n",
    "    pred = clf.predict(Z_test_scaled)\n",
    "    acc = accuracy_score(Y_test, pred)\n",
    "    clf_results[name] = acc\n",
    "    print(f\"{name:15s}: {acc:.3f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "names = list(clf_results.keys())\n",
    "accs = list(clf_results.values())\n",
    "colors = ['steelblue' if a > 0.5 else 'coral' for a in accs]\n",
    "bars = ax.bar(names, accs, color=colors, edgecolor='black')\n",
    "ax.axhline(0.5, color='red', linestyle='--', linewidth=2, label='Random baseline (50%)')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Classifier Comparison on Autoencoder Embeddings')\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.legend()\n",
    "\n",
    "for bar, acc in zip(bars, accs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, acc + 0.02, f'{acc:.2f}', \n",
    "            ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest classifier: {max(clf_results, key=clf_results.get)} ({max(clf_results.values()):.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "examples_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples of correct and incorrect classifications\n",
    "correct_mask = (Y_pred == Y_test)\n",
    "correct_idx = np.where(correct_mask)[0]\n",
    "incorrect_idx = np.where(~correct_mask)[0]\n",
    "\n",
    "n_examples = min(6, len(correct_idx), max(1, len(incorrect_idx)))\n",
    "\n",
    "fig, axes = plt.subplots(2, n_examples, figsize=(2 * n_examples, 4.5))\n",
    "if n_examples == 1:\n",
    "    axes = axes.reshape(2, 1)\n",
    "\n",
    "# Correct predictions\n",
    "for i in range(n_examples):\n",
    "    if i < len(correct_idx):\n",
    "        idx = correct_idx[i]\n",
    "        img, _ = test_dataset[idx]\n",
    "        axes[0, i].imshow(img[0].numpy(), cmap=\"hot\")\n",
    "        axes[0, i].set_title(f\"True: {CLASS_NAMES[Y_test[idx]]}\\nPred: {CLASS_NAMES[Y_pred[idx]]}\", \n",
    "                           fontsize=9, color='green')\n",
    "    axes[0, i].axis(\"off\")\n",
    "\n",
    "# Incorrect predictions\n",
    "for i in range(n_examples):\n",
    "    if i < len(incorrect_idx):\n",
    "        idx = incorrect_idx[i]\n",
    "        img, _ = test_dataset[idx]\n",
    "        axes[1, i].imshow(img[0].numpy(), cmap=\"hot\")\n",
    "        axes[1, i].set_title(f\"True: {CLASS_NAMES[Y_test[idx]]}\\nPred: {CLASS_NAMES[Y_pred[idx]]}\", \n",
    "                           fontsize=9, color='red')\n",
    "    else:\n",
    "        axes[1, i].text(0.5, 0.5, \"No errors\", ha='center', va='center')\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "axes[0, 0].set_ylabel(\"Correct\", fontsize=11)\n",
    "axes[1, 0].set_ylabel(\"Incorrect\", fontsize=11)\n",
    "plt.suptitle(\"kNN Classification Examples\", fontsize=12)\n",
    "#plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCorrect: {correct_mask.sum()} / {len(Y_test)} ({100*correct_mask.mean():.1f}%)\")\n",
    "print(f\"Incorrect: {(~correct_mask).sum()} / {len(Y_test)} ({100*(~correct_mask).mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96698c91",
   "metadata": {},
   "source": [
    "## 13. Summary\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Reconstruction quality**: The autoencoder learns to reconstruct the essential morphological features of radio galaxies — the core, jets, and lobes are preserved.\n",
    "\n",
    "2. **Latent space structure**: PCA/UMAP visualizations show the structure learned by the autoencoder. Complete separation of FRI/FRII is challenging because:\n",
    "   - The visual distinction between FR classes can be subtle\n",
    "   - Some galaxies have intermediate morphologies\n",
    "   - The autoencoder optimizes for reconstruction, not classification\n",
    "\n",
    "3. **Latent interpolation**: Smooth transitions between FRI and FRII suggest the latent space is continuous — intermediate representations decode to plausible intermediate morphologies.\n",
    "\n",
    "4. **Classification performance**: This is a genuinely challenging problem! Achieving accuracy significantly above 50% demonstrates that the embeddings capture FR-discriminative information, even though the autoencoder was not trained with class labels.\n",
    "\n",
    "### Improvements Made in This Version\n",
    "\n",
    "- **Deeper architecture**: 3 conv layers with BatchNorm instead of 2\n",
    "- **More capacity**: 32→64→128 channels capture complex morphological patterns\n",
    "- **Data augmentation**: Rotations and flips (physically valid for radio galaxies)\n",
    "- **Log-scale preprocessing**: `asinh` scaling for better dynamic range handling\n",
    "- **AdamW + LR scheduler**: Better optimization with learning rate reduction on plateau\n",
    "- **Multiple classifier comparison**: kNN, Random Forest, Logistic Regression, SVM\n",
    "\n",
    "### Why is FR Classification Hard?\n",
    "\n",
    "FRI/FRII classification from images alone is genuinely difficult:\n",
    "- **Subtle visual differences**: The key distinction (edge-darkened vs edge-brightened) can be hard to see\n",
    "- **Projection effects**: 3D structures viewed at different angles look different\n",
    "- **Resolution**: 64×64 may not capture fine details needed for classification\n",
    "- **Dataset size**: ~800 images is small for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "import joblib\n",
    "\n",
    "#model_path = Path(cfg.data_dir) / \"mirabest_autoencoder.pth\"\n",
    "#scaler_path = Path(cfg.data_dir) / \"mirabest_scaler.pkl\"\n",
    "\n",
    "model_path = \"mirabest_autoencoder.pth\"\n",
    "scaler_path = \"mirabest_scaler.pkl\"\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'latent_dim': cfg.latent_dim,\n",
    "    'image_size': cfg.image_size,\n",
    "    'history': history,\n",
    "}, model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save scaler for consistent normalization\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to: {scaler_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6a617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r2ska-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
