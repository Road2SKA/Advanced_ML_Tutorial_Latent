{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# Session 4A: Generate Your Own Clay Embeddings\n",
    "\n",
    "**Road to SKA: Foundation Models, Embeddings, and Latent Spaces**\n",
    "\n",
    "In Session 2, we used **precomputed Clay embeddings** so everyone could run on CPU without installing Clay. In this advanced notebook, you will learn to generate embeddings **from scratch** using the Clay foundation model.\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Acquire satellite imagery** via STAC or from sample GeoTIFF files\n",
    "2. **Chip imagery** into 256×256 tiles suitable for Clay\n",
    "3. **Prepare Clay inputs** (normalized pixels, timestamps, wavelengths)\n",
    "4. **Generate embeddings** using the Clay encoder\n",
    "5. **Save embeddings** in a format compatible with Session 2 workflows\n",
    "6. **Reuse the Session 2 classifier/retrieval pipeline** on your own embeddings\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook requires a **dedicated Clay environment** (separate from the main `r2ska-tutorial` environment) because Clay needs Python ≥3.11 and PyTorch ≥2.4.\n",
    "\n",
    "### Quick Setup (Recommended)\n",
    "\n",
    "Run the setup script which creates the environment and downloads the checkpoint:\n",
    "\n",
    "```bash\n",
    "# From the tutorial directory\n",
    "./setup_clay_env.sh\n",
    "```\n",
    "\n",
    "Then activate the environment:\n",
    "```bash\n",
    "conda activate r2ska-clay\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "### Manual Setup\n",
    "\n",
    "If you prefer to set up manually:\n",
    "\n",
    "**1. Create the Clay environment:**\n",
    "```bash\n",
    "conda env create -f environment-clay.yml\n",
    "conda activate r2ska-clay\n",
    "```\n",
    "\n",
    "**2. Download the Clay v1.5 checkpoint (~1.2 GB):**\n",
    "```bash\n",
    "wget https://huggingface.co/made-with-clay/Clay/resolve/main/v1.5/clay-v1.5.ckpt\n",
    "```\n",
    "\n",
    "Or download manually from: https://huggingface.co/made-with-clay/Clay/tree/main/v1.5\n",
    "\n",
    "Place `clay-v1.5.ckpt` in the same directory as this notebook (or update `CHECKPOINT_PATH` below).\n",
    "\n",
    "### Environment Differences\n",
    "\n",
    "| Feature | `r2ska-tutorial` | `r2ska-clay` |\n",
    "|---------|------------------|--------------|\n",
    "| Python | 3.10 | 3.11 |\n",
    "| PyTorch | ≥2.0 | ≥2.4 |\n",
    "| Clay | Not installed | Installed |\n",
    "| STAC tools | Optional | Included |\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- Clay documentation: https://clay-foundation.github.io/model/\n",
    "- Clay \"Basic Use\" tutorial: https://clay-foundation.github.io/model/getting-started/basic_use.html\n",
    "- Earth Search STAC: https://earth-search.aws.element84.com/v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-setup-header",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Check for Clay\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_OK = True\n",
    "except ImportError:\n",
    "    TORCH_OK = False\n",
    "    print(\"ERROR: PyTorch not installed. Install with: pip install torch\")\n",
    "\n",
    "try:\n",
    "    from clay.module import ClayMAEModule\n",
    "    CLAY_OK = True\n",
    "except ImportError:\n",
    "    try:\n",
    "        # Alternative import path\n",
    "        from claymodel.module import ClayMAEModule\n",
    "        CLAY_OK = True\n",
    "    except ImportError:\n",
    "        CLAY_OK = False\n",
    "        print(\"ERROR: Clay not installed.\")\n",
    "        print(\"Install with: pip install git+https://github.com/Clay-foundation/model.git\")\n",
    "\n",
    "# Check for optional STAC dependencies\n",
    "try:\n",
    "    import pystac_client\n",
    "    import stackstac\n",
    "    STAC_OK = True\n",
    "except ImportError:\n",
    "    STAC_OK = False\n",
    "    print(\"Note: pystac-client/stackstac not available. Will use sample GeoTIFF fallback.\")\n",
    "    print(\"Install with: pip install pystac-client stackstac\")\n",
    "\n",
    "# Check for rasterio (needed for GeoTIFF loading)\n",
    "try:\n",
    "    import rasterio\n",
    "    RASTERIO_OK = True\n",
    "except ImportError:\n",
    "    RASTERIO_OK = False\n",
    "    print(\"Note: rasterio not available. Install with: pip install rasterio\")\n",
    "\n",
    "# Check for shapely (needed for geometry operations)\n",
    "try:\n",
    "    from shapely.geometry import box, Point, Polygon\n",
    "    from shapely import wkb\n",
    "    SHAPELY_OK = True\n",
    "except ImportError:\n",
    "    SHAPELY_OK = False\n",
    "    print(\"Note: shapely not available. Install with: pip install shapely\")\n",
    "\n",
    "print(\"\\n=== Dependency Status ===\")\n",
    "print(f\"PyTorch:    {'OK' if TORCH_OK else 'MISSING'}\")\n",
    "print(f\"Clay:       {'OK' if CLAY_OK else 'MISSING'}\")\n",
    "print(f\"STAC tools: {'OK' if STAC_OK else 'Not installed (will use fallback)'}\")\n",
    "print(f\"Rasterio:   {'OK' if RASTERIO_OK else 'Not installed'}\")\n",
    "print(f\"Shapely:    {'OK' if SHAPELY_OK else 'Not installed'}\")\n",
    "\n",
    "if not CLAY_OK:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLAY IS REQUIRED FOR THIS NOTEBOOK\")\n",
    "    print(\"Please install Clay before continuing.\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "device-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device selection: CUDA > MPS (Apple Silicon) > CPU\n",
    "if TORCH_OK:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS (Apple Silicon)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU (embedding generation will be slow)\")\n",
    "else:\n",
    "    device = None\n",
    "    print(\"PyTorch not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for embedding generation.\"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    data_dir: str = \"./data/session2b\"\n",
    "    checkpoint_path: str = \"./clay-v1.5.ckpt\"\n",
    "    output_parquet: str = \"./data/session2b/my_embeddings.parquet\"\n",
    "    \n",
    "    # AOI: SF Bay Area (same region as Session 2 precomputed data)\n",
    "    bbox: Tuple[float, float, float, float] = (-122.52, 37.70, -122.35, 37.83)\n",
    "    \n",
    "    # STAC parameters\n",
    "    stac_url: str = \"https://earth-search.aws.element84.com/v1\"\n",
    "    collection: str = \"sentinel-2-l2a\"\n",
    "    datetime_range: str = \"2022-05-01/2022-05-31\"\n",
    "    max_cloud_cover: int = 20\n",
    "    \n",
    "    # Sentinel-2 bands and wavelengths (nm)\n",
    "    # Using 4 bands for simplicity: Blue, Green, Red, NIR\n",
    "    bands: List[str] = field(default_factory=lambda: [\"B02\", \"B03\", \"B04\", \"B08\"])\n",
    "    wavelengths_nm: List[float] = field(default_factory=lambda: [490.0, 560.0, 665.0, 842.0])\n",
    "    resolution_m: int = 10\n",
    "    \n",
    "    # Chipping parameters\n",
    "    chip_size: int = 256  # Clay expects 256x256\n",
    "    max_chips: int = 100  # Limit for workshop\n",
    "    nodata_threshold: float = 0.1  # Skip chips with >10% nodata\n",
    "    \n",
    "    # Processing\n",
    "    batch_size: int = 8\n",
    "    random_seed: int = 42\n",
    "    \n",
    "    def __repr__(self):\n",
    "        lines = [\"Config:\"]\n",
    "        for k, v in self.__dict__.items():\n",
    "            lines.append(f\"  {k}: {v}\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(cfg.random_seed)\n",
    "np.random.seed(cfg.random_seed)\n",
    "if TORCH_OK:\n",
    "    torch.manual_seed(cfg.random_seed)\n",
    "\n",
    "# Create data directory\n",
    "Path(cfg.data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": "## Part 2: Data Acquisition via STAC\n\n### What is STAC?\n\n**STAC (SpatioTemporal Asset Catalog)** is a standardized way to describe and search for geospatial data. Think of it as a \"search engine for satellite imagery\" that allows you to:\n\n- **Search** for imagery by location (bounding box), time range, and properties (e.g., cloud cover)\n- **Discover** what data is available without downloading everything first\n- **Access** imagery directly from cloud storage (Cloud-Optimized GeoTIFFs)\n\n### Key STAC Concepts\n\n| Term | Description |\n|------|-------------|\n| **Catalog** | A collection of STAC items, like a library |\n| **Collection** | A group of related items (e.g., \"sentinel-2-l2a\" for Sentinel-2 Level-2A) |\n| **Item** | A single spatiotemporal asset (one satellite scene at one time) |\n| **Asset** | A file within an item (e.g., individual spectral bands) |\n\n### Our Data Source: Earth Search\n\nWe use [Earth Search](https://earth-search.aws.element84.com/v1), a free STAC API hosted by Element 84 that provides access to:\n- **Sentinel-2 L2A**: Surface reflectance imagery (atmospherically corrected)\n- **Landsat**: USGS Landsat Collection 2\n- **NAIP**: US aerial imagery\n- And more...\n\n### The Query Process\n\n```\n1. Connect to STAC catalog (Earth Search)\n2. Search for items matching:\n   - Collection: sentinel-2-l2a\n   - Bounding box: our area of interest\n   - Date range: May 2022\n   - Cloud cover: < 20%\n3. Select the best matching scene\n4. Load specific bands (Blue, Green, Red, NIR)\n5. Stack bands into a single array\n```\n\nIf STAC is unavailable, the notebook falls back to synthetic data for demonstration."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url: str, dst: Path, chunk_size: int = 1 << 20) -> Path:\n",
    "    \"\"\"Download a file with progress indication.\"\"\"\n",
    "    dst = Path(dst)\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if dst.exists() and dst.stat().st_size > 0:\n",
    "        print(f\"  File exists: {dst}\")\n",
    "        return dst\n",
    "    \n",
    "    print(f\"  Downloading: {url}\")\n",
    "    with requests.get(url, stream=True, timeout=120) as r:\n",
    "        r.raise_for_status()\n",
    "        total = int(r.headers.get('content-length', 0))\n",
    "        downloaded = 0\n",
    "        with open(dst, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    downloaded += len(chunk)\n",
    "                    if total > 0:\n",
    "                        pct = 100 * downloaded / total\n",
    "                        print(f\"\\r  Progress: {pct:.1f}%\", end=\"\", flush=True)\n",
    "        print()\n",
    "    return dst\n",
    "\n",
    "\n",
    "# Mapping from Sentinel-2 band codes to Earth Search asset names\n",
    "SENTINEL2_BAND_MAP = {\n",
    "    \"B01\": \"coastal\",\n",
    "    \"B02\": \"blue\",\n",
    "    \"B03\": \"green\",\n",
    "    \"B04\": \"red\",\n",
    "    \"B05\": \"rededge1\",\n",
    "    \"B06\": \"rededge2\",\n",
    "    \"B07\": \"rededge3\",\n",
    "    \"B08\": \"nir\",\n",
    "    \"B8A\": \"nir08\",\n",
    "    \"B09\": \"nir09\",\n",
    "    \"B11\": \"swir16\",\n",
    "    \"B12\": \"swir22\",\n",
    "    \"SCL\": \"scl\",\n",
    "}\n",
    "\n",
    "\n",
    "def acquire_via_stac(cfg: Config) -> Tuple[np.ndarray, dict]:\n",
    "    \"\"\"\n",
    "    Acquire imagery via STAC query.\n",
    "    \n",
    "    Returns:\n",
    "        imagery: np.ndarray of shape (bands, height, width)\n",
    "        metadata: dict with acquisition info\n",
    "    \"\"\"\n",
    "    if not STAC_OK:\n",
    "        raise ImportError(\"STAC dependencies not available\")\n",
    "    \n",
    "    print(f\"Searching STAC: {cfg.stac_url}\")\n",
    "    print(f\"  Collection: {cfg.collection}\")\n",
    "    print(f\"  BBox: {cfg.bbox}\")\n",
    "    print(f\"  Date range: {cfg.datetime_range}\")\n",
    "    \n",
    "    catalog = pystac_client.Client.open(cfg.stac_url)\n",
    "    \n",
    "    search = catalog.search(\n",
    "        collections=[cfg.collection],\n",
    "        bbox=cfg.bbox,\n",
    "        datetime=cfg.datetime_range,\n",
    "        query={\"eo:cloud_cover\": {\"lt\": cfg.max_cloud_cover}},\n",
    "        max_items=1,\n",
    "    )\n",
    "    \n",
    "    items = list(search.items())\n",
    "    if len(items) == 0:\n",
    "        raise RuntimeError(\"No STAC items found. Try different date range or cloud cover threshold.\")\n",
    "    \n",
    "    item = items[0]\n",
    "    print(f\"\\nFound item: {item.id}\")\n",
    "    print(f\"  Datetime: {item.datetime}\")\n",
    "    print(f\"  Cloud cover: {item.properties.get('eo:cloud_cover', 'N/A')}%\")\n",
    "    \n",
    "    # Debug: show available assets\n",
    "    available_assets = list(item.assets.keys())\n",
    "    print(f\"  Available assets: {available_assets[:10]}{'...' if len(available_assets) > 10 else ''}\")\n",
    "    \n",
    "    # Map band codes to asset names (Earth Search uses descriptive names)\n",
    "    asset_names = []\n",
    "    for band in cfg.bands:\n",
    "        if band in available_assets:\n",
    "            # Band code exists as-is\n",
    "            asset_names.append(band)\n",
    "        elif band in SENTINEL2_BAND_MAP and SENTINEL2_BAND_MAP[band] in available_assets:\n",
    "            # Map band code to descriptive name\n",
    "            asset_names.append(SENTINEL2_BAND_MAP[band])\n",
    "        elif band.lower() in available_assets:\n",
    "            # Try lowercase\n",
    "            asset_names.append(band.lower())\n",
    "        else:\n",
    "            raise RuntimeError(f\"Band {band} not found in available assets: {available_assets}\")\n",
    "    \n",
    "    print(f\"  Mapped bands: {cfg.bands} -> {asset_names}\")\n",
    "\n",
    "    # Stack bands using stackstac\n",
    "    print(f\"\\nLoading bands: {asset_names}\")\n",
    "\n",
    "    # Debug: show STAC item metadata for CRS/bounds\n",
    "    print(f\"  Item EPSG from metadata: {item.properties.get('proj:epsg')}\")\n",
    "    print(f\"  Item bbox: {item.bbox}\")\n",
    "\n",
    "    # Get the item's native EPSG code from metadata\n",
    "    # If not available, determine UTM zone from bbox center longitude\n",
    "    item_epsg = item.properties.get('proj:epsg')\n",
    "    if item_epsg is None:\n",
    "        # Calculate UTM zone from center longitude of bbox\n",
    "        center_lon = (cfg.bbox[0] + cfg.bbox[2]) / 2\n",
    "        utm_zone = int((center_lon + 180) / 6) + 1\n",
    "        # Northern hemisphere for positive latitude\n",
    "        center_lat = (cfg.bbox[1] + cfg.bbox[3]) / 2\n",
    "        item_epsg = 32600 + utm_zone if center_lat >= 0 else 32700 + utm_zone\n",
    "        print(f\"  Computed UTM zone {utm_zone}N -> EPSG:{item_epsg}\")\n",
    "    print(f\"  Using EPSG: {item_epsg}\")\n",
    "\n",
    "    # Transform bbox from EPSG:4326 to target CRS for explicit bounds\n",
    "    # stackstac needs explicit bounds when STAC metadata is incomplete (GitHub #187)\n",
    "    try:\n",
    "        from pyproj import Transformer\n",
    "        transformer = Transformer.from_crs(\"EPSG:4326\", f\"EPSG:{item_epsg}\", always_xy=True)\n",
    "        min_lon, min_lat, max_lon, max_lat = cfg.bbox\n",
    "        x_min, y_min = transformer.transform(min_lon, min_lat)\n",
    "        x_max, y_max = transformer.transform(max_lon, max_lat)\n",
    "        stack_bounds = (x_min, y_min, x_max, y_max)\n",
    "        print(f\"  Transformed bounds: {stack_bounds}\")\n",
    "    except ImportError:\n",
    "        print(\"  Warning: pyproj not available, cannot transform bounds\")\n",
    "        stack_bounds = None\n",
    "\n",
    "    stack = stackstac.stack(\n",
    "        [item],\n",
    "        assets=asset_names,\n",
    "        resolution=cfg.resolution_m,\n",
    "        epsg=item_epsg,  # Explicit EPSG prevents metadata inference issues\n",
    "        bounds=stack_bounds,  # Explicit bounds in target CRS\n",
    "        chunksize=2048,  # Load in reasonable chunks\n",
    "    )\n",
    "    \n",
    "    # Debug: check stack shape before computing\n",
    "    print(f\"  Stack shape (lazy): {stack.shape}\")\n",
    "    print(f\"  Stack dims: {stack.dims}\")\n",
    "    \n",
    "    if stack.shape[0] == 0:\n",
    "        raise RuntimeError(\"Stack has no time dimension - no data was loaded\")\n",
    "    if stack.shape[1] == 0:\n",
    "        raise RuntimeError(\"Stack has no bands - check asset names\")\n",
    "    \n",
    "    # Load into memory (first time slice)\n",
    "    arr = stack.isel(time=0).compute().values  # (bands, height, width)\n",
    "    \n",
    "    print(f\"  Loaded array: {arr.shape}\")\n",
    "    \n",
    "    # Handle NaN values (nodata)\n",
    "    arr = np.nan_to_num(arr, nan=0).astype(np.float32)\n",
    "    \n",
    "    metadata = {\n",
    "        \"source\": \"stac\",\n",
    "        \"item_id\": item.id,\n",
    "        \"datetime\": item.datetime.isoformat() if item.datetime else None,\n",
    "        \"bbox\": cfg.bbox,\n",
    "        \"bands\": cfg.bands,\n",
    "    }\n",
    "    \n",
    "    return arr, metadata\n",
    "\n",
    "\n",
    "def create_synthetic_imagery(cfg: Config) -> Tuple[np.ndarray, dict]:\n",
    "    \"\"\"\n",
    "    Create synthetic imagery for demonstration when real data is unavailable.\n",
    "    \n",
    "    Returns:\n",
    "        imagery: np.ndarray of shape (bands, height, width)\n",
    "        metadata: dict with acquisition info\n",
    "    \"\"\"\n",
    "    print(\"Creating synthetic sample data for demonstration...\")\n",
    "    \n",
    "    # Create synthetic data matching expected dimensions\n",
    "    # Enough for ~16 chips at 256x256\n",
    "    height = width = cfg.chip_size * 4\n",
    "    n_bands = len(cfg.bands)\n",
    "    \n",
    "    # Generate realistic-looking data with some structure\n",
    "    np.random.seed(cfg.random_seed)\n",
    "    arr = np.random.uniform(0, 3000, (n_bands, height, width)).astype(np.float32)\n",
    "    \n",
    "    # Add some spatial structure (gradient + noise)\n",
    "    y_grad = np.linspace(0, 1, height).reshape(-1, 1)\n",
    "    x_grad = np.linspace(0, 1, width).reshape(1, -1)\n",
    "    for b in range(n_bands):\n",
    "        arr[b] += 1000 * y_grad * x_grad\n",
    "    \n",
    "    print(f\"Created synthetic data: {arr.shape}\")\n",
    "    \n",
    "    metadata = {\n",
    "        \"source\": \"synthetic\",\n",
    "        \"datetime\": \"2022-05-18T00:00:00\",\n",
    "        \"bbox\": cfg.bbox,\n",
    "        \"bands\": cfg.bands,\n",
    "    }\n",
    "    \n",
    "    return arr, metadata\n",
    "\n",
    "\n",
    "def acquire_imagery(cfg: Config) -> Tuple[np.ndarray, dict]:\n",
    "    \"\"\"Acquire imagery using STAC, falling back to synthetic data if unavailable.\"\"\"\n",
    "    \n",
    "    # Try STAC if dependencies are available\n",
    "    if STAC_OK:\n",
    "        try:\n",
    "            print(\"Attempting STAC acquisition...\")\n",
    "            return acquire_via_stac(cfg)\n",
    "        except Exception as e:\n",
    "            print(f\"STAC acquisition failed: {e}\")\n",
    "            print(\"Falling back to synthetic data...\\n\")\n",
    "            return create_synthetic_imagery(cfg)\n",
    "    else:\n",
    "        # STAC not available - use synthetic data\n",
    "        print(\"STAC dependencies not installed.\")\n",
    "        return create_synthetic_imagery(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquire-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquire the imagery\n",
    "imagery, acq_metadata = acquire_imagery(cfg)\n",
    "\n",
    "print(f\"\\n=== Acquisition Summary ===\")\n",
    "print(f\"Source: {acq_metadata['source']}\")\n",
    "print(f\"Shape: {imagery.shape} (bands, height, width)\")\n",
    "print(f\"Dtype: {imagery.dtype}\")\n",
    "print(f\"Value range: [{imagery.min():.1f}, {imagery.max():.1f}]\")\n",
    "print(f\"NaN percentage: {100 * np.isnan(imagery).mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-imagery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the acquired imagery\n",
    "def normalize_for_display(arr, percentile_low=2, percentile_high=98):\n",
    "    \"\"\"Normalize array for display using percentile stretching.\"\"\"\n",
    "    arr = np.nan_to_num(arr, nan=0)\n",
    "    vmin = np.percentile(arr, percentile_low)\n",
    "    vmax = np.percentile(arr, percentile_high)\n",
    "    return np.clip((arr - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "# Create RGB composite (bands: B04, B03, B02 = indices 2, 1, 0)\n",
    "if imagery.shape[0] >= 3:\n",
    "    rgb = np.stack([\n",
    "        normalize_for_display(imagery[2]),  # Red (B04)\n",
    "        normalize_for_display(imagery[1]),  # Green (B03)\n",
    "        normalize_for_display(imagery[0]),  # Blue (B02)\n",
    "    ], axis=-1)\n",
    "else:\n",
    "    rgb = normalize_for_display(imagery[0])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# RGB composite\n",
    "axes[0].imshow(rgb)\n",
    "axes[0].set_title(f\"RGB Composite\\nSource: {acq_metadata['source']}\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# NIR band (index 3)\n",
    "if imagery.shape[0] >= 4:\n",
    "    nir_display = normalize_for_display(imagery[3])\n",
    "    axes[1].imshow(nir_display, cmap=\"RdYlGn\")\n",
    "    axes[1].set_title(\"NIR Band (B08)\")\n",
    "else:\n",
    "    axes[1].imshow(normalize_for_display(imagery[-1]), cmap=\"gray\")\n",
    "    axes[1].set_title(\"Last Band\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image dimensions: {imagery.shape[1]} x {imagery.shape[2]} pixels\")\n",
    "print(f\"At {cfg.resolution_m}m resolution: ~{imagery.shape[1] * cfg.resolution_m / 1000:.1f} x {imagery.shape[2] * cfg.resolution_m / 1000:.1f} km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-header",
   "metadata": {},
   "source": [
    "## Part 3: Chipping and Preprocessing\n",
    "\n",
    "Clay expects input chips of shape `(batch, bands, 256, 256)`. We need to:\n",
    "\n",
    "1. **Tile** the imagery into non-overlapping 256×256 chips\n",
    "2. **Filter** chips that contain too much nodata\n",
    "3. **Normalize** pixel values (Sentinel-2 L2A values are typically 0-10000, we scale to 0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chips(imagery: np.ndarray, chip_size: int = 256, \n",
    "                 nodata_threshold: float = 0.1, max_chips: int = None) -> Tuple[np.ndarray, List[dict]]:\n",
    "    \"\"\"\n",
    "    Create non-overlapping chips from imagery.\n",
    "    \n",
    "    Args:\n",
    "        imagery: Array of shape (bands, height, width)\n",
    "        chip_size: Size of each chip (default 256 for Clay)\n",
    "        nodata_threshold: Skip chips with more than this fraction of nodata\n",
    "        max_chips: Maximum number of chips to return\n",
    "    \n",
    "    Returns:\n",
    "        chips: Array of shape (n_chips, bands, chip_size, chip_size)\n",
    "        chip_info: List of dicts with chip metadata (row, col, bounds)\n",
    "    \"\"\"\n",
    "    bands, height, width = imagery.shape\n",
    "    \n",
    "    chips = []\n",
    "    chip_info = []\n",
    "    \n",
    "    n_rows = height // chip_size\n",
    "    n_cols = width // chip_size\n",
    "    \n",
    "    print(f\"Creating {chip_size}x{chip_size} chips...\")\n",
    "    print(f\"  Image size: {height} x {width}\")\n",
    "    print(f\"  Potential chips: {n_rows} rows x {n_cols} cols = {n_rows * n_cols}\")\n",
    "    \n",
    "    for row in range(n_rows):\n",
    "        for col in range(n_cols):\n",
    "            # Extract chip\n",
    "            y0 = row * chip_size\n",
    "            x0 = col * chip_size\n",
    "            chip = imagery[:, y0:y0+chip_size, x0:x0+chip_size]\n",
    "            \n",
    "            # Check for nodata (NaN or zeros)\n",
    "            nodata_frac = np.isnan(chip).mean() + (chip == 0).mean()\n",
    "            if nodata_frac > nodata_threshold:\n",
    "                continue\n",
    "            \n",
    "            chips.append(chip)\n",
    "            chip_info.append({\n",
    "                \"row\": row,\n",
    "                \"col\": col,\n",
    "                \"y0\": y0,\n",
    "                \"x0\": x0,\n",
    "            })\n",
    "            \n",
    "            if max_chips and len(chips) >= max_chips:\n",
    "                break\n",
    "        if max_chips and len(chips) >= max_chips:\n",
    "            break\n",
    "    \n",
    "    chips = np.stack(chips, axis=0)  # (n_chips, bands, h, w)\n",
    "    \n",
    "    print(f\"  Valid chips: {len(chips)} (skipped {n_rows * n_cols - len(chips)} with nodata)\")\n",
    "    \n",
    "    return chips, chip_info\n",
    "\n",
    "\n",
    "def normalize_sentinel2(chips: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize Sentinel-2 L2A values to [0, 1] range.\n",
    "    \n",
    "    Sentinel-2 L2A surface reflectance values are typically in [0, 10000].\n",
    "    We divide by 10000 and clip to [0, 1].\n",
    "    \"\"\"\n",
    "    chips = np.nan_to_num(chips, nan=0)\n",
    "    chips = chips / 10000.0\n",
    "    chips = np.clip(chips, 0, 1)\n",
    "    return chips.astype(np.float32)\n",
    "\n",
    "\n",
    "# Create chips\n",
    "chips_raw, chip_info = create_chips(\n",
    "    imagery, \n",
    "    chip_size=cfg.chip_size,\n",
    "    nodata_threshold=cfg.nodata_threshold,\n",
    "    max_chips=cfg.max_chips\n",
    ")\n",
    "\n",
    "# Normalize\n",
    "chips_norm = normalize_sentinel2(chips_raw)\n",
    "\n",
    "print(f\"\\n=== Chips Summary ===\")\n",
    "print(f\"Shape: {chips_norm.shape} (n_chips, bands, height, width)\")\n",
    "print(f\"Value range: [{chips_norm.min():.4f}, {chips_norm.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-chips",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample chips as RGB\n",
    "n_show = min(12, len(chips_norm))\n",
    "n_cols = 4\n",
    "n_rows = (n_show + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(3 * n_cols, 3 * n_rows))\n",
    "axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes.flatten()\n",
    "\n",
    "for i in range(n_show):\n",
    "    chip = chips_norm[i]\n",
    "    \n",
    "    # Create RGB (B04, B03, B02 = indices 2, 1, 0)\n",
    "    if chip.shape[0] >= 3:\n",
    "        # Use percentile normalization per-chip for proper display\n",
    "        def chip_normalize(band):\n",
    "            vmin, vmax = np.percentile(band, [2, 98])\n",
    "            return np.clip((band - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "        rgb = np.stack([chip_normalize(chip[2]), chip_normalize(chip[1]), chip_normalize(chip[0])], axis=-1)\n",
    "    else:\n",
    "        rgb = chip[0]\n",
    "    \n",
    "    axes[i].imshow(rgb, cmap=\"gray\" if rgb.ndim == 2 else None)\n",
    "    axes[i].set_title(f\"Chip {i}\\n({chip_info[i]['row']}, {chip_info[i]['col']})\")\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "# Hide unused axes\n",
    "for i in range(n_show, len(axes)):\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Sample chips ({cfg.chip_size}x{cfg.chip_size} pixels)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-header",
   "metadata": {},
   "source": "## Part 4: Prepare Clay Inputs\n\n### Why Does Clay Need Special Input Preparation?\n\nClay is a **foundation model** trained on diverse satellite imagery from multiple sensors. To handle this diversity, Clay uses a flexible input format that explicitly encodes:\n\n1. **What** the model sees (pixel values and which spectral bands)\n2. **When** the image was captured (temporal encoding)\n3. **Where** on Earth the image is located (spatial encoding)\n\nThis design allows Clay to generalize across different sensors, seasons, and geographic regions.\n\n### The Four Input Components\n\n| Input | Shape | Description |\n|-------|-------|-------------|\n| **pixels** | `[B, C, 256, 256]` | Normalized pixel values (0-1 range) |\n| **time** | `[B, 4]` | Encoded timestamp: `[week_sin, week_cos, hour_sin, hour_cos]` |\n| **latlon** | `[B, 4]` | Encoded location: `[lat_sin, lat_cos, lon_sin, lon_cos]` |\n| **waves** | `[N]` | Center wavelengths of each band in nanometers |\n| **gsd** | scalar | Ground sampling distance in meters |\n\n### Why Sine/Cosine Encoding?\n\nTime and location are **cyclical** quantities:\n- Week 52 is close to week 1 (end of year wraps to beginning)\n- Hour 23 is close to hour 0 (midnight)\n- Longitude -180° is the same as +180°\n\nSimple linear encoding would make week 52 seem \"far\" from week 1. Sine/cosine encoding preserves cyclical relationships:\n\n```python\n# Week 1 and Week 52 have similar encodings:\nweek_1:  [sin(2π×1/52),  cos(2π×1/52)]  ≈ [0.12, 0.99]\nweek_52: [sin(2π×52/52), cos(2π×52/52)] ≈ [0.00, 1.00]\n```\n\n### Why Wavelengths Matter\n\nClay was trained on imagery from multiple sensors with different spectral bands. By explicitly providing the center wavelength of each band (e.g., 490nm for blue, 842nm for NIR), Clay can:\n- Understand what each band measures\n- Generalize to sensors it wasn't explicitly trained on\n- Handle different band combinations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-inputs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_timestamps(chip_info: List[dict], acq_metadata: dict, \n",
    "                       imagery_shape: Tuple, cfg: Config) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Prepare timestamp arrays for Clay.\n",
    "    \n",
    "    Args:\n",
    "        chip_info: List of chip metadata dicts\n",
    "        acq_metadata: Acquisition metadata with datetime\n",
    "        imagery_shape: Shape of original imagery (bands, height, width)\n",
    "        cfg: Configuration object\n",
    "    \n",
    "    Returns:\n",
    "        timestamps: Array of shape (n_chips, 4) with [week, hour, lat, lon]\n",
    "    \"\"\"\n",
    "    # Parse datetime\n",
    "    dt_str = acq_metadata.get(\"datetime\", \"2022-05-18T00:00:00\")\n",
    "    if dt_str:\n",
    "        try:\n",
    "            dt = datetime.fromisoformat(dt_str.replace(\"Z\", \"+00:00\"))\n",
    "        except:\n",
    "            dt = datetime(2022, 5, 18, 0, 0)\n",
    "    else:\n",
    "        dt = datetime(2022, 5, 18, 0, 0)\n",
    "    \n",
    "    week = dt.isocalendar()[1]  # Week of year\n",
    "    hour = dt.hour\n",
    "    \n",
    "    # Calculate chip centroids in lat/lon\n",
    "    _, height, width = imagery_shape\n",
    "    min_lon, min_lat, max_lon, max_lat = cfg.bbox\n",
    "    \n",
    "    timestamps = []\n",
    "    for info in chip_info:\n",
    "        # Chip center in pixel coordinates\n",
    "        cy = info[\"y0\"] + cfg.chip_size / 2\n",
    "        cx = info[\"x0\"] + cfg.chip_size / 2\n",
    "        \n",
    "        # Convert to lat/lon (simple linear interpolation)\n",
    "        lat = max_lat - (cy / height) * (max_lat - min_lat)\n",
    "        lon = min_lon + (cx / width) * (max_lon - min_lon)\n",
    "        \n",
    "        timestamps.append([week, hour, lat, lon])\n",
    "    \n",
    "    return np.array(timestamps, dtype=np.float32)\n",
    "\n",
    "\n",
    "# Prepare timestamps\n",
    "timestamps = prepare_timestamps(chip_info, acq_metadata, imagery.shape, cfg)\n",
    "\n",
    "# Wavelengths (in nanometers)\n",
    "wavelengths = np.array([cfg.wavelengths_nm], dtype=np.float32)  # (1, n_bands)\n",
    "\n",
    "print(\"=== Clay Input Summary ===\")\n",
    "print(f\"Chips: {chips_norm.shape}\")\n",
    "print(f\"Timestamps: {timestamps.shape}\")\n",
    "print(f\"  Sample: week={timestamps[0,0]:.0f}, hour={timestamps[0,1]:.0f}, lat={timestamps[0,2]:.4f}, lon={timestamps[0,3]:.4f}\")\n",
    "print(f\"Wavelengths: {wavelengths.shape}\")\n",
    "print(f\"  Values (nm): {wavelengths[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-header",
   "metadata": {},
   "source": "## Part 5: Generate Clay Embeddings\n\n### What Happens Inside Clay?\n\nClay uses a **Vision Transformer (ViT)** architecture adapted for satellite imagery. Here's the processing pipeline:\n\n```\nInput Chip (256×256×C)\n    ↓\n┌─────────────────────────────────────┐\n│  1. PATCH EMBEDDING                 │\n│     Split into 8×8 pixel patches    │\n│     → 32×32 = 1024 patches          │\n│     Each patch → 1024-dim vector    │\n└─────────────────────────────────────┘\n    ↓\n┌─────────────────────────────────────┐\n│  2. POSITION ENCODING               │\n│     Add spatial position info       │\n│     Add time/location encoding      │\n└─────────────────────────────────────┘\n    ↓\n┌─────────────────────────────────────┐\n│  3. TRANSFORMER ENCODER             │\n│     Self-attention across patches   │\n│     12 transformer layers           │\n│     Learns relationships between    │\n│     all parts of the image          │\n└─────────────────────────────────────┘\n    ↓\n┌─────────────────────────────────────┐\n│  4. CLS TOKEN EXTRACTION            │\n│     Special [CLS] token aggregates  │\n│     information from all patches    │\n│     → 1024-dim embedding vector     │\n└─────────────────────────────────────┘\n    ↓\nOutput Embedding (1024-dim)\n```\n\n### The CLS Token\n\nThe **CLS (classification) token** is a special learnable token prepended to the patch sequence. Through self-attention, it \"attends\" to all patches and learns to aggregate the most important information into a single vector. This makes it ideal as a global representation of the entire chip.\n\n### Inference vs Training Mode\n\nDuring training, Clay uses **masking** (hiding 75% of patches) to learn robust representations. For inference, we set `mask_ratio=0` to use all patches, producing the best quality embeddings.\n\n### Output: A 1024-Dimensional Vector\n\nEach chip is compressed from ~262,144 pixel values (256×256×4 bands) into a single 1024-dimensional vector that captures:\n- Land cover type (urban, forest, water, agriculture)\n- Spatial patterns and textures\n- Seasonal characteristics (via time encoding)\n- Geographic context (via location encoding)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-clay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if checkpoint exists\n",
    "checkpoint_path = Path(cfg.checkpoint_path)\n",
    "\n",
    "if not checkpoint_path.exists():\n",
    "    # Try alternate locations\n",
    "    alt_paths = [\n",
    "        Path(\"clay-v1.5.ckpt\"),\n",
    "        Path.home() / \"clay-v1.5.ckpt\",\n",
    "        Path(cfg.data_dir) / \"clay-v1.5.ckpt\",\n",
    "    ]\n",
    "    for alt in alt_paths:\n",
    "        if alt.exists():\n",
    "            checkpoint_path = alt\n",
    "            print(f\"Found checkpoint at: {checkpoint_path}\")\n",
    "            break\n",
    "\n",
    "if not checkpoint_path.exists():\n",
    "    print(\"ERROR: Clay checkpoint not found!\")\n",
    "    print(\"\\nPlease download the checkpoint:\")\n",
    "    print(\"  wget https://huggingface.co/made-with-clay/Clay/resolve/main/v1.5/clay-v1.5.ckpt\")\n",
    "    print(f\"\\nSearched locations:\")\n",
    "    print(f\"  - {cfg.checkpoint_path}\")\n",
    "    for alt in alt_paths:\n",
    "        print(f\"  - {alt}\")\n",
    "    CHECKPOINT_OK = False\n",
    "else:\n",
    "    print(f\"Checkpoint found: {checkpoint_path}\")\n",
    "    print(f\"  Size: {checkpoint_path.stat().st_size / 1e9:.2f} GB\")\n",
    "    CHECKPOINT_OK = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLAY_OK and CHECKPOINT_OK and TORCH_OK:\n",
    "    print(\"Loading Clay model...\")\n",
    "    \n",
    "    # Clay expects configs/metadata.yaml in the current directory.\n",
    "    # This file is not included in the pip package, so we download it from the repo.\n",
    "    configs_dir = Path(\"configs\")\n",
    "    metadata_path = configs_dir / \"metadata.yaml\"\n",
    "    \n",
    "    if not metadata_path.exists():\n",
    "        print(\"Downloading Clay metadata.yaml from GitHub...\")\n",
    "        configs_dir.mkdir(exist_ok=True)\n",
    "        metadata_url = \"https://raw.githubusercontent.com/Clay-foundation/model/main/configs/metadata.yaml\"\n",
    "        response = requests.get(metadata_url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        metadata_path.write_text(response.text)\n",
    "        print(f\"  Saved to {metadata_path}\")\n",
    "    else:\n",
    "        print(f\"Using existing {metadata_path}\")\n",
    "    \n",
    "    # Load model with mask_ratio=0 for inference (no masking)\n",
    "    model = ClayMAEModule.load_from_checkpoint(\n",
    "        str(checkpoint_path),\n",
    "        mask_ratio=0.0,  # No masking during inference\n",
    "        shuffle=False,   # Don't shuffle patches\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded on {device}\")\n",
    "    \n",
    "    # Convert inputs to tensors\n",
    "    chips_tensor = torch.tensor(chips_norm, dtype=torch.float32)\n",
    "    \n",
    "    # Clay expects time and latlon as sine/cosine encoded pairs:\n",
    "    # time: [week_sin, week_cos, hour_sin, hour_cos] -> [B, 4]\n",
    "    # latlon: [lat_sin, lat_cos, lon_sin, lon_cos] -> [B, 4]\n",
    "    \n",
    "    def encode_time_latlon(timestamps):\n",
    "        \"\"\"Encode timestamps to Clay's expected format with sin/cos encoding.\"\"\"\n",
    "        week = timestamps[:, 0]  # Week of year (0-52)\n",
    "        hour = timestamps[:, 1]  # Hour of day (0-23)\n",
    "        lat = timestamps[:, 2]   # Latitude (-90 to 90)\n",
    "        lon = timestamps[:, 3]   # Longitude (-180 to 180)\n",
    "        \n",
    "        # Normalize and encode with sin/cos\n",
    "        # Week: 0-52 -> 0-2π\n",
    "        week_rad = 2 * np.pi * week / 52\n",
    "        week_norm = np.stack([np.sin(week_rad), np.cos(week_rad)], axis=1)\n",
    "        \n",
    "        # Hour: 0-24 -> 0-2π\n",
    "        hour_rad = 2 * np.pi * hour / 24\n",
    "        hour_norm = np.stack([np.sin(hour_rad), np.cos(hour_rad)], axis=1)\n",
    "        \n",
    "        # Latitude: -90 to 90 -> -π/2 to π/2\n",
    "        lat_rad = np.pi * lat / 180\n",
    "        lat_norm = np.stack([np.sin(lat_rad), np.cos(lat_rad)], axis=1)\n",
    "        \n",
    "        # Longitude: -180 to 180 -> -π to π\n",
    "        lon_rad = np.pi * lon / 180\n",
    "        lon_norm = np.stack([np.sin(lon_rad), np.cos(lon_rad)], axis=1)\n",
    "        \n",
    "        # Combine: time [B, 4], latlon [B, 4]\n",
    "        time_encoded = np.hstack([week_norm, hour_norm]).astype(np.float32)\n",
    "        latlon_encoded = np.hstack([lat_norm, lon_norm]).astype(np.float32)\n",
    "        \n",
    "        return time_encoded, latlon_encoded\n",
    "    \n",
    "    time_encoded, latlon_encoded = encode_time_latlon(timestamps)\n",
    "    time_tensor = torch.tensor(time_encoded, dtype=torch.float32)\n",
    "    latlon_tensor = torch.tensor(latlon_encoded, dtype=torch.float32)\n",
    "    \n",
    "    # Wavelengths as 1D tensor\n",
    "    waves_tensor = torch.tensor(cfg.wavelengths_nm, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # GSD (ground sampling distance) in meters\n",
    "    gsd_tensor = torch.tensor(cfg.resolution_m, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Generate embeddings in batches\n",
    "    print(f\"\\nGenerating embeddings (batch_size={cfg.batch_size})...\")\n",
    "    \n",
    "    embeddings_list = []\n",
    "    n_chips = len(chips_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, n_chips, cfg.batch_size):\n",
    "            batch_end = min(i + cfg.batch_size, n_chips)\n",
    "            \n",
    "            # Get batch\n",
    "            batch_chips = chips_tensor[i:batch_end].to(device)\n",
    "            batch_time = time_tensor[i:batch_end].to(device)\n",
    "            batch_latlon = latlon_tensor[i:batch_end].to(device)\n",
    "            \n",
    "            # Create datacube dict for Clay encoder\n",
    "            datacube = {\n",
    "                \"pixels\": batch_chips,      # [B C H W]\n",
    "                \"time\": batch_time,         # [B 4] - week_sin, week_cos, hour_sin, hour_cos\n",
    "                \"latlon\": batch_latlon,     # [B 4] - lat_sin, lat_cos, lon_sin, lon_cos\n",
    "                \"gsd\": gsd_tensor,          # scalar\n",
    "                \"waves\": waves_tensor,      # [N] - wavelengths in nm\n",
    "            }\n",
    "            \n",
    "            # Get encoder output\n",
    "            # Returns: (encoded_patches, unmasked_indices, masked_indices, masked_matrix)\n",
    "            encoded_patches, _, _, _ = model.model.encoder(datacube)\n",
    "            \n",
    "            # Extract CLS token embedding (first token)\n",
    "            cls_embedding = encoded_patches[:, 0, :]  # [B D]\n",
    "            \n",
    "            embeddings_list.append(cls_embedding.cpu().numpy())\n",
    "            \n",
    "            print(f\"  Processed {batch_end}/{n_chips} chips\", end=\"\\r\")\n",
    "            \n",
    "            # Clear GPU cache periodically\n",
    "            if device.type == \"cuda\" and (i + cfg.batch_size) % (cfg.batch_size * 10) == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    print()\n",
    "    embeddings = np.vstack(embeddings_list)\n",
    "    \n",
    "    print(f\"\\n=== Embeddings Generated ===\")\n",
    "    print(f\"Shape: {embeddings.shape}\")\n",
    "    print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "    print(f\"Value range: [{embeddings.min():.4f}, {embeddings.max():.4f}]\")\n",
    "    \n",
    "    EMBEDDINGS_OK = True\n",
    "    \n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLAY NOT AVAILABLE - Creating placeholder embeddings\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nTo generate real embeddings:\")\n",
    "    print(\"1. Install Clay: pip install git+https://github.com/Clay-foundation/model.git\")\n",
    "    print(\"2. Download checkpoint: wget https://huggingface.co/made-with-clay/Clay/resolve/main/v1.5/clay-v1.5.ckpt\")\n",
    "    print(\"\\nCreating PCA-based placeholder embeddings for demonstration...\")\n",
    "    \n",
    "    # Create placeholder embeddings using PCA\n",
    "    flat_chips = chips_norm.reshape(chips_norm.shape[0], -1)\n",
    "    pca = PCA(n_components=min(128, flat_chips.shape[1]), random_state=cfg.random_seed)\n",
    "    embeddings = pca.fit_transform(flat_chips).astype(np.float32)\n",
    "    \n",
    "    print(f\"Placeholder embeddings shape: {embeddings.shape}\")\n",
    "    print(\"Note: These are NOT real Clay embeddings!\")\n",
    "    \n",
    "    EMBEDDINGS_OK = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embedding distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Histogram of embedding values\n",
    "axes[0].hist(embeddings.flatten(), bins=50, edgecolor='none', alpha=0.7)\n",
    "axes[0].set_xlabel(\"Embedding value\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].set_title(\"Distribution of embedding values\")\n",
    "\n",
    "# Per-dimension mean\n",
    "dim_means = embeddings.mean(axis=0)\n",
    "axes[1].bar(range(len(dim_means)), dim_means, width=1.0, color='steelblue', edgecolor='none')\n",
    "axes[1].set_xlabel(\"Dimension\")\n",
    "axes[1].set_ylabel(\"Mean value\")\n",
    "axes[1].set_title(\"Per-dimension mean\")\n",
    "axes[1].axhline(0, color='black', linewidth=0.5)\n",
    "\n",
    "# PCA projection\n",
    "pca_2d = PCA(n_components=2, random_state=cfg.random_seed)\n",
    "emb_2d = pca_2d.fit_transform(embeddings)\n",
    "\n",
    "axes[2].scatter(emb_2d[:, 0], emb_2d[:, 1], s=30, alpha=0.7, c=range(len(emb_2d)), cmap='viridis')\n",
    "axes[2].set_xlabel(f\"PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% var)\")\n",
    "axes[2].set_ylabel(f\"PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% var)\")\n",
    "axes[2].set_title(\"Embeddings (PCA projection)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "if EMBEDDINGS_OK:\n",
    "    print(\"(768 = Clay v1, 1024 = Clay v1.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8mc063nem1b",
   "source": "### Visualizing the Embedding Space\n\nThe plots below help us understand the structure of our embeddings:\n\n1. **Distribution of values**: Shows the range and spread of embedding values. A roughly symmetric distribution centered near zero is typical.\n\n2. **Per-dimension mean**: Each of the 1024 dimensions may capture different features. Some dimensions consistently activate (positive mean), others consistently deactivate (negative mean).\n\n3. **PCA projection**: Projects 1024 dimensions to 2D. Points close together have similar embeddings (and likely similar content or position).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "part6-header",
   "metadata": {},
   "source": "## Part 6: Save Embeddings\n\n### Why Save Embeddings?\n\nGenerating embeddings is computationally expensive (requires GPU, takes time). Once generated, embeddings can be:\n- **Reused** for multiple downstream tasks (classification, retrieval, clustering)\n- **Shared** with others who don't have GPU access\n- **Combined** with embeddings from other regions or time periods\n- **Indexed** for fast similarity search at scale\n\n### The Parquet Format\n\nWe save embeddings as **Parquet** files, a columnar storage format that:\n- Compresses numerical data efficiently\n- Supports complex types (lists, nested structures)\n- Is widely supported (pandas, PyArrow, Spark, DuckDB)\n- Enables fast partial reads (only load columns you need)\n\n### Our Schema\n\n| Column | Type | Description |\n|--------|------|-------------|\n| `item_id` | string | Unique identifier for each chip |\n| `embeddings` | list[float] | The 1024-dim embedding vector |\n| `geometry` | bytes (WKB) | Geographic bounds of the chip |\n| `chip_row` | int | Row position in the original image grid |\n| `chip_col` | int | Column position in the original image grid |\n\n### Why WKB for Geometry?\n\n**WKB (Well-Known Binary)** is a compact binary format for geometries:\n- More space-efficient than text formats (WKT)\n- Compatible with GeoParquet specification\n- Readable by GIS tools (QGIS, PostGIS, GeoPandas)\n\nThis allows you to later visualize or query embeddings spatially."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chip_geometries(chip_info: List[dict], imagery_shape: Tuple, \n",
    "                           cfg: Config) -> List[bytes]:\n",
    "    \"\"\"\n",
    "    Create WKB geometries for each chip.\n",
    "    \n",
    "    Returns:\n",
    "        List of WKB-encoded polygon geometries\n",
    "    \"\"\"\n",
    "    if not SHAPELY_OK:\n",
    "        print(\"Shapely not available - storing placeholder geometries\")\n",
    "        return [b\"\" for _ in chip_info]\n",
    "    \n",
    "    _, height, width = imagery_shape\n",
    "    min_lon, min_lat, max_lon, max_lat = cfg.bbox\n",
    "    \n",
    "    # Pixel to geo coordinate conversion\n",
    "    def pixel_to_geo(px, py):\n",
    "        lon = min_lon + (px / width) * (max_lon - min_lon)\n",
    "        lat = max_lat - (py / height) * (max_lat - min_lat)\n",
    "        return lon, lat\n",
    "    \n",
    "    geometries = []\n",
    "    for info in chip_info:\n",
    "        x0, y0 = info[\"x0\"], info[\"y0\"]\n",
    "        x1, y1 = x0 + cfg.chip_size, y0 + cfg.chip_size\n",
    "        \n",
    "        # Get corners in lon/lat\n",
    "        lon0, lat0 = pixel_to_geo(x0, y0)\n",
    "        lon1, lat1 = pixel_to_geo(x1, y1)\n",
    "        \n",
    "        # Create polygon\n",
    "        poly = box(lon0, lat1, lon1, lat0)  # (minx, miny, maxx, maxy)\n",
    "        geometries.append(wkb.dumps(poly))\n",
    "    \n",
    "    return geometries\n",
    "\n",
    "\n",
    "# Create geometries\n",
    "geometries = create_chip_geometries(chip_info, imagery.shape, cfg)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"item_id\": [f\"{acq_metadata.get('item_id', 'generated')}_{i}\" for i in range(len(embeddings))],\n",
    "    \"embeddings\": [emb.tolist() for emb in embeddings],\n",
    "    \"geometry\": geometries,\n",
    "    \"chip_row\": [info[\"row\"] for info in chip_info],\n",
    "    \"chip_col\": [info[\"col\"] for info in chip_info],\n",
    "})\n",
    "\n",
    "print(\"DataFrame created:\")\n",
    "print(df.head())\n",
    "print(f\"\\nShape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-parquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Parquet\n",
    "output_path = Path(cfg.output_parquet)\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df.to_parquet(output_path, index=False)\n",
    "\n",
    "print(f\"Embeddings saved to: {output_path}\")\n",
    "print(f\"File size: {output_path.stat().st_size / 1e6:.2f} MB\")\n",
    "\n",
    "# Verify by reading back\n",
    "df_verify = pd.read_parquet(output_path)\n",
    "print(f\"\\nVerification - loaded {len(df_verify)} rows\")\n",
    "print(f\"Embedding length: {len(df_verify['embeddings'].iloc[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7-header",
   "metadata": {},
   "source": "## Part 7: Explore the Embedding Space\n\nNow that we have embeddings, we can explore and use them. This demonstrates workflows you learned in Session 2:\n\n### What Can We Do With Embeddings?\n\n| Task | How It Works |\n|------|--------------|\n| **Similarity Search** | Find chips with similar embeddings (nearest neighbors) |\n| **Classification** | Train a classifier on labeled embeddings |\n| **Clustering** | Group similar chips without labels (k-means, HDBSCAN) |\n| **Anomaly Detection** | Find chips that are \"far\" from all others |\n| **Visualization** | Project to 2D/3D with PCA, t-SNE, or UMAP |\n\n### Similarity Search with Cosine Distance\n\nWe use **cosine similarity** to find similar embeddings:\n- Measures the angle between vectors (ignores magnitude)\n- Value of 1.0 = identical direction, 0.0 = orthogonal\n- Works well for high-dimensional embeddings\n\n```python\ncosine_similarity = dot(A, B) / (norm(A) * norm(B))\ncosine_distance = 1 - cosine_similarity\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings we just saved\n",
    "emb_df = pd.read_parquet(cfg.output_parquet)\n",
    "\n",
    "# Extract embedding matrix\n",
    "X_all = np.vstack(emb_df[\"embeddings\"].values)\n",
    "\n",
    "print(f\"Loaded {len(emb_df)} embeddings\")\n",
    "print(f\"Embedding matrix shape: {X_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA visualization of embedding space\n",
    "pca = PCA(n_components=2, random_state=cfg.random_seed)\n",
    "X_pca = pca.fit_transform(X_all)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Colored by chip position (row)\n",
    "sc1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                      c=emb_df[\"chip_row\"], cmap='viridis', \n",
    "                      s=50, alpha=0.7)\n",
    "plt.colorbar(sc1, ax=axes[0], label='Chip row')\n",
    "axes[0].set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)\")\n",
    "axes[0].set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)\")\n",
    "axes[0].set_title(\"Embedding space (colored by row position)\")\n",
    "\n",
    "# Colored by chip position (column)\n",
    "sc2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                      c=emb_df[\"chip_col\"], cmap='plasma', \n",
    "                      s=50, alpha=0.7)\n",
    "plt.colorbar(sc2, ax=axes[1], label='Chip column')\n",
    "axes[1].set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)\")\n",
    "axes[1].set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)\")\n",
    "axes[1].set_title(\"Embedding space (colored by column position)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total variance explained: {pca.explained_variance_ratio_.sum()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jgy3y7av2yp",
   "source": "### Understanding the PCA Visualization\n\n**PCA (Principal Component Analysis)** projects our 1024-dimensional embeddings down to 2D for visualization. Each point represents one chip's embedding.\n\n### Why Do We See a Grid Pattern?\n\nIf you see a **regular grid pattern** in the PCA plot, this is expected behavior when processing chips from a single satellite image. Here's why:\n\n**Clay embeddings encode BOTH content AND position.** The model receives:\n- `time`: Same for all chips (one acquisition time)\n- `latlon`: Varies systematically in a grid pattern across the image\n\nSince all chips share the same timestamp, the **position encoding dominates** the embedding structure. The latitude and longitude of each chip vary in a regular grid pattern (row by row, column by column), creating the grid structure you see.\n\n### What This Means\n\n| Observation | Interpretation |\n|-------------|----------------|\n| Grid pattern | Position encoding is working correctly |\n| Smooth color gradient | Embeddings vary continuously with position |\n| Low variance explained | Most information is in higher dimensions |\n\n### When Would We NOT See a Grid?\n\nYou would see **semantic clustering** (grouping by land cover type) when:\n1. Chips come from **different locations** (diverse lat/lon, breaking the grid)\n2. Chips come from **different times** (varied timestamps)\n3. Position encoding is **zeroed out** (Clay's training sometimes does this)\n\n### The Takeaway\n\nThe grid pattern confirms that Clay is correctly encoding spatial information. For downstream tasks like classification, a linear classifier can learn to use BOTH the position-dependent AND content-dependent parts of the embedding.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similarity-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity search using nearest neighbors\n",
    "print(\"Building similarity search index...\")\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "X_norm = X_all / (np.linalg.norm(X_all, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "# Build index\n",
    "nn = NearestNeighbors(n_neighbors=min(10, len(X_norm)), metric=\"cosine\")\n",
    "nn.fit(X_norm)\n",
    "\n",
    "# Query with the first chip\n",
    "query_idx = 0\n",
    "distances, indices = nn.kneighbors(X_norm[query_idx:query_idx+1])\n",
    "\n",
    "print(f\"\\nQuery chip: {query_idx}\")\n",
    "print(f\"Nearest neighbors (by cosine similarity):\")\n",
    "for rank, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "    print(f\"  {rank+1}. idx={idx:4d}  distance={dist:.4f}  row={emb_df.iloc[idx]['chip_row']}  col={emb_df.iloc[idx]['chip_col']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize query and retrieved chips\n",
    "k_show = min(6, len(indices[0]))\n",
    "\n",
    "fig, axes = plt.subplots(2, k_show, figsize=(3 * k_show, 6))\n",
    "\n",
    "for i, idx in enumerate(indices[0][:k_show]):\n",
    "    chip = chips_norm[idx]\n",
    "    \n",
    "    # Create RGB with percentile normalization\n",
    "    if chip.shape[0] >= 3:\n",
    "        def chip_normalize(band):\n",
    "            vmin, vmax = np.percentile(band, [2, 98])\n",
    "            return np.clip((band - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "        rgb = np.stack([chip_normalize(chip[2]), chip_normalize(chip[1]), chip_normalize(chip[0])], axis=-1)\n",
    "    else:\n",
    "        rgb = chip[0]\n",
    "    \n",
    "    # Top row: RGB\n",
    "    axes[0, i].imshow(rgb, cmap=\"gray\" if rgb.ndim == 2 else None)\n",
    "    if i == 0:\n",
    "        axes[0, i].set_title(f\"Query (idx={idx})\\ndist=0.0000\", fontsize=10)\n",
    "    else:\n",
    "        axes[0, i].set_title(f\"Neighbor {i}\\nidx={idx}, dist={distances[0][i]:.4f}\", fontsize=10)\n",
    "    axes[0, i].axis(\"off\")\n",
    "    \n",
    "    # Bottom row: embedding bar chart\n",
    "    emb = embeddings[idx][:50]  # Show first 50 dimensions\n",
    "    axes[1, i].bar(range(len(emb)), emb, width=1.0, color='steelblue', edgecolor='none')\n",
    "    axes[1, i].set_xlabel(\"Dimension (first 50)\")\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel(\"Value\")\n",
    "    axes[1, i].set_ylim([embeddings[:, :50].min(), embeddings[:, :50].max()])\n",
    "\n",
    "plt.suptitle(\"Similarity Search Results\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-neighbors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spatial distribution of nearest neighbors\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# All chips (gray)\n",
    "ax.scatter(emb_df[\"chip_col\"], emb_df[\"chip_row\"], \n",
    "           s=50, c='lightgray', alpha=0.5, label='All chips')\n",
    "\n",
    "# Nearest neighbors (colored by distance)\n",
    "neighbor_rows = [emb_df.iloc[idx][\"chip_row\"] for idx in indices[0]]\n",
    "neighbor_cols = [emb_df.iloc[idx][\"chip_col\"] for idx in indices[0]]\n",
    "\n",
    "sc = ax.scatter(neighbor_cols, neighbor_rows, \n",
    "                c=distances[0], cmap='viridis_r', \n",
    "                s=200, edgecolor='black', linewidth=1.5,\n",
    "                label='Nearest neighbors')\n",
    "plt.colorbar(sc, ax=ax, label='Cosine distance')\n",
    "\n",
    "# Mark query with star\n",
    "ax.scatter([emb_df.iloc[query_idx][\"chip_col\"]], \n",
    "           [emb_df.iloc[query_idx][\"chip_row\"]],\n",
    "           marker='*', s=400, c='red', edgecolor='black',\n",
    "           label='Query chip', zorder=10)\n",
    "\n",
    "ax.set_xlabel(\"Chip column\")\n",
    "ax.set_ylabel(\"Chip row\")\n",
    "ax.set_title(\"Spatial distribution of nearest neighbors\")\n",
    "ax.legend(loc='upper right')\n",
    "ax.invert_yaxis()  # Match image coordinates\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Embeddings capture semantic similarity, not just spatial proximity.\")\n",
    "print(\"Similar embeddings may come from distant locations if they have similar content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part8-header",
   "metadata": {},
   "source": [
    "## Part 8: Summary\n",
    "\n",
    "Congratulations! You have learned to generate your own Clay embeddings from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"SESSION 2B SUMMARY: Generate Your Own Clay Embeddings\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- Data Acquisition ---\")\n",
    "print(f\"Source:           {acq_metadata['source']}\")\n",
    "print(f\"Image size:       {imagery.shape[1]} x {imagery.shape[2]} pixels\")\n",
    "print(f\"Bands:            {len(cfg.bands)} ({', '.join(cfg.bands)})\")\n",
    "\n",
    "print(\"\\n--- Chipping ---\")\n",
    "print(f\"Chip size:        {cfg.chip_size} x {cfg.chip_size} pixels\")\n",
    "print(f\"Valid chips:      {len(chips_norm)}\")\n",
    "\n",
    "print(\"\\n--- Embeddings ---\")\n",
    "print(f\"Embedding dim:    {embeddings.shape[1]}\")\n",
    "print(f\"Total embeddings: {len(embeddings)}\")\n",
    "print(f\"Generated with:   {'Clay encoder' if EMBEDDINGS_OK else 'PCA (placeholder)'}\")\n",
    "\n",
    "print(\"\\n--- Output ---\")\n",
    "print(f\"Saved to:         {cfg.output_parquet}\")\n",
    "print(f\"File size:        {Path(cfg.output_parquet).stat().st_size / 1e6:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WHAT YOU LEARNED:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. Acquire satellite imagery via STAC or sample files\")\n",
    "print(\"2. Chip imagery into 256x256 tiles for Clay\")\n",
    "print(\"3. Prepare Clay inputs (pixels, timestamps, wavelengths)\")\n",
    "print(\"4. Generate embeddings using Clay's encoder\")\n",
    "print(\"5. Save embeddings in Session 2-compatible format\")\n",
    "print(\"6. Use embeddings for similarity search\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"- Session 3: Fine-tune Clay with LoRA for your specific task\")\n",
    "print(\"- Try different AOIs and time ranges\")\n",
    "print(\"- Add labels and train a classifier (like Session 2)\")\n",
    "print(\"- Scale up: process larger areas with batching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-comparison-header",
   "metadata": {},
   "source": [
    "## Optional: Compare with Session 2 Precomputed Embeddings\n",
    "\n",
    "If you have the Session 2 precomputed embeddings, you can compare statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load Session 2 precomputed embeddings for comparison\n",
    "session2_dir = Path(\"./data/practical2_clay\")\n",
    "session2_files = list(session2_dir.glob(\"*.gpq\")) if session2_dir.exists() else []\n",
    "\n",
    "if len(session2_files) > 0:\n",
    "    print(f\"Found {len(session2_files)} Session 2 embedding files\")\n",
    "    \n",
    "    # Load first file\n",
    "    s2_df = pd.read_parquet(session2_files[0])\n",
    "    s2_emb = np.vstack(s2_df[\"embeddings\"].values)\n",
    "    \n",
    "    print(f\"\\nSession 2 embeddings: {s2_emb.shape}\")\n",
    "    print(f\"Your embeddings:      {embeddings.shape}\")\n",
    "    \n",
    "    print(f\"\\n--- Comparison ---\")\n",
    "    print(f\"Session 2 dim: {s2_emb.shape[1]}  (768 = Clay v1)\")\n",
    "    print(f\"Your dim:      {embeddings.shape[1]}  ({'Clay v1.5' if embeddings.shape[1] == 1024 else 'Clay v1' if embeddings.shape[1] == 768 else 'Other'})\")\n",
    "    \n",
    "    print(f\"\\n--- Value Statistics ---\")\n",
    "    print(f\"Session 2 mean: {s2_emb.mean():.4f}, std: {s2_emb.std():.4f}\")\n",
    "    print(f\"Your mean:      {embeddings.mean():.4f}, std: {embeddings.std():.4f}\")\n",
    "else:\n",
    "    print(\"Session 2 precomputed embeddings not found.\")\n",
    "    print(\"Run Session 2 first to download the precomputed embeddings for comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6a2fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r2ska-clay",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}