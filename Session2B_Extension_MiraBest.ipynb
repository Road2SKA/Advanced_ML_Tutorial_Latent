{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Practical 2B - Extension: Embeddings Workflow on MiraBest Radio Galaxies\n",
    "\n",
    "**Road to SKA: Foundation Models, Embeddings, and Latent Spaces**\n",
    "\n",
    "This notebook applies the **embeddings-first workflow** from Session 2A to radio galaxy classification using the MiraBest dataset.\n",
    "\n",
    "## What you will learn\n",
    "\n",
    "1. Generate embeddings for radio galaxy images (PCA baseline or autoencoder)\n",
    "2. Train lightweight classifiers (Random Forest, Logistic Regression) on embeddings\n",
    "3. Evaluate FR classification performance with confusion matrices\n",
    "4. Build a similarity search system to find morphologically similar galaxies\n",
    "\n",
    "---\n",
    "\n",
    "## The Embeddings-First Workflow\n",
    "\n",
    "This practical demonstrates the same pattern used with foundation models:\n",
    "\n",
    "1. **Freeze** the encoder (or use a simple embedding method)\n",
    "2. **Extract embeddings** for all images\n",
    "3. **Train a lightweight head** (Random Forest, Logistic Regression) on embeddings\n",
    "4. **Run inference** using the small classifier\n",
    "\n",
    "We provide two embedding options:\n",
    "- **Option A**: Use the autoencoder trained in Session 1A (if available)\n",
    "- **Option B**: PCA on flattened pixel values (simpler baseline, no dependencies)\n",
    "\n",
    "---\n",
    "\n",
    "## About MiraBest\n",
    "\n",
    "**MiraBest** contains ~800 labelled Fanaroff-Riley radio galaxies in CIFAR-style pickle format:\n",
    "- **FRI (class 0)**: Edge-darkened — jets fade with distance\n",
    "- **FRII (class 1)**: Edge-brightened — bright hotspots at jet termination\n",
    "\n",
    "References:\n",
    "- Zenodo: https://doi.org/10.5281/zenodo.4288837\n",
    "- Paper: https://academic.oup.com/rasti/article/2/1/293/7202349"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7tdos3ifd94",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Road2SKA/Advanced_ML_Tutorial_Latent/blob/colab/Session2B_Extension_MiraBest.ipynb)\n\n---\n\n## Environment Setup (Colab / Local)\n\nRun the cell below to detect your environment and set up paths. On **Google Colab**, it will install required packages automatically.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "sv7tny4b2np",
   "source": "# Detect environment and set up paths\nimport sys\n\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"Running in Google Colab\")\n    DATA_ROOT = '/content/data'\n    # No additional packages needed for this notebook\nelse:\n    print(\"Running locally\")\n    DATA_ROOT = './data'\n\nprint(f\"Data directory: {DATA_ROOT}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Optional: PyTorch for autoencoder embeddings\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    TORCH_OK = True\n",
    "except ImportError:\n",
    "    TORCH_OK = False\n",
    "    print(\"PyTorch not available. Will use PCA embeddings only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": "# Paths\nDATA_DIR = Path(f\"{DATA_ROOT}/mirabest\")\nDATA_DIR.mkdir(parents=True, exist_ok=True)\n\n# Settings\nIMAGE_SIZE = 64\nEMBEDDING_DIM = 64  # dimension for PCA or autoencoder latent space (matches Session 1A)\nRANDOM_SEED = 42\nTEST_FRACTION = 0.2\n\n# Set random seed\nnp.random.seed(RANDOM_SEED)\n\n# FR class names\nCLASS_NAMES = [\"FRI\", \"FRII\"]\n\nprint(f\"Data directory: {DATA_DIR}\")\nprint(f\"Image size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\nprint(f\"Embedding dimension: {EMBEDDING_DIM}\")"
  },
  {
   "cell_type": "markdown",
   "id": "download_header",
   "metadata": {},
   "source": [
    "## 3. Download MiraBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIRABEST_TAR_URL = \"https://zenodo.org/records/4288837/files/batches.tar.gz?download=1\"\n",
    "\n",
    "def download_with_retries(url: str, dst: Path, retries: int = 5, chunk_size: int = 1 << 20):\n",
    "    \"\"\"\n",
    "    Download a file from URL with retry logic and progress bar.\n",
    "    \"\"\"\n",
    "    dst = Path(dst)\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if dst.exists() and dst.stat().st_size > 0:\n",
    "        print(f\"File already exists: {dst}\")\n",
    "        return\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            print(f\"Downloading {dst.name} (attempt {attempt + 1}/{retries})...\")\n",
    "            with requests.get(url, stream=True, timeout=120) as r:\n",
    "                r.raise_for_status()\n",
    "                total_size = int(r.headers.get('content-length', 0))\n",
    "                \n",
    "                with open(dst, \"wb\") as f:\n",
    "                    with tqdm(total=total_size, unit='B', unit_scale=True, desc=dst.name) as pbar:\n",
    "                        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                            if chunk:\n",
    "                                f.write(chunk)\n",
    "                                pbar.update(len(chunk))\n",
    "            print(f\"Downloaded: {dst}\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Download failed (attempt {attempt + 1}/{retries}): {repr(e)}\")\n",
    "            if dst.exists():\n",
    "                dst.unlink()\n",
    "    \n",
    "    raise RuntimeError(\n",
    "        f\"Could not download MiraBest after {retries} attempts.\\n\"\n",
    "        f\"You can manually download from Zenodo and place batches.tar.gz in {DATA_DIR}/\"\n",
    "    )\n",
    "\n",
    "# Download\n",
    "tar_path = DATA_DIR / \"batches.tar.gz\"\n",
    "download_with_retries(MIRABEST_TAR_URL, tar_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the tarball\n",
    "extract_dir = DATA_DIR / \"batches\"\n",
    "\n",
    "# Check for the nested batches/batches structure or data_batch files\n",
    "batches_inner = extract_dir / \"batches\"\n",
    "if batches_inner.exists() and (batches_inner / \"data_batch_1\").exists():\n",
    "    batches_path = batches_inner\n",
    "    print(f\"Using existing extraction: {batches_path}\")\n",
    "elif (extract_dir / \"data_batch_1\").exists():\n",
    "    batches_path = extract_dir\n",
    "    print(f\"Using existing extraction: {batches_path}\")\n",
    "else:\n",
    "    print(f\"Extracting {tar_path.name}...\")\n",
    "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=extract_dir)\n",
    "    \n",
    "    # Handle nested extraction\n",
    "    if (extract_dir / \"batches\" / \"data_batch_1\").exists():\n",
    "        batches_path = extract_dir / \"batches\"\n",
    "    else:\n",
    "        batches_path = extract_dir\n",
    "    print(f\"Extracted to: {batches_path}\")\n",
    "\n",
    "# Verify extraction\n",
    "batch_files = sorted(batches_path.glob(\"data_batch_*\"))\n",
    "print(f\"Found {len(batch_files)} data batch files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_header",
   "metadata": {},
   "source": [
    "## 4. Load Images and Labels\n",
    "\n",
    "MiraBest uses a CIFAR-style pickle format with 150×150 grayscale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_images",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mirabest_batch(batch_path: Path):\n",
    "    \"\"\"\n",
    "    Load a single MiraBest batch file.\n",
    "    \"\"\"\n",
    "    with open(batch_path, 'rb') as f:\n",
    "        batch = pickle.load(f, encoding='bytes')\n",
    "    \n",
    "    # Handle both string and bytes keys\n",
    "    if b'data' in batch:\n",
    "        images = batch[b'data']\n",
    "        labels = batch[b'labels']\n",
    "    else:\n",
    "        images = batch['data']\n",
    "        labels = batch['labels']\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def load_all_mirabest(batches_path: Path, include_test: bool = True):\n",
    "    \"\"\"\n",
    "    Load all MiraBest data from batch files.\n",
    "    \"\"\"\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Load training batches\n",
    "    for i in range(1, 9):  # data_batch_1 through data_batch_8\n",
    "        batch_path = batches_path / f\"data_batch_{i}\"\n",
    "        if batch_path.exists():\n",
    "            images, labels = load_mirabest_batch(batch_path)\n",
    "            all_images.extend(images)\n",
    "            all_labels.extend(labels)\n",
    "            print(f\"Loaded {batch_path.name}: {len(images)} images\")\n",
    "    \n",
    "    # Optionally load test batch\n",
    "    if include_test:\n",
    "        test_path = batches_path / \"test_batch\"\n",
    "        if test_path.exists():\n",
    "            images, labels = load_mirabest_batch(test_path)\n",
    "            all_images.extend(images)\n",
    "            all_labels.extend(labels)\n",
    "            print(f\"Loaded test_batch: {len(images)} images\")\n",
    "    \n",
    "    return np.array(all_images), np.array(all_labels)\n",
    "\n",
    "\n",
    "# Load all data\n",
    "images_raw, labels = load_all_mirabest(batches_path)\n",
    "\n",
    "print(f\"\\nTotal: {len(images_raw)} images\")\n",
    "print(f\"Image shape: {images_raw[0].shape}\")\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "for i, name in enumerate(CLASS_NAMES):\n",
    "    count = (labels == i).sum()\n",
    "    print(f\"  {name} (class {i}): {count} images ({100*count/len(labels):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resize_images",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images: resize and apply asinh scaling for dynamic range\n",
    "def preprocess_images(images, target_size, use_log_scale=True):\n",
    "    \"\"\"\n",
    "    Preprocess images: resize and normalize.\n",
    "    \n",
    "    Radio astronomy images often have high dynamic range (bright cores, faint lobes).\n",
    "    asinh scaling helps compress this range and makes faint features more visible.\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for img in tqdm(images, desc=\"Preprocessing images\"):\n",
    "        pil_img = Image.fromarray(img)\n",
    "        pil_img = pil_img.resize((target_size, target_size), Image.BILINEAR)\n",
    "        arr = np.array(pil_img, dtype=np.float32)\n",
    "        \n",
    "        if use_log_scale:\n",
    "            # asinh scaling: like log but handles zeros\n",
    "            arr = np.arcsinh(arr / 10.0)\n",
    "        \n",
    "        processed.append(arr)\n",
    "    \n",
    "    processed = np.array(processed)\n",
    "    \n",
    "    # Normalize to [0, 1] range\n",
    "    vmin, vmax = processed.min(), processed.max()\n",
    "    processed = (processed - vmin) / (vmax - vmin + 1e-8)\n",
    "    \n",
    "    return processed\n",
    "\n",
    "\n",
    "# Preprocess images with asinh scaling\n",
    "images = preprocess_images(images_raw, IMAGE_SIZE, use_log_scale=True)\n",
    "print(f\"Preprocessed images shape: {images.shape}\")\n",
    "print(f\"Value range: [{images.min():.3f}, {images.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from each class\n",
    "fig, axes = plt.subplots(2, 8, figsize=(14, 4))\n",
    "\n",
    "for class_idx in range(2):\n",
    "    class_mask = labels == class_idx\n",
    "    class_images = images[class_mask]\n",
    "    \n",
    "    for j in range(8):\n",
    "        if j < len(class_images):\n",
    "            axes[class_idx, j].imshow(class_images[j], cmap=\"hot\")\n",
    "        axes[class_idx, j].axis(\"off\")\n",
    "        if j == 0:\n",
    "            axes[class_idx, j].set_ylabel(CLASS_NAMES[class_idx], fontsize=12)\n",
    "\n",
    "plt.suptitle(\"MiraBest Radio Galaxy Samples by FR Class\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split_header",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified train/test split\n",
    "indices = np.arange(len(images))\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    indices,\n",
    "    test_size=TEST_FRACTION,\n",
    "    stratify=labels,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "X_train_imgs = images[train_idx]\n",
    "X_test_imgs = images[test_idx]\n",
    "y_train = labels[train_idx]\n",
    "y_test = labels[test_idx]\n",
    "\n",
    "print(f\"Training set: {len(X_train_imgs)} images\")\n",
    "print(f\"Test set: {len(X_test_imgs)} images\")\n",
    "\n",
    "# Verify stratification\n",
    "print(\"\\nTraining set class distribution:\")\n",
    "for i, name in enumerate(CLASS_NAMES):\n",
    "    count = (y_train == i).sum()\n",
    "    print(f\"  {name}: {count} ({100*count/len(y_train):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embeddings_header",
   "metadata": {},
   "source": [
    "## 6. Generate Embeddings\n",
    "\n",
    "We provide two options for generating embeddings:\n",
    "\n",
    "- **Option A**: Load the autoencoder from Session 1A and use its encoder\n",
    "- **Option B**: Use PCA on flattened pixel values (simpler, no dependencies)\n",
    "\n",
    "Option B is the default as it works without needing to complete Session 1A first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embeddings_choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose embedding method\n",
    "# Set USE_AUTOENCODER = True if you have completed Session 1A and saved the model\n",
    "USE_AUTOENCODER = True\n",
    "\n",
    "autoencoder_path = DATA_DIR / \"mirabest_autoencoder.pth\"\n",
    "if USE_AUTOENCODER and not autoencoder_path.exists():\n",
    "    print(f\"Autoencoder not found at {autoencoder_path}\")\n",
    "    print(\"Falling back to PCA embeddings.\")\n",
    "    USE_AUTOENCODER = False\n",
    "\n",
    "print(f\"Embedding method: {'Autoencoder (Session 1A)' if USE_AUTOENCODER else 'PCA baseline'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embeddings_pca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_AUTOENCODER:\n",
    "    # Option B: PCA on flattened pixels\n",
    "    print(\"Generating PCA embeddings...\")\n",
    "    \n",
    "    # Flatten images\n",
    "    X_train_flat = X_train_imgs.reshape(len(X_train_imgs), -1)\n",
    "    X_test_flat = X_test_imgs.reshape(len(X_test_imgs), -1)\n",
    "    \n",
    "    # Fit PCA on training data\n",
    "    pca = PCA(n_components=EMBEDDING_DIM, random_state=RANDOM_SEED)\n",
    "    Z_train = pca.fit_transform(X_train_flat)\n",
    "    Z_test = pca.transform(X_test_flat)\n",
    "    \n",
    "    print(f\"PCA explained variance: {pca.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "    print(f\"Training embeddings: {Z_train.shape}\")\n",
    "    print(f\"Test embeddings: {Z_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embeddings_autoencoder",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_AUTOENCODER and TORCH_OK:\n",
    "    # Option A: Use autoencoder from Session 1A\n",
    "    print(\"Loading autoencoder from Session 1A...\")\n",
    "    \n",
    "    # Define the autoencoder architecture (must match Session 1A - deeper version with BatchNorm)\n",
    "    class ConvAutoencoder64(nn.Module):\n",
    "        def __init__(self, latent_dim: int = 64):\n",
    "            super().__init__()\n",
    "            # Deeper encoder with BatchNorm\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, 3, stride=2, padding=1),    # -> (B, 32, 32, 32)\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, 3, stride=2, padding=1),   # -> (B, 64, 16, 16)\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, 3, stride=2, padding=1),  # -> (B, 128, 8, 8)\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            self.enc_fc = nn.Linear(128 * 8 * 8, latent_dim)\n",
    "        \n",
    "        def encode(self, x):\n",
    "            h = self.encoder(x)\n",
    "            h = h.view(h.size(0), -1)\n",
    "            z = self.enc_fc(h)\n",
    "            return z\n",
    "    \n",
    "    # Load model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                         \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    checkpoint = torch.load(autoencoder_path, map_location=device, weights_only=False)\n",
    "    latent_dim = checkpoint.get('latent_dim', EMBEDDING_DIM)\n",
    "    \n",
    "    model = ConvAutoencoder64(latent_dim=latent_dim).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Loaded autoencoder with latent_dim={latent_dim}\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    def get_embeddings(images, model, device, batch_size=64):\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(images), batch_size):\n",
    "                batch = images[i:i+batch_size]\n",
    "                batch_tensor = torch.from_numpy(batch).unsqueeze(1).float().to(device)\n",
    "                z = model.encode(batch_tensor).cpu().numpy()\n",
    "                embeddings.append(z)\n",
    "        return np.concatenate(embeddings)\n",
    "    \n",
    "    Z_train = get_embeddings(X_train_imgs, model, device)\n",
    "    Z_test = get_embeddings(X_test_imgs, model, device)\n",
    "    \n",
    "    # Update EMBEDDING_DIM to match loaded model\n",
    "    EMBEDDING_DIM = latent_dim\n",
    "    \n",
    "    print(f\"Training embeddings: {Z_train.shape}\")\n",
    "    print(f\"Test embeddings: {Z_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz_embeddings_header",
   "metadata": {},
   "source": [
    "## 7. Visualize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding value distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Distribution of embedding values\n",
    "axes[0].hist(Z_train.flatten(), bins=50, alpha=0.7, edgecolor='none')\n",
    "axes[0].set_xlabel(\"Embedding value\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].set_title(f\"Distribution of embedding values ({Z_train.shape[1]} dims)\")\n",
    "\n",
    "# Variance per dimension\n",
    "dim_vars = Z_train.var(axis=0)\n",
    "axes[1].bar(range(len(dim_vars)), np.sort(dim_vars)[::-1], color='steelblue', edgecolor='none')\n",
    "axes[1].set_xlabel(\"Dimension (sorted by variance)\")\n",
    "axes[1].set_ylabel(\"Variance\")\n",
    "axes[1].set_title(\"Variance per embedding dimension\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_pca_scatter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D visualization of embeddings\n",
    "pca_2d = PCA(n_components=2, random_state=RANDOM_SEED)\n",
    "Z_train_2d = pca_2d.fit_transform(Z_train)\n",
    "Z_test_2d = pca_2d.transform(Z_test)\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training embeddings\n",
    "for i, name in enumerate(CLASS_NAMES):\n",
    "    mask = y_train == i\n",
    "    axes[0].scatter(Z_train_2d[mask, 0], Z_train_2d[mask, 1], \n",
    "                   c=colors[i], label=name, s=40, alpha=0.7, edgecolors='white', linewidth=0.5)\n",
    "axes[0].set_xlabel(\"PC1\")\n",
    "axes[0].set_ylabel(\"PC2\")\n",
    "axes[0].set_title(\"Training Embeddings (PCA projection)\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test embeddings\n",
    "for i, name in enumerate(CLASS_NAMES):\n",
    "    mask = y_test == i\n",
    "    axes[1].scatter(Z_test_2d[mask, 0], Z_test_2d[mask, 1], \n",
    "                   c=colors[i], label=name, s=40, alpha=0.7, edgecolors='white', linewidth=0.5)\n",
    "axes[1].set_xlabel(\"PC1\")\n",
    "axes[1].set_ylabel(\"PC2\")\n",
    "axes[1].set_title(\"Test Embeddings (PCA projection)\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f\"MiraBest Embeddings — {EMBEDDING_DIM}D {'Autoencoder' if USE_AUTOENCODER else 'PCA'}\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"2D PCA explains {pca_2d.explained_variance_ratio_.sum()*100:.1f}% of embedding variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classifier_header",
   "metadata": {},
   "source": [
    "## 8. Train Classifiers on Embeddings\n",
    "\n",
    "We train two lightweight classifiers:\n",
    "- **Random Forest**: robust, handles non-linear boundaries\n",
    "- **Logistic Regression**: fast linear baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_classifiers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize embeddings (important for distance-based methods)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Z_train_scaled = scaler.fit_transform(Z_train)\n",
    "Z_test_scaled = scaler.transform(Z_test)\n",
    "\n",
    "# Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200, \n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(Z_train_scaled, y_train)\n",
    "y_pred_rf = rf.predict(Z_test_scaled)\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr = LogisticRegression(\n",
    "    max_iter=2000, \n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "lr.fit(Z_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(Z_test_scaled)\n",
    "\n",
    "# kNN (try different k values)\n",
    "print(\"Training kNN...\")\n",
    "best_knn_acc = 0\n",
    "best_k = 5\n",
    "for k in [3, 5, 7, 11]:\n",
    "    knn_temp = KNeighborsClassifier(n_neighbors=k, metric='cosine')\n",
    "    knn_temp.fit(Z_train_scaled, y_train)\n",
    "    acc = accuracy_score(y_test, knn_temp.predict(Z_test_scaled))\n",
    "    if acc > best_knn_acc:\n",
    "        best_knn_acc = acc\n",
    "        best_k = k\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=best_k, metric='cosine')\n",
    "knn.fit(Z_train_scaled, y_train)\n",
    "y_pred_knn = knn.predict(Z_test_scaled)\n",
    "\n",
    "print(f\"\\nDone! (Best kNN: k={best_k})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_header",
   "metadata": {},
   "source": [
    "## 9. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracies\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "acc_knn = accuracy_score(y_test, y_pred_knn)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"CLASSIFICATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nEmbedding method: {'Autoencoder' if USE_AUTOENCODER else 'PCA'} ({EMBEDDING_DIM} dimensions)\")\n",
    "print(f\"Random baseline (2 classes): {0.5:.3f}\")\n",
    "print(f\"\\nTest set accuracies:\")\n",
    "print(f\"  Random Forest:       {acc_rf:.3f}\")\n",
    "print(f\"  Logistic Regression: {acc_lr:.3f}\")\n",
    "print(f\"  kNN (k=5):           {acc_knn:.3f}\")\n",
    "\n",
    "# Detailed report for best model\n",
    "best_model = \"Random Forest\" if acc_rf >= acc_lr else \"Logistic Regression\"\n",
    "best_pred = y_pred_rf if acc_rf >= acc_lr else y_pred_lr\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Detailed Report ({best_model})\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, best_pred, target_names=CLASS_NAMES, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, (name, preds, acc) in zip(axes, [\n",
    "    (\"Random Forest\", y_pred_rf, acc_rf),\n",
    "    (\"Logistic Regression\", y_pred_lr, acc_lr),\n",
    "    (\"kNN (k=5)\", y_pred_knn, acc_knn)\n",
    "]):\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASS_NAMES)\n",
    "    disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
    "    ax.set_title(f\"{name}\\nAccuracy: {acc:.3f}\")\n",
    "\n",
    "plt.suptitle(f\"Confusion Matrices — {EMBEDDING_DIM}D {'Autoencoder' if USE_AUTOENCODER else 'PCA'} Embeddings\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_per_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class accuracy comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "x = np.arange(len(CLASS_NAMES))\n",
    "width = 0.25\n",
    "\n",
    "# Calculate per-class accuracies\n",
    "for offset, (name, preds, color) in enumerate([\n",
    "    (\"Random Forest\", y_pred_rf, '#1f77b4'),\n",
    "    (\"Logistic Regression\", y_pred_lr, '#ff7f0e'),\n",
    "    (\"kNN\", y_pred_knn, '#2ca02c')\n",
    "]):\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    per_class = cm.diagonal() / cm.sum(axis=1)\n",
    "    bars = ax.bar(x + offset * width, per_class, width, label=name, color=color)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, per_class):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, val + 0.02, f\"{val:.2f}\", \n",
    "               ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_xlabel(\"FR Class\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Per-Class Accuracy by Classifier\")\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(CLASS_NAMES)\n",
    "ax.set_ylim(0, 1.15)\n",
    "ax.legend(loc='upper right')\n",
    "ax.axhline(0.5, color='red', linestyle='--', alpha=0.5, label='Random baseline')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples of correct and incorrect classifications\n",
    "best_pred = y_pred_rf  # Use Random Forest predictions\n",
    "\n",
    "correct_mask = best_pred == y_test\n",
    "correct_idx = np.where(correct_mask)[0]\n",
    "incorrect_idx = np.where(~correct_mask)[0]\n",
    "\n",
    "n_examples = min(6, len(correct_idx), max(1, len(incorrect_idx)))\n",
    "\n",
    "fig, axes = plt.subplots(2, n_examples, figsize=(2 * n_examples, 4.5))\n",
    "if n_examples == 1:\n",
    "    axes = axes.reshape(2, 1)\n",
    "\n",
    "# Correct predictions\n",
    "for i in range(n_examples):\n",
    "    if i < len(correct_idx):\n",
    "        idx = correct_idx[i]\n",
    "        axes[0, i].imshow(X_test_imgs[idx], cmap=\"hot\")\n",
    "        axes[0, i].set_title(f\"True: {CLASS_NAMES[y_test[idx]]}\\nPred: {CLASS_NAMES[best_pred[idx]]}\", \n",
    "                           fontsize=9, color='green')\n",
    "    axes[0, i].axis(\"off\")\n",
    "\n",
    "# Incorrect predictions\n",
    "for i in range(n_examples):\n",
    "    if i < len(incorrect_idx):\n",
    "        idx = incorrect_idx[i]\n",
    "        axes[1, i].imshow(X_test_imgs[idx], cmap=\"hot\")\n",
    "        axes[1, i].set_title(f\"True: {CLASS_NAMES[y_test[idx]]}\\nPred: {CLASS_NAMES[best_pred[idx]]}\", \n",
    "                           fontsize=9, color='red')\n",
    "    else:\n",
    "        axes[1, i].text(0.5, 0.5, \"No errors!\", ha='center', va='center', fontsize=10)\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "axes[0, 0].set_ylabel(\"Correct\", fontsize=11)\n",
    "axes[1, 0].set_ylabel(\"Incorrect\", fontsize=11)\n",
    "plt.suptitle(\"Classification Examples (Random Forest)\", fontsize=12)\n",
    "#plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Correct: {correct_mask.sum()} / {len(y_test)} ({100*correct_mask.mean():.1f}%)\")\n",
    "print(f\"Incorrect: {(~correct_mask).sum()} / {len(y_test)} ({100*(~correct_mask).mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similarity_header",
   "metadata": {},
   "source": [
    "## 10. Similarity Search\n",
    "\n",
    "Because embeddings capture morphological features, we can find similar galaxies using vector distance. This is useful for:\n",
    "- **QA/debugging**: \"show me galaxies similar to this misclassified one\"\n",
    "- **Data curation**: \"find more examples like my few positives\"\n",
    "- **Discovery**: \"what other galaxies look like this unusual one?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similarity_index",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a nearest-neighbor index using all embeddings\n",
    "Z_all = np.vstack([Z_train, Z_test])\n",
    "y_all = np.concatenate([y_train, y_test])\n",
    "imgs_all = np.vstack([X_train_imgs, X_test_imgs])\n",
    "\n",
    "# Normalize for cosine similarity\n",
    "Z_norm = Z_all / (np.linalg.norm(Z_all, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "# Build index\n",
    "nn_index = NearestNeighbors(n_neighbors=12, metric='cosine')\n",
    "nn_index.fit(Z_norm)\n",
    "\n",
    "print(f\"Built similarity index with {len(Z_all)} embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similarity_search",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_similar_galaxies(query_idx, k=11):\n",
    "    \"\"\"\n",
    "    Show a query galaxy and its k most similar galaxies.\n",
    "    \"\"\"\n",
    "    distances, indices = nn_index.kneighbors(Z_norm[query_idx:query_idx+1], n_neighbors=k+1)\n",
    "    \n",
    "    # First result is the query itself\n",
    "    indices = indices[0]\n",
    "    distances = distances[0]\n",
    "    \n",
    "    n_cols = min(6, k + 1)\n",
    "    n_rows = (k + 1 + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(2 * n_cols, 2.5 * n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "    \n",
    "    for i, (idx, dist) in enumerate(zip(indices, distances)):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "        axes[i].imshow(imgs_all[idx], cmap=\"hot\")\n",
    "        label = CLASS_NAMES[y_all[idx]]\n",
    "        if i == 0:\n",
    "            axes[i].set_title(f\"QUERY\\n{label}\", fontsize=10, color='blue', fontweight='bold')\n",
    "        else:\n",
    "            axes[i].set_title(f\"{label}\\nd={dist:.3f}\", fontsize=9)\n",
    "        axes[i].axis(\"off\")\n",
    "    \n",
    "    # Hide unused axes\n",
    "    for i in range(len(indices), len(axes)):\n",
    "        axes[i].axis(\"off\")\n",
    "    \n",
    "    plt.suptitle(f\"Similar Galaxies (Query idx={query_idx}, class={CLASS_NAMES[y_all[query_idx]]})\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print class distribution of neighbors\n",
    "    neighbor_classes = [CLASS_NAMES[y_all[i]] for i in indices[1:]]\n",
    "    print(f\"Query class: {CLASS_NAMES[y_all[query_idx]]}\")\n",
    "    print(f\"Neighbor classes: {neighbor_classes}\")\n",
    "\n",
    "\n",
    "# Show examples from each class\n",
    "print(\"FRI query:\")\n",
    "fri_idx = np.where(y_all == 0)[0][0]\n",
    "show_similar_galaxies(fri_idx, k=11)\n",
    "\n",
    "print(\"\\nFRII query:\")\n",
    "frii_idx = np.where(y_all == 1)[0][0]\n",
    "show_similar_galaxies(frii_idx, k=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similarity_misclassified",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate misclassified examples\n",
    "if len(incorrect_idx) > 0:\n",
    "    print(\"Investigating a misclassified galaxy...\")\n",
    "    \n",
    "    # Get a misclassified test sample\n",
    "    misc_test_idx = incorrect_idx[0]\n",
    "    # Find its index in the combined array (test samples come after train)\n",
    "    misc_all_idx = len(Z_train) + misc_test_idx\n",
    "    \n",
    "    print(f\"\\nMisclassified sample:\")\n",
    "    print(f\"  True class: {CLASS_NAMES[y_test[misc_test_idx]]}\")\n",
    "    print(f\"  Predicted: {CLASS_NAMES[best_pred[misc_test_idx]]}\")\n",
    "    \n",
    "    show_similar_galaxies(misc_all_idx, k=11)\n",
    "else:\n",
    "    print(\"No misclassified examples to investigate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Key Results\n",
    "\n",
    "1. **Embeddings-first workflow**: We successfully classified FR radio galaxy morphologies using lightweight classifiers on embeddings, without training a deep network end-to-end.\n",
    "\n",
    "2. **Classification performance**: Both Random Forest and Logistic Regression achieve accuracy well above the random baseline (50%), demonstrating that embeddings capture FR-discriminative information.\n",
    "\n",
    "3. **Similarity search**: The embedding space preserves morphological similarity — similar-looking galaxies cluster together, enabling retrieval-based exploration.\n",
    "\n",
    "### Comparison with Session 2\n",
    "\n",
    "| Aspect | Session 2 (Clay/SF Bay) | Session 2A (MiraBest) |\n",
    "|--------|------------------------|----------------------|\n",
    "| Domain | Geospatial (satellite) | Radio astronomy |\n",
    "| Embeddings | Foundation model (768D) | PCA or Autoencoder (32D) |\n",
    "| Classes | 2 (marina/not marina) | 2 (FRI/FRII) |\n",
    "| Task | Binary classification | Binary classification |\n",
    "\n",
    "### Dataset Notes\n",
    "\n",
    "- MiraBest contains ~800 images in CIFAR-style pickle format\n",
    "- Binary classification: FRI (edge-darkened) vs FRII (edge-brightened)\n",
    "- Original images are 150×150, resized to 64×64\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Run with autoencoder**: Set `USE_AUTOENCODER = True` after completing Session 1A\n",
    "- **Try different embedding dimensions**: Does 64D or 128D improve classification?\n",
    "- **Data augmentation**: Would augmenting training data help?\n",
    "- **Foundation models**: What if we had a pretrained radio astronomy foundation model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7732f91",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r2ska-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}