{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a0fdca1",
   "metadata": {},
   "source": [
    "# Practical 3 (Path 2) — Fine-tuning a Geospatial Foundation Model with **TerraTorch** (Prithvi-EO-2.0, Flood Mapping)\n",
    "\n",
    "**Road to SKA: Foundation Models, Embeddings, and Latent Spaces**\n",
    "\n",
    "This practical is a **realistic EO workflow**: use the **TerraTorch** toolkit to fine-tune a **Geospatial Foundation Model (GFM)** for **flood segmentation**.\n",
    "\n",
    "We focus on:\n",
    "- **Prithvi-EO-2.0** (IBM + NASA) as the foundation backbone\n",
    "- **Sen1Floods11** as the downstream dataset (flood masks)\n",
    "- **TerraTorch** CLI + config-driven training\n",
    "- a **low-resource path**: short runs, small subsets, and PEFT pointers (LoRA/VPT)\n",
    "\n",
    "---\n",
    "\n",
    "## Key links (keep these handy)\n",
    "\n",
    "- Prithvi-EO-2.0 release + example configs/notebooks: https://github.com/NASA-IMPACT/Prithvi-EO-2.0  \n",
    "- Sen1Floods11 dataset repo (download instructions + citation): https://github.com/cloudtostreet/Sen1Floods11  \n",
    "- TerraTorch repo + quickstart + install notes (GDAL!): https://github.com/terrastackai/terratorch  \n",
    "- Fine-tuned reference model (flood segmentation): https://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-2.0-300M-TL-Sen1Floods11  \n",
    "- PEFT in geospatial (LoRA, VPT, ViT-adapter) integrated into TerraTorch: https://github.com/IBM/peft-geofm  \n",
    "\n",
    "> **Tip:** If installs get painful (GDAL) or downloads are blocked, skip to **Section 9 (Fallback paths)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf7164",
   "metadata": {},
   "source": [
    "## 0. What you’ll build\n",
    "\n",
    "A minimal end-to-end pipeline:\n",
    "\n",
    "1. Download a **tiny subset** of Sen1Floods11 chips (or use a pre-downloaded folder).\n",
    "2. Sanity check: read GeoTIFFs, visualise RGB + label.\n",
    "3. Create a **small TerraTorch config** (based on Prithvi-EO-2.0 example) and run:\n",
    "\n",
    "```bash\n",
    "terratorch fit -c sen1floods11_small.yaml\n",
    "```\n",
    "\n",
    "4. Evaluate a checkpoint and run inference on a sample chip.\n",
    "\n",
    "This is *config-first* to match how TerraTorch is used in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e624bde0",
   "metadata": {},
   "source": [
    "## 1. Setup and installs (TerraTorch + GDAL)\n",
    "\n",
    "TerraTorch depends on **GDAL**. The least painful way is usually **conda**:\n",
    "\n",
    "```bash\n",
    "conda create -n terratorch python=3.10 -y\n",
    "conda activate terratorch\n",
    "conda install -c conda-forge gdal -y\n",
    "pip install terratorch\n",
    "```\n",
    "\n",
    "If you *must* use pip only, you may need system GDAL already installed.\n",
    "\n",
    "In this notebook, we’ll assume the environment can run `terratorch` as a CLI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956699c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional pip installs for the notebook helper steps (visualisation + yaml)\n",
    "# (TerraTorch itself is installed separately; see instructions above)\n",
    "# %pip -q install rasterio pyyaml matplotlib numpy\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# rasterio can be finicky if GDAL isn't present\n",
    "import rasterio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c823eef",
   "metadata": {},
   "source": [
    "## 2. Data: Sen1Floods11\n",
    "\n",
    "Sen1Floods11 is hosted in a public Google Cloud Storage bucket and can be downloaded with `gsutil`.\n",
    "\n",
    "### Option A — Download the full dataset (~14GB)\n",
    "```bash\n",
    "gsutil -m rsync -r gs://sen1floods11 /home/files/sen1floods11\n",
    "```\n",
    "\n",
    "### Option B — Download a tiny subset (recommended for workshop)\n",
    "We’ll pull **just a handful of chips** for a quick run.\n",
    "\n",
    "> If you don't have `gsutil`, you can (1) install it, or (2) ask the workshop organisers to provide a pre-downloaded subset folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53682e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your local dataset directory here\n",
    "DATA_ROOT = \"./sen1floods11_subset\"   # put chips here (or point to pre-downloaded data)\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "\n",
    "print(\"DATA_ROOT:\", os.path.abspath(DATA_ROOT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e0bcd9",
   "metadata": {},
   "source": [
    "### 2.1 Download a small subset using `gsutil` (optional)\n",
    "\n",
    "This cell uses shell commands. If you're on Windows or don't have gsutil, skip it.\n",
    "\n",
    "We try to download:\n",
    "- one Sentinel-2 hand-labelled raster (`*_S2Hand.tif`) and\n",
    "- its corresponding flood mask (`*_LabelHand.tif`) if present in the bucket layout.\n",
    "\n",
    "Bucket structure can change between versions; if a path fails, consult the Sen1Floods11 README.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06e609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have gsutil available, uncomment and run this cell.\n",
    "# The pattern below is intentionally conservative; adjust as needed based on bucket layout/version.\n",
    "\n",
    "# %bash\n",
    "# set -e\n",
    "# mkdir -p sen1floods11_subset\n",
    "# echo \"Listing a few S2Hand files (may take a moment)...\"\n",
    "# gsutil ls \"gs://sen1floods11/**/**_S2Hand.tif\" | head -n 10\n",
    "#\n",
    "# # Pick ONE file from the list and copy it:\n",
    "# FILE=\"gs://sen1floods11/data/Bolivia/Bolivia_103757_S2Hand.tif\"\n",
    "# gsutil cp \"$FILE\" sen1floods11_subset/\n",
    "#\n",
    "# # Try common label name patterns (may need adjustment):\n",
    "# gsutil -m cp \"gs://sen1floods11/**/Bolivia_103757_LabelHand.tif\" sen1floods11_subset/ || true\n",
    "# gsutil -m cp \"gs://sen1floods11/**/Bolivia_103757_Label.tif\" sen1floods11_subset/ || true\n",
    "#\n",
    "# echo \"Done. Files:\"\n",
    "# ls -lh sen1floods11_subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee8376c",
   "metadata": {},
   "source": [
    "## 3. Sanity check: read a chip and visualise\n",
    "\n",
    "We’ll:\n",
    "- load one `*_S2Hand.tif` chip (Sentinel-2 bands)\n",
    "- (optionally) load the label chip\n",
    "- plot an RGB composite + mask\n",
    "\n",
    "If you don’t have labels downloaded, you can still visualise the imagery.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb4fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_one(pattern: str):\n",
    "    matches = sorted(glob.glob(os.path.join(DATA_ROOT, pattern)))\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "s2_path = find_one(\"*_S2Hand.tif\")\n",
    "lbl_path = find_one(\"*_Label*.tif\")  # tries LabelHand / Label etc.\n",
    "\n",
    "print(\"S2 chip:\", s2_path)\n",
    "print(\"Label:\", lbl_path)\n",
    "assert s2_path is not None, \"No *_S2Hand.tif found. Download a subset or point DATA_ROOT to the dataset folder.\"\n",
    "\n",
    "with rasterio.open(s2_path) as src:\n",
    "    s2 = src.read()  # (bands, H, W)\n",
    "    profile = src.profile\n",
    "\n",
    "print(\"S2 shape:\", s2.shape)\n",
    "print(\"dtype:\", s2.dtype)\n",
    "print(\"crs:\", profile.get(\"crs\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff18a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb(s2_arr):\n",
    "    # Sen1Floods11 chips often store S2 bands as a stack.\n",
    "    # If band order is unknown, we try a reasonable guess and clip to percentiles.\n",
    "    # You may need to adjust which indices correspond to (R,G,B).\n",
    "    bands, H, W = s2_arr.shape\n",
    "    if bands >= 3:\n",
    "        # guess: last 3 are RGB or first 3 are RGB; try first 3\n",
    "        rgb = np.stack([s2_arr[2], s2_arr[1], s2_arr[0]], axis=-1).astype(np.float32)\n",
    "    else:\n",
    "        rgb = np.repeat(s2_arr[0][...,None], 3, axis=-1).astype(np.float32)\n",
    "\n",
    "    # robust scaling\n",
    "    lo, hi = np.percentile(rgb, [2, 98])\n",
    "    rgb = np.clip((rgb - lo) / (hi - lo + 1e-6), 0, 1)\n",
    "    return rgb\n",
    "\n",
    "rgb = to_rgb(s2)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(rgb)\n",
    "plt.title(os.path.basename(s2_path))\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "if lbl_path is not None:\n",
    "    with rasterio.open(lbl_path) as src:\n",
    "        lbl = src.read(1)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(lbl, cmap=\"gray\")\n",
    "    plt.title(os.path.basename(lbl_path))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb458d",
   "metadata": {},
   "source": [
    "## 4. TerraTorch training: config-driven fine-tuning\n",
    "\n",
    "TerraTorch is typically run via CLI with a YAML config.\n",
    "\n",
    "### What we will do\n",
    "1. Create a **small** training config (`sen1floods11_small.yaml`) by starting from the Prithvi-EO-2.0 example and editing:\n",
    "   - dataset paths\n",
    "   - batch size / epochs\n",
    "   - limits for a short workshop run\n",
    "2. Run:\n",
    "```bash\n",
    "terratorch fit -c sen1floods11_small.yaml\n",
    "```\n",
    "\n",
    "> The Prithvi-EO-2.0 repo provides an example config for Sen1Floods11 and notes that the reference model was fine-tuned with `terratorch fit -c sen1floods11.yaml`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1714d5bb",
   "metadata": {},
   "source": [
    "### 4.1 Create a small config skeleton\n",
    "\n",
    "Because workshop environments differ, we generate a **template config** you can edit.\n",
    "\n",
    "If you have the official config file, you can download it and modify it instead:\n",
    "- https://github.com/NASA-IMPACT/Prithvi-EO-2.0 (see `configs/sen1floods11.yaml`)\n",
    "\n",
    "Below we provide a **minimal template** that you should treat as a starting point.\n",
    "\n",
    "> TerraTorch configs can evolve between versions. If something fails, check the TerraTorch docs / examples for your installed version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32641b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_OUT = \"sen1floods11_small.yaml\"\n",
    "\n",
    "# Minimal / illustrative template.\n",
    "# You will likely need to tweak keys to match your TerraTorch version and the dataset folder structure you downloaded.\n",
    "config = {\n",
    "    \"seed_everything\": 42,\n",
    "    \"trainer\": {\n",
    "        \"accelerator\": \"auto\",\n",
    "        \"devices\": 1,\n",
    "        \"max_epochs\": 2,\n",
    "        \"log_every_n_steps\": 10,\n",
    "        # Short-run controls (workshop):\n",
    "        \"limit_train_batches\": 20,\n",
    "        \"limit_val_batches\": 5,\n",
    "    },\n",
    "    \"model\": {\n",
    "        # You typically choose a backbone (Prithvi-EO-2.0) and a decoder head (e.g., UNet-like)\n",
    "        # Exact names depend on TerraTorch version.\n",
    "        \"task\": \"segmentation\",\n",
    "        \"backbone\": {\n",
    "            \"name\": \"prithvi_eo_v2_300_tl\",  # common shorthand in examples; adjust to match your registry\n",
    "            \"pretrained\": True,\n",
    "        },\n",
    "        \"decoder\": {\n",
    "            \"name\": \"unet\",                 # example; may be \"unet\" / \"fcn\" / etc. depending on version\n",
    "            \"num_classes\": 2,               # water vs non-water (often binary)\n",
    "        },\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"name\": \"sen1floods11\",\n",
    "        \"data_root\": DATA_ROOT,\n",
    "        \"batch_size\": 2,\n",
    "        \"num_workers\": 2,\n",
    "        # Optionally: patch size, bands, normalisation settings, etc.\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"name\": \"adamw\",\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 1e-2,\n",
    "    },\n",
    "    \"lr_scheduler\": {\n",
    "        \"name\": \"cosine\",\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(CONFIG_OUT, \"w\") as f:\n",
    "    yaml.safe_dump(config, f, sort_keys=False)\n",
    "\n",
    "print(\"Wrote:\", CONFIG_OUT)\n",
    "print(\"\\n--- config preview ---\")\n",
    "print(open(CONFIG_OUT, \"r\").read()[:1200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3258ed5",
   "metadata": {},
   "source": [
    "### 4.2 Run training\n",
    "\n",
    "If `terratorch` is installed and on PATH, run:\n",
    "\n",
    "```bash\n",
    "terratorch fit -c sen1floods11_small.yaml\n",
    "```\n",
    "\n",
    "You should see logs, checkpoints, and metrics.\n",
    "\n",
    "> If you get errors about model names/registries, check TerraTorch examples and adjust `model.backbone.name` / `decoder.name`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dabd751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run (requires TerraTorch installed + working config)\n",
    "# !terratorch fit -c {CONFIG_OUT}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d9dd07",
   "metadata": {},
   "source": [
    "## 5. (Optional) Evaluate and run inference\n",
    "\n",
    "Once training finishes, you typically have a `.ckpt` file.\n",
    "\n",
    "You can test using TerraTorch:\n",
    "\n",
    "```bash\n",
    "terratorch test -c sen1floods11_small.yaml --ckpt_path path/to/checkpoint.ckpt\n",
    "```\n",
    "\n",
    "Or export a Torch model and run inference over chips.\n",
    "\n",
    "Below is a placeholder cell you can adapt once you know your checkpoint path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244c0c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your checkpoint path if you ran training\n",
    "CKPT_PATH = None  # e.g., \"lightning_logs/version_0/checkpoints/epoch=1-step=....ckpt\"\n",
    "\n",
    "# Example test command:\n",
    "# !terratorch test -c {CONFIG_OUT} --ckpt_path {CKPT_PATH}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bd67ab",
   "metadata": {},
   "source": [
    "## 6. PEFT (LoRA / VPT) with TerraTorch\n",
    "\n",
    "Under tighter resource constraints, you often want **parameter-efficient** adaptation:\n",
    "- LoRA (train low-rank adapters)\n",
    "- Visual Prompt Tuning (train small prompt tokens)\n",
    "- ViT-Adapters (train small adapter modules)\n",
    "\n",
    "A good “known working” starting point is the **peft-geofm** repo, which integrates these methods into TerraTorch and provides configs:\n",
    "\n",
    "- https://github.com/IBM/peft-geofm\n",
    "\n",
    "### Suggested workshop activity\n",
    "1. Clone `peft-geofm`\n",
    "2. Choose a config for Prithvi-EO-2.0 + a downstream task (e.g., burn scars or floods if available)\n",
    "3. Run `terratorch fit -c <config.yaml>`\n",
    "\n",
    "> Config keys differ by model/task; the repo’s configs are the ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f791d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggested (optional) workflow:\n",
    "# !git clone https://github.com/IBM/peft-geofm\n",
    "# %cd peft-geofm\n",
    "# # Explore available configs:\n",
    "# !find configs -maxdepth 4 -type f -name \"*.yaml\" | head -n 30\n",
    "#\n",
    "# # Then run something like:\n",
    "# # !terratorch fit -c configs/peft/<...>/lora.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5492d51",
   "metadata": {},
   "source": [
    "## 7. Compare: full fine-tune vs PEFT (what to measure)\n",
    "\n",
    "Have students report:\n",
    "\n",
    "- **Trainable parameter count**\n",
    "- GPU memory / batch size feasible\n",
    "- Training time per epoch\n",
    "- Validation IoU / F1 (segmentation)\n",
    "- Robustness across flood events (generalisation)\n",
    "\n",
    "Even if you can't run a full benchmark in class, logging these values helps them “think like practitioners”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e661dfb",
   "metadata": {},
   "source": [
    "## 8. Connection to embeddings and latent spaces\n",
    "\n",
    "How this connects to Practical 2 (embeddings):\n",
    "\n",
    "- The **backbone encoder** produces a representation (latent embedding) per patch/token.\n",
    "- Decoders (UNet/FCN) turn these into per-pixel predictions.\n",
    "- PEFT methods (LoRA/VPT) nudge the latent space **just enough** to match the new task, without rewriting all weights.\n",
    "\n",
    "A useful mental model:\n",
    "\n",
    "> Pretraining learns a *general-purpose latent space*; fine-tuning learns a *task-aligned readout* (and sometimes small latent tweaks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d1305d",
   "metadata": {},
   "source": [
    "## 9. Fallback plans (low resources / low connectivity)\n",
    "\n",
    "If GPUs, storage, or bandwidth are limited:\n",
    "\n",
    "### Fallback A — Use the already fine-tuned flood model\n",
    "Use the reference model (Prithvi-EO-2.0-300M-TL-Sen1Floods11) and run inference only:\n",
    "- https://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-2.0-300M-TL-Sen1Floods11\n",
    "\n",
    "Students can still:\n",
    "- visualise predictions\n",
    "- compute IoU on a few labelled chips\n",
    "- do error analysis and qualitative checks\n",
    "\n",
    "### Fallback B — Use a tiny/small backbone\n",
    "IBM has released “tiny/small” variants of EO models (Prithvi/TerraMind) aimed at running on consumer devices.\n",
    "(See IBM Research blog posts and the TerraMind repo.)\n",
    "\n",
    "### Fallback C — Freeze the backbone, train decoder only\n",
    "In config:\n",
    "- set `backbone.pretrained = True`\n",
    "- freeze backbone parameters\n",
    "- train only the segmentation head/decoder\n",
    "\n",
    "### Fallback D — Train on *embeddings*, not pixels\n",
    "If you can’t run segmentation:\n",
    "- extract patch embeddings for chips (Practical 2)\n",
    "- train a lightweight classifier/regressor on top\n",
    "- use nearest-neighbour retrieval for weak localisation / similarity search\n",
    "\n",
    "### Fallback E — Reduce everything\n",
    "- fewer bands\n",
    "- fewer chips\n",
    "- smaller patches\n",
    "- fewer epochs\n",
    "- mixed precision if available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaddc9fd",
   "metadata": {},
   "source": [
    "## 10. Where this goes next\n",
    "\n",
    "If you want a capstone:\n",
    "\n",
    "1. Use STAC (Practical 2 Option B) to pull a real flood event AOI.\n",
    "2. Extract embeddings with a GFM encoder.\n",
    "3. Fine-tune with PEFT using a small set of hand labels.\n",
    "4. Deploy a “flood alert” notebook pipeline that:\n",
    "   - ingests new imagery\n",
    "   - runs inference\n",
    "   - produces a raster + quicklook map + summary metrics\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
