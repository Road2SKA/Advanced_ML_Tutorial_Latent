{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f6db01",
   "metadata": {},
   "source": [
    "# Practical 3 (Path 1) — Parameter-Efficient Fine-Tuning with LoRA (Vision, Minimal Dependencies)\n",
    "\n",
    "**Road to SKA: Foundation Models, Embeddings, and Latent Spaces**\n",
    "\n",
    "This practical demonstrates **Path 1**: LoRA on a small vision encoder *without* heavy EO plumbing.\n",
    "\n",
    "You will:\n",
    "\n",
    "1. Train a **base CNN classifier** (a stand-in for a pretrained foundation encoder).\n",
    "2. Freeze the base weights and adapt to a **shifted target task** using:\n",
    "   - **Full fine-tuning** (update all weights)\n",
    "   - **Head-only fine-tuning** (update only last layer)\n",
    "   - **LoRA fine-tuning** (update only small low-rank adapter matrices)\n",
    "3. Compare:\n",
    "   - accuracy vs training time\n",
    "   - number of trainable parameters\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "- LoRA paper (Hu et al., 2021 / ICLR 2022): https://arxiv.org/abs/2106.09685  \n",
    "- Microsoft LoRA repo / `loralib`: https://github.com/microsoft/LoRA  \n",
    "- Hugging Face PEFT LoRA docs (concept + config): https://huggingface.co/docs/peft/en/conceptual_guides/lora  \n",
    "\n",
    "> We implement LoRA **directly in PyTorch** here so it works anywhere (no Transformers dependency).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875db2c",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "CPU-friendly. Uses GPU if available.\n",
    "\n",
    "Required:\n",
    "- `torch`, `torchvision`, `numpy`, `matplotlib`, `scikit-learn`\n",
    "\n",
    "Optional:\n",
    "- `tqdm` (progress bars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional installs (uncomment if needed)\n",
    "# %pip -q install torch torchvision scikit-learn tqdm\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    tqdm = lambda x, **kwargs: x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c61a7",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "We create a **domain shift**:\n",
    "\n",
    "- **Base training:** MNIST with standard images  \n",
    "- **Target adaptation:** MNIST with rotation + noise, and **few-shot** labels\n",
    "\n",
    "This mimics adapting a pretrained encoder to a new domain with limited labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    data_dir: str = \"./data\"\n",
    "    seed: int = 42\n",
    "    batch_size: int = 128\n",
    "    base_epochs: int = 5\n",
    "    target_epochs: int = 5\n",
    "    lr_base: float = 1e-3\n",
    "    lr_target: float = 1e-3\n",
    "    max_base_train: int = 20000\n",
    "    max_target_train: int = 2000\n",
    "    max_test: int = 5000\n",
    "    lora_rank: int = 8\n",
    "    lora_alpha: float = 16.0\n",
    "    lora_dropout: float = 0.0\n",
    "    rotate_deg: float = 25.0\n",
    "    noise_std: float = 0.15\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "\n",
    "# Device selection: CUDA > MPS (Apple Silicon) > CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b989f7",
   "metadata": {},
   "source": [
    "## 2. Data: Base vs Target\n",
    "\n",
    "- Base transform: standard MNIST  \n",
    "- Target transform: rotation + gaussian noise (synthetic shift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise:\n",
    "    def __init__(self, std: float = 0.1):\n",
    "        self.std = std\n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.clamp(x + torch.randn_like(x) * self.std, 0.0, 1.0)\n",
    "\n",
    "base_tfm = transforms.Compose([transforms.ToTensor()])\n",
    "target_tfm = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=cfg.rotate_deg),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(std=cfg.noise_std),\n",
    "])\n",
    "test_tfm = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "base_train = datasets.MNIST(cfg.data_dir, train=True, download=True, transform=base_tfm)\n",
    "target_train = datasets.MNIST(cfg.data_dir, train=True, download=True, transform=target_tfm)\n",
    "test_ds = datasets.MNIST(cfg.data_dir, train=False, download=True, transform=test_tfm)\n",
    "\n",
    "def subset(ds, n: Optional[int], seed: int):\n",
    "    if n is None or n >= len(ds):\n",
    "        return ds\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = rng.choice(len(ds), size=n, replace=False)\n",
    "    return Subset(ds, idx)\n",
    "\n",
    "base_train_s = subset(base_train, cfg.max_base_train, cfg.seed)\n",
    "target_train_s = subset(target_train, cfg.max_target_train, cfg.seed + 1)\n",
    "test_s = subset(test_ds, cfg.max_test, cfg.seed + 2)\n",
    "\n",
    "# num_workers=0 for compatibility (set higher if you have multiprocessing issues resolved)\n",
    "base_loader = DataLoader(base_train_s, batch_size=cfg.batch_size, shuffle=True, num_workers=0)\n",
    "target_loader = DataLoader(target_train_s, batch_size=cfg.batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_s, batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Visualise base vs target samples\n",
    "xb, yb = next(iter(base_loader))\n",
    "xt, yt = next(iter(target_loader))\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(10, 3))\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    ax.imshow(xb[i,0], cmap=\"gray\"); ax.set_title(int(yb[i])); ax.axis(\"off\")\n",
    "for i, ax in enumerate(axes[1]):\n",
    "    ax.imshow(xt[i,0], cmap=\"gray\"); ax.set_title(int(yt[i])); ax.axis(\"off\")\n",
    "axes[0,0].set_ylabel(\"Base\")\n",
    "axes[1,0].set_ylabel(\"Target\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a8476d",
   "metadata": {},
   "source": [
    "## 3. Base model (small CNN)\n",
    "\n",
    "This is our “pretrained encoder + classifier head”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c3b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, n_classes: int = 10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 14x14\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 7x7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 128), nn.ReLU(),\n",
    "            nn.Linear(128, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "def count_params(model: nn.Module) -> Tuple[int, int]:\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "base_model = SmallCNN().to(device)\n",
    "print(\"Params (total, trainable):\", count_params(base_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd46e18",
   "metadata": {},
   "source": [
    "## 4. Pretrain the base model\n",
    "\n",
    "Short training just to get a reasonable base model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba863ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, opt, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    for x, y in tqdm(loader, leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        y_true.append(y.detach().cpu().numpy())\n",
    "        y_pred.append(logits.argmax(dim=1).detach().cpu().numpy())\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    return total_loss / len(loader.dataset), accuracy_score(y_true, y_pred)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        y_true.append(y.detach().cpu().numpy())\n",
    "        y_pred.append(logits.argmax(dim=1).detach().cpu().numpy())\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(base_model.parameters(), lr=cfg.lr_base)\n",
    "\n",
    "hist_base = {\"train_acc\": [], \"test_acc\": []}\n",
    "for ep in range(1, cfg.base_epochs + 1):\n",
    "    tl, ta = train_epoch(base_model, base_loader, opt, loss_fn)\n",
    "    va = eval_model(base_model, test_loader)\n",
    "    hist_base[\"train_acc\"].append(ta)\n",
    "    hist_base[\"test_acc\"].append(va)\n",
    "    print(f\"[Base] epoch {ep:02d} loss={tl:.4f} train_acc={ta:.3f} test_acc={va:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(hist_base[\"train_acc\"], label=\"train acc\")\n",
    "plt.plot(hist_base[\"test_acc\"], label=\"test acc\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb550355",
   "metadata": {},
   "source": [
    "## 5. LoRA implementation (Linear layers)\n",
    "\n",
    "LoRA adds a low-rank update to a frozen linear weight:\n",
    "\n",
    "\\[\n",
    "W' = W + \\Delta W,\\quad \\Delta W = \\frac{\\alpha}{r}BA\n",
    "\\]\n",
    "\n",
    "We implement LoRA for `nn.Linear` and then swap all linear layers in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f7ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"Wrap a frozen nn.Linear with a trainable low-rank adapter.\"\"\"\n",
    "    def __init__(self, linear: nn.Linear, r: int = 8, alpha: float = 16.0, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        assert isinstance(linear, nn.Linear)\n",
    "        self.base = linear\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.in_features = linear.in_features\n",
    "        self.out_features = linear.out_features\n",
    "        self.r = int(r)\n",
    "        self.alpha = float(alpha)\n",
    "        self.scaling = self.alpha / max(self.r, 1)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        if self.r > 0:\n",
    "            self.A = nn.Parameter(torch.zeros(self.r, self.in_features))\n",
    "            self.B = nn.Parameter(torch.zeros(self.out_features, self.r))\n",
    "            nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.B)  # start from base model\n",
    "        else:\n",
    "            self.register_parameter(\"A\", None)\n",
    "            self.register_parameter(\"B\", None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.base(x)\n",
    "        if self.r > 0:\n",
    "            x_d = self.dropout(x)\n",
    "            delta = (x_d @ self.A.t()) @ self.B.t()\n",
    "            y = y + self.scaling * delta\n",
    "        return y\n",
    "\n",
    "def apply_lora_to_linears(model: nn.Module, r: int, alpha: float, dropout: float):\n",
    "    \"\"\"Recursively replace all nn.Linear layers with LoRALinear.\"\"\"\n",
    "    for name, module in list(model.named_children()):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            setattr(model, name, LoRALinear(module, r=r, alpha=alpha, dropout=dropout))\n",
    "        else:\n",
    "            apply_lora_to_linears(module, r=r, alpha=alpha, dropout=dropout)\n",
    "\n",
    "def set_trainable(model: nn.Module, trainable: bool):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = trainable\n",
    "\n",
    "def set_trainable_last_layer(model: nn.Module):\n",
    "    set_trainable(model, False)\n",
    "    for p in model.classifier[-1].parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "def set_trainable_lora_only(model: nn.Module):\n",
    "    set_trainable(model, False)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, LoRALinear):\n",
    "            if m.A is not None: m.A.requires_grad = True\n",
    "            if m.B is not None: m.B.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0072d9",
   "metadata": {},
   "source": [
    "## 6. Create adaptation variants\n",
    "\n",
    "- **Full fine-tune**\n",
    "- **Head-only**\n",
    "- **LoRA-only**\n",
    "\n",
    "All start from the same base weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1846f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def clone_model(m: nn.Module) -> nn.Module:\n",
    "    return copy.deepcopy(m)\n",
    "\n",
    "m_full = clone_model(base_model).to(device)\n",
    "set_trainable(m_full, True)\n",
    "\n",
    "m_head = clone_model(base_model).to(device)\n",
    "set_trainable_last_layer(m_head)\n",
    "\n",
    "m_lora = clone_model(base_model).to(device)\n",
    "apply_lora_to_linears(m_lora, r=cfg.lora_rank, alpha=cfg.lora_alpha, dropout=cfg.lora_dropout)\n",
    "set_trainable_lora_only(m_lora)\n",
    "\n",
    "print(\"Full FT params (total, trainable):\", count_params(m_full))\n",
    "print(\"Head-only params (total, trainable):\", count_params(m_head))\n",
    "print(\"LoRA params (total, trainable):\", count_params(m_lora))\n",
    "\n",
    "trainable_names = [n for n,p in m_lora.named_parameters() if p.requires_grad]\n",
    "print(\"LoRA trainable tensors:\", len(trainable_names))\n",
    "print(\"Example trainables:\", trainable_names[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525529d7",
   "metadata": {},
   "source": [
    "## 7. Fine-tune on the shifted few-shot target data\n",
    "\n",
    "We train each method on `target_loader` and evaluate on `test_loader`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dfdb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model: nn.Module, train_loader, test_loader, lr: float, epochs: int, label: str):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam([p for p in model.parameters() if p.requires_grad], lr=lr)\n",
    "    hist = {\"test_acc\": [], \"time_s\": []}\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        t0 = time.time()\n",
    "        tl, ta = train_epoch(model, train_loader, opt, loss_fn)\n",
    "        va = eval_model(model, test_loader)\n",
    "        hist[\"test_acc\"].append(va)\n",
    "        hist[\"time_s\"].append(time.time() - t0)\n",
    "        print(f\"[{label}] epoch {ep:02d} loss={tl:.4f} train_acc={ta:.3f} test_acc={va:.3f} time={hist['time_s'][-1]:.2f}s\")\n",
    "    return hist\n",
    "\n",
    "hist_full = fit(m_full, target_loader, test_loader, cfg.lr_target, cfg.target_epochs, \"FULL\")\n",
    "hist_head = fit(m_head, target_loader, test_loader, cfg.lr_target, cfg.target_epochs, \"HEAD\")\n",
    "hist_lora = fit(m_lora, target_loader, test_loader, cfg.lr_target, cfg.target_epochs, \"LoRA\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(hist_full[\"test_acc\"], label=\"Full FT\")\n",
    "plt.plot(hist_head[\"test_acc\"], label=\"Head-only\")\n",
    "plt.plot(hist_lora[\"test_acc\"], label=\"LoRA\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"test accuracy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(np.cumsum(hist_full[\"time_s\"]), label=\"Full FT\")\n",
    "plt.plot(np.cumsum(hist_head[\"time_s\"]), label=\"Head-only\")\n",
    "plt.plot(np.cumsum(hist_lora[\"time_s\"]), label=\"LoRA\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"cumulative seconds\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a25a3",
   "metadata": {},
   "source": [
    "## 8. Final evaluation (classification report)\n",
    "\n",
    "How do the adapted models behave across classes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d3209",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(model, loader):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        ps.append(logits.argmax(dim=1).cpu().numpy())\n",
    "        ys.append(y.numpy())\n",
    "    return np.concatenate(ys), np.concatenate(ps)\n",
    "\n",
    "y_true, p_full = predict(m_full, test_loader)\n",
    "_, p_head = predict(m_head, test_loader)\n",
    "_, p_lora = predict(m_lora, test_loader)\n",
    "\n",
    "print(\"=== Full fine-tune ===\")\n",
    "print(classification_report(y_true, p_full, zero_division=0))\n",
    "print(\"=== Head-only ===\")\n",
    "print(classification_report(y_true, p_head, zero_division=0))\n",
    "print(\"=== LoRA ===\")\n",
    "print(classification_report(y_true, p_lora, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff18bd8",
   "metadata": {},
   "source": [
    "## 9. Discussion & extensions\n",
    "\n",
    "1. Which method performs best **per trainable parameter**?\n",
    "2. When does LoRA beat head-only fine-tuning?\n",
    "3. How does rank `r` change the tradeoff?\n",
    "\n",
    "**Extensions**\n",
    "- Make a **shifted test set** (apply rotation+noise at test time) and evaluate there too.\n",
    "- Try `r ∈ {2, 4, 8, 16}` and plot accuracy vs trainable params.\n",
    "- Swap the base CNN for a pretrained model and repeat (more realistic).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927307f8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fa2d278",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r2ska-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
