{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f6db01",
   "metadata": {},
   "source": [
    "# Practical 3A — Parameter-Efficient Fine-Tuning with LoRA\n",
    "\n",
    "**Road to SKA: Foundation Models, Embeddings, and Latent Spaces**\n",
    "\n",
    "This practical demonstrates running LoRA on a small vision encoder *without* heavy EO plumbing.\n",
    "\n",
    "You will:\n",
    "\n",
    "1. Train a **base CNN classifier** (a stand-in for a pretrained foundation encoder).\n",
    "2. Freeze the base weights and adapt to a **shifted target task** using:\n",
    "   - **Full fine-tuning** (update all weights)\n",
    "   - **Head-only fine-tuning** (update only last layer)\n",
    "   - **LoRA fine-tuning** (update only small low-rank adapter matrices)\n",
    "3. Compare:\n",
    "   - accuracy vs training time\n",
    "   - number of trainable parameters\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "- LoRA paper (Hu et al., 2021 / ICLR 2022): https://arxiv.org/abs/2106.09685  \n",
    "- Microsoft LoRA repo / `loralib`: https://github.com/microsoft/LoRA  \n",
    "- Hugging Face PEFT LoRA docs (concept + config): https://huggingface.co/docs/peft/en/conceptual_guides/lora  \n",
    "\n",
    "> We implement LoRA **directly in PyTorch** here so it works anywhere (no Transformers dependency).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ypweuodfea",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cambridge-iccs/R2SKA_Advanced_Tutorial/blob/main/Session3A_LoRA_Finetuning.ipynb)\n\n---\n\n## Environment Setup (Colab / Local)\n\nRun the cell below to detect your environment and set up paths. This notebook uses only PyTorch and standard libraries, which are pre-installed on Colab.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gsrmwblgnfm",
   "source": "# Detect environment and set up paths\nimport sys\n\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"Running in Google Colab\")\n    DATA_ROOT = '/content/data'\n    # PyTorch is pre-installed on Colab\nelse:\n    print(\"Running locally\")\n    DATA_ROOT = './data'\n\nprint(f\"Data directory: {DATA_ROOT}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2875db2c",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "CPU-friendly. Uses GPU if available.\n",
    "\n",
    "Required:\n",
    "- `torch`, `torchvision`, `numpy`, `matplotlib`, `scikit-learn`\n",
    "\n",
    "Optional:\n",
    "- `tqdm` (progress bars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional installs (uncomment if needed)\n",
    "# %pip -q install torch torchvision scikit-learn tqdm\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass, fields\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    tqdm = lambda x, **kwargs: x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c61a7",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "We create a **domain shift**:\n",
    "\n",
    "- **Base training:** MNIST with standard images  \n",
    "- **Target adaptation:** MNIST with rotation + noise, and **few-shot** labels\n",
    "\n",
    "This mimics adapting a pretrained encoder to a new domain with limited labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c6eef",
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass Config:\n    data_dir: str = DATA_ROOT    # Uses environment-detected path\n    seed: int = 42\n    batch_size: int = 128\n    base_epochs: int = 5\n    target_epochs: int = 5\n    lr_base: float = 1e-3\n    lr_target: float = 1e-3\n    max_base_train: int = 20000\n    max_target_train: int = 2000\n    max_test: int = 5000\n    lora_rank: int = 8\n    lora_alpha: float = 16.0\n    lora_dropout: float = 0.0\n    rotate_deg: float = 45.0\n    noise_std: float = 0.15\n\n    # Formatted string representation for readable output\n    def __repr__(self):\n        lines = [f\"{self.__class__.__name__}:\"]\n        for f in fields(self):\n            lines.append(f\"  {f.name:20s} = {getattr(self, f.name)!r}\")\n        return \"\\n\".join(lines)\n\ncfg = Config()\n\nrandom.seed(cfg.seed)\nnp.random.seed(cfg.seed)\ntorch.manual_seed(cfg.seed)\n\n# Device selection: CUDA > MPS (Apple Silicon) > CPU\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(\"Device:\", device)\nprint(cfg)"
  },
  {
   "cell_type": "markdown",
   "id": "59b989f7",
   "metadata": {},
   "source": [
    "## 3. Data: Base vs Target\n",
    "\n",
    "- Base transform: standard MNIST  \n",
    "- Target transform: rotation + gaussian noise (synthetic shift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise:\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to the data.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, std: float = 0.1):\n",
    "        self.std = std\n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.clamp(x + torch.randn_like(x) * self.std, 0.0, 1.0)\n",
    "\n",
    "# Stack transformations to operate on the data\n",
    "base_tfm = transforms.Compose([transforms.ToTensor()])  # Create tensor\n",
    "target_tfm = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=cfg.rotate_deg),  # Rand rotate\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(std=cfg.noise_std),                # Add noise\n",
    "])\n",
    "\n",
    "base_train = datasets.MNIST(cfg.data_dir, train=True, download=True, transform=base_tfm)\n",
    "target_train = datasets.MNIST(cfg.data_dir, train=True, download=True, transform=target_tfm)\n",
    "test_ds = datasets.MNIST(cfg.data_dir, train=False, download=True, transform=base_tfm)\n",
    "\n",
    "# Sample a subset of the data\n",
    "def subset(ds, n: Optional[int], seed: int):\n",
    "    if n is None or n >= len(ds):\n",
    "        return ds\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = rng.choice(len(ds), size=n, replace=False)\n",
    "    return Subset(ds, idx)\n",
    "\n",
    "base_train_s = subset(base_train, cfg.max_base_train, cfg.seed)\n",
    "target_train_s = subset(target_train, cfg.max_target_train, cfg.seed + 1)\n",
    "test_s = subset(test_ds, cfg.max_test, cfg.seed + 2)\n",
    "\n",
    "# num_workers=0 for compatibility (set higher for multiprocessing)\n",
    "base_loader = DataLoader(base_train_s, batch_size=cfg.batch_size, shuffle=True, num_workers=0)\n",
    "target_loader = DataLoader(target_train_s, batch_size=cfg.batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_s, batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Visualise base vs target transforms on THE SAME images\n",
    "# Get a batch of indices to use for both\n",
    "sample_indices = list(range(8))\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(10, 3))\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    # Get the same underlying image with base transform\n",
    "    img_base, label = base_train[idx]\n",
    "    # Get with target transform (will have rotation + noise)\n",
    "    img_target, _ = target_train[idx]\n",
    "\n",
    "    axes[0, i].imshow(img_base[0], cmap=\"gray\")\n",
    "    axes[0, i].set_title(int(label))\n",
    "    axes[0, i].axis(\"off\")\n",
    "\n",
    "    axes[1, i].imshow(img_target[0], cmap=\"gray\")\n",
    "    axes[1, i].set_title(int(label))\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "axes[0, 0].set_ylabel(\"Base\", fontsize=11)\n",
    "axes[1, 0].set_ylabel(\"Target\", fontsize=11)\n",
    "plt.suptitle(\"Same images: Base (top) vs Target with rotation + noise (bottom)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a8476d",
   "metadata": {},
   "source": [
    "## 4. Base model - small CNN \n",
    "\n",
    "This is our “pretrained encoder + classifier head”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c3b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, n_classes: int = 10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 14x14\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 7x7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 128), nn.ReLU(),\n",
    "            nn.Linear(128, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "def count_params(model: nn.Module) -> Tuple[int, int]:\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "base_model = SmallCNN().to(device)\n",
    "print(\"Params (total, trainable):\", count_params(base_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd46e18",
   "metadata": {},
   "source": [
    "## 5. Pretrain the base model\n",
    "\n",
    "Short training just to get a reasonable base model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba863ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, opt, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    for x, y in tqdm(loader, leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        y_true.append(y.detach().cpu().numpy())\n",
    "        y_pred.append(logits.argmax(dim=1).detach().cpu().numpy())\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    return total_loss / len(loader.dataset), accuracy_score(y_true, y_pred)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        y_true.append(y.detach().cpu().numpy())\n",
    "        y_pred.append(logits.argmax(dim=1).detach().cpu().numpy())\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(base_model.parameters(), lr=cfg.lr_base)\n",
    "\n",
    "hist_base = {\"train_acc\": [], \"test_acc\": []}\n",
    "for ep in range(1, cfg.base_epochs + 1):\n",
    "    tl, ta = train_epoch(base_model, base_loader, opt, loss_fn)\n",
    "    va = eval_model(base_model, test_loader)\n",
    "    hist_base[\"train_acc\"].append(ta)\n",
    "    hist_base[\"test_acc\"].append(va)\n",
    "    print(f\"[Base] epoch {ep:02d} loss={tl:.4f} train_acc={ta:.3f} test_acc={va:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(hist_base[\"train_acc\"], label=\"train acc\")\n",
    "plt.plot(hist_base[\"test_acc\"], label=\"test acc\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb550355",
   "metadata": {},
   "source": [
    "## 6. LoRA implementation (Linear layers)\n",
    "\n",
    "When adapting large pretrained models, full fine-tuning updates **all** weights. For a model with millions or billions of parameters, this means:\n",
    "- Storing a complete copy of gradients and optimizer states\n",
    "- Risk of catastrophic forgetting (overwriting useful pretrained features)\n",
    "- Needing separate weight copies for each downstream task\n",
    "\n",
    "**Low-Rank Adaptation (LoRA)** is based on a key empirical observation from Hu et al. (2021):\n",
    "\n",
    "> *\"The change in weights during fine-tuning has a low intrinsic rank.\"*\n",
    "\n",
    "In other words, even though weight matrices are large (e.g., 4096 × 4096), the **update** $\\Delta W$ needed for adaptation can be well-approximated by a much smaller, low-rank matrix.\n",
    "\n",
    "### How LoRA Works\n",
    "\n",
    "Instead of updating $W$ directly, LoRA **freezes** the pretrained weights and adds a parallel low-rank branch:\n",
    "\n",
    "$$W' = W + \\Delta W, \\quad \\text{where} \\quad \\Delta W = \\frac{\\alpha}{r} B A$$\n",
    "\n",
    "- $W \\in \\mathbb{R}^{d_{out} \\times d_{in}}$ — original frozen weight matrix\n",
    "- $A \\in \\mathbb{R}^{r \\times d_{in}}$ — projects input down to rank $r$\n",
    "- $B \\in \\mathbb{R}^{d_{out} \\times r}$ — projects back up to output dimension\n",
    "- $r \\ll \\min(d_{in}, d_{out})$ — the **rank** (typically 4–64)\n",
    "- $\\alpha$ — scaling hyperparameter (controls magnitude of adaptation)\n",
    "\n",
    "### Parameter Savings\n",
    "\n",
    "For a weight matrix $W$ of shape $(d_{out}, d_{in})$:\n",
    "- **Full fine-tuning**: $d_{out} \\times d_{in}$ trainable parameters\n",
    "- **LoRA**: $r \\times (d_{in} + d_{out})$ trainable parameters\n",
    "\n",
    "**Example**: A linear layer with shape (4096, 4096):\n",
    "- Full: 16.7M parameters\n",
    "- LoRA (r=8): 65.5K parameters → **255× fewer!**\n",
    "\n",
    "### Initialization Strategy\n",
    "\n",
    "- **A** is initialized with Kaiming/He initialization (random)\n",
    "- **B** is initialized to **zero**\n",
    "\n",
    "This means $\\Delta W = BA = 0$ at the start, so the model begins exactly at the pretrained solution. Training then learns the minimal adjustment needed.\n",
    "\n",
    "### The Scaling Factor $\\alpha/r$\n",
    "\n",
    "The ratio $\\alpha/r$ controls how much the low-rank update contributes:\n",
    "- Larger $\\alpha$ → stronger adaptation signal\n",
    "- Common practice: set $\\alpha = 2r$ (so scaling = 2.0)\n",
    "- This decouples the learning rate from the choice of rank\n",
    "\n",
    "### Why LoRA Works Well\n",
    "\n",
    "1. **Preserves pretrained knowledge**: Frozen base weights retain learned features\n",
    "2. **Regularization effect**: Low-rank constraint prevents overfitting on small datasets\n",
    "3. **Efficient multi-task**: Store only small adapter weights per task, share the base model\n",
    "4. **No inference overhead**: Can merge $BA$ into $W$ after training: $W_{merged} = W + \\frac{\\alpha}{r}BA$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f7ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps a frozen nn.Linear with a trainable low-rank adapter.\n",
    "    \n",
    "    Forward pass computes: y = W_frozen @ x + (alpha/r) * B @ A @ x\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    linear : nn.Linear\n",
    "        The original linear layer to wrap (will be frozen)\n",
    "    r : int\n",
    "        Rank of the low-rank adaptation matrices A and B\n",
    "    alpha : float\n",
    "        Scaling factor; the update is scaled by alpha/r\n",
    "    dropout : float\n",
    "        Dropout probability applied to input before the low-rank path\n",
    "    \"\"\"\n",
    "    def __init__(self, linear: nn.Linear, r: int = 8, alpha: float = 16.0, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        assert isinstance(linear, nn.Linear)\n",
    "        \n",
    "        # Store and freeze the original pretrained weights\n",
    "        self.base = linear\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.in_features = linear.in_features\n",
    "        self.out_features = linear.out_features\n",
    "        self.r = int(r)\n",
    "        self.alpha = float(alpha)\n",
    "        self.scaling = self.alpha / max(self.r, 1)  # alpha/r scaling factor\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        if self.r > 0:\n",
    "            # A: projects input from d_in -> r (down-projection)\n",
    "            # B: projects from r -> d_out (up-projection)\n",
    "            # ΔW = B @ A has shape (d_out, d_in) but rank at most r\n",
    "            self.A = nn.Parameter(torch.zeros(self.r, self.in_features))\n",
    "            self.B = nn.Parameter(torch.zeros(self.out_features, self.r))\n",
    "            \n",
    "            # Initialize A with Kaiming uniform (like nn.Linear)\n",
    "            nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "            # Initialize B to zero so ΔW = 0 at start (begin at pretrained solution)\n",
    "            nn.init.zeros_(self.B)\n",
    "        else:\n",
    "            self.register_parameter(\"A\", None)\n",
    "            self.register_parameter(\"B\", None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Frozen pretrained path: y = W @ x + b\n",
    "        y = self.base(x)\n",
    "        \n",
    "        if self.r > 0:\n",
    "            # Low-rank adaptation path: Δy = (alpha/r) * (x @ A.T) @ B.T\n",
    "            # Equivalent to: Δy = (alpha/r) * x @ (B @ A).T\n",
    "            x_d = self.dropout(x)\n",
    "            delta = (x_d @ self.A.t()) @ self.B.t()  # shape: (batch, d_out)\n",
    "            y = y + self.scaling * delta\n",
    "        return y\n",
    "\n",
    "\n",
    "def apply_lora_to_linears(model: nn.Module, r: int, alpha: float, dropout: float):\n",
    "    \"\"\"\n",
    "    Recursively replace all nn.Linear layers in the model with LoRALinear.\n",
    "    \n",
    "    This freezes the original weights and adds trainable low-rank adapters.\n",
    "    \"\"\"\n",
    "    for name, module in list(model.named_children()):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            setattr(model, name, LoRALinear(module, r=r, alpha=alpha, dropout=dropout))\n",
    "        else:\n",
    "            apply_lora_to_linears(module, r=r, alpha=alpha, dropout=dropout)\n",
    "\n",
    "\n",
    "def set_trainable(model: nn.Module, trainable: bool):\n",
    "    \"\"\"Set requires_grad for all parameters in the model.\"\"\"\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = trainable\n",
    "\n",
    "\n",
    "def set_trainable_last_layer(model: nn.Module):\n",
    "    \"\"\"Freeze all weights except the final classification layer.\"\"\"\n",
    "    set_trainable(model, False)\n",
    "    for p in model.classifier[-1].parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "\n",
    "def set_trainable_lora_only(model: nn.Module):\n",
    "    \"\"\"Freeze everything except LoRA adapter matrices (A and B).\"\"\"\n",
    "    set_trainable(model, False)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, LoRALinear):\n",
    "            if m.A is not None: m.A.requires_grad = True\n",
    "            if m.B is not None: m.B.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0072d9",
   "metadata": {},
   "source": [
    "## 7. Create adaptation variants\n",
    "\n",
    "- **Full fine-tune**\n",
    "- **Head-only**\n",
    "- **LoRA-only**\n",
    "\n",
    "All start from the same base weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1846f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def clone_model(m: nn.Module) -> nn.Module:\n",
    "    return copy.deepcopy(m)\n",
    "\n",
    "m_full = clone_model(base_model).to(device)\n",
    "set_trainable(m_full, True)\n",
    "\n",
    "m_head = clone_model(base_model).to(device)\n",
    "set_trainable_last_layer(m_head)\n",
    "\n",
    "# For LoRA: apply adapters BEFORE moving to device, so A and B tensors are also moved\n",
    "m_lora = clone_model(base_model)\n",
    "apply_lora_to_linears(m_lora, r=cfg.lora_rank, alpha=cfg.lora_alpha, dropout=cfg.lora_dropout)\n",
    "m_lora = m_lora.to(device)  # Now move everything (including A, B) to device\n",
    "set_trainable_lora_only(m_lora)\n",
    "\n",
    "print(\"Full FT params (total, trainable):\", count_params(m_full))\n",
    "print(\"Head-only params (total, trainable):\", count_params(m_head))\n",
    "print(\"LoRA params (total, trainable):\", count_params(m_lora))\n",
    "\n",
    "trainable_names = [n for n,p in m_lora.named_parameters() if p.requires_grad]\n",
    "print(\"LoRA trainable tensors:\", len(trainable_names))\n",
    "print(\"Example trainables:\", trainable_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525529d7",
   "metadata": {},
   "source": [
    "## 8. Fine-tune on the shifted few-shot target data\n",
    "\n",
    "We train each method on `target_loader` and evaluate on `test_loader`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dfdb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model: nn.Module, train_loader, test_loader, lr: float, epochs: int, label: str):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam([p for p in model.parameters() if p.requires_grad], lr=lr)\n",
    "    hist = {\"test_acc\": [], \"time_s\": []}\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        t0 = time.time()\n",
    "        tl, ta = train_epoch(model, train_loader, opt, loss_fn)\n",
    "        va = eval_model(model, test_loader)\n",
    "        hist[\"test_acc\"].append(va)\n",
    "        hist[\"time_s\"].append(time.time() - t0)\n",
    "        print(f\"[{label}] epoch {ep:02d} loss={tl:.4f} train_acc={ta:.3f} test_acc={va:.3f} time={hist['time_s'][-1]:.2f}s\")\n",
    "    return hist\n",
    "\n",
    "hist_full = fit(m_full, target_loader, test_loader, cfg.lr_target, cfg.target_epochs, \"FULL\")\n",
    "hist_head = fit(m_head, target_loader, test_loader, cfg.lr_target, cfg.target_epochs, \"HEAD\")\n",
    "hist_lora = fit(m_lora, target_loader, test_loader, cfg.lr_target, cfg.target_epochs, \"LoRA\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(hist_full[\"test_acc\"], label=\"Full FT\")\n",
    "plt.plot(hist_head[\"test_acc\"], label=\"Head-only\")\n",
    "plt.plot(hist_lora[\"test_acc\"], label=\"LoRA\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"test accuracy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(np.cumsum(hist_full[\"time_s\"]), label=\"Full FT\")\n",
    "plt.plot(np.cumsum(hist_head[\"time_s\"]), label=\"Head-only\")\n",
    "plt.plot(np.cumsum(hist_lora[\"time_s\"]), label=\"LoRA\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"cumulative seconds\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a25a3",
   "metadata": {},
   "source": [
    "## 9. Final evaluation (classification report)\n",
    "\n",
    "How do the adapted models behave across classes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d3209",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(model, loader):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        ps.append(logits.argmax(dim=1).cpu().numpy())\n",
    "        ys.append(y.numpy())\n",
    "    return np.concatenate(ys), np.concatenate(ps)\n",
    "\n",
    "y_true, p_full = predict(m_full, test_loader)\n",
    "_, p_head = predict(m_head, test_loader)\n",
    "_, p_lora = predict(m_lora, test_loader)\n",
    "\n",
    "print(\"=== Full fine-tune ===\")\n",
    "print(classification_report(y_true, p_full, zero_division=0))\n",
    "print(\"=== Head-only ===\")\n",
    "print(classification_report(y_true, p_head, zero_division=0))\n",
    "print(\"=== LoRA ===\")\n",
    "print(classification_report(y_true, p_lora, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff18bd8",
   "metadata": {},
   "source": [
    "## 9. Discussion & extensions\n",
    "\n",
    "1. Which method performs best **per trainable parameter**?\n",
    "2. When does LoRA beat head-only fine-tuning?\n",
    "3. How does rank `r` change the tradeoff?\n",
    "\n",
    "**Extensions**\n",
    "- Make a **shifted test set** (apply rotation+noise at test time) and evaluate there too.\n",
    "- Try `r ∈ {2, 4, 8, 16}` and plot accuracy vs trainable params.\n",
    "- Swap the base CNN for a pretrained model and repeat (more realistic).\n",
    "\n",
    "---\n",
    "\n",
    "### What models is LoRA designed for?\n",
    "\n",
    "You may notice LoRA is *slower* than full fine-tuning here. This is expected for small models—LoRA adds extra matrix multiplications in the forward pass, and the backward-pass savings are minimal at this scale. LoRA's speed advantage only emerges on large models (millions+ parameters) where backward-pass computation dominates and memory savings enable larger batch sizes.\n",
    "\n",
    "| Model Size | LoRA Speed Benefit |\n",
    "|------------|-------------------|\n",
    "| <1M params | Slower (like this tutorial) |\n",
    "| 1–10M params | Break-even |\n",
    "| 10–100M params | Modest speedup |\n",
    "| 100M+ params | Significant speedup + memory savings |\n",
    "\n",
    "To demonstrate LoRA's speed benefits, you'd need a larger model such as ResNet-18 (~11M params) or a small ViT (~5-10M params).\n",
    "\n",
    "For models like BERT-base (110M params) or GPT-2 (124M params), full fine-tuning requires storing gradients + Adam optimizer states (2× params for momentum and variance) for all weights. LoRA cuts this dramatically, enabling fine-tuning on GPUs that couldn't otherwise fit the training.\n",
    "\n",
    "LoRA was designed for foundation models (100M–100B+ params) where full fine-tuning is impractical or impossible. At the ~400K param scale used here, the value is in **parameter efficiency** and **regularization**, not raw speed. This tutorial makes that tradeoff visible—which is useful for understanding when to reach for LoRA in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d10ec89",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r2ska-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}